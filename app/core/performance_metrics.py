"""
qPCR Performance Metrics with Error Tolerance Thresholds
0.5 cycle, 1.0 cycle ê¸°ì¤€ KPI ê³„ì‚°
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple
import plotly.graph_objects as go
import plotly.express as px


class PerformanceMetrics:
    """
    qPCR Early-Ct ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
    - ê¸°ì¡´ MAE/RMSE/R2
    - ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ê¸°ì¤€ (0.5, 1.0 cycle)
    - Fold-change ê´€ì  ì§€í‘œ
    """
    
    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray):
        """
        Args:
            y_true: ì‹¤ì œ Ct ê°’
            y_pred: ì˜ˆì¸¡ Ct ê°’
        """
        self.y_true = y_true
        self.y_pred = y_pred
        self.errors = y_pred - y_true
        self.abs_errors = np.abs(self.errors)
    
    def calculate_basic_metrics(self) -> Dict[str, float]:
        """ê¸°ë³¸ íšŒê·€ ì§€í‘œ"""
        from sklearn.metrics import (
            mean_absolute_error, 
            mean_squared_error, 
            r2_score
        )
        
        mae = mean_absolute_error(self.y_true, self.y_pred)
        rmse = np.sqrt(mean_squared_error(self.y_true, self.y_pred))
        r2 = r2_score(self.y_true, self.y_pred)
        
        return {
            'MAE': mae,
            'RMSE': rmse,
            'RÂ²': r2
        }
    
    def calculate_tolerance_metrics(self) -> Dict[str, float]:
        """
        ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ê¸°ì¤€ ì •í™•ë„
        
        Returns:
            dict: {
                'P(|error| â‰¤ 0.5)': 0.5 cycle ì´ë‚´ ë¹„ìœ¨,
                'P(|error| â‰¤ 1.0)': 1.0 cycle ì´ë‚´ ë¹„ìœ¨,
                'P(|error| â‰¤ 2.0)': 2.0 cycle ì´ë‚´ ë¹„ìœ¨,
                ...
            }
        """
        thresholds = [0.3, 0.5, 1.0, 2.0, 3.0]
        
        tolerance_metrics = {}
        
        for threshold in thresholds:
            within_threshold = (self.abs_errors <= threshold).sum()
            ratio = within_threshold / len(self.abs_errors)
            tolerance_metrics[f'P(|error| â‰¤ {threshold})'] = ratio
        
        return tolerance_metrics
    
    def calculate_fold_change_metrics(self) -> Dict[str, float]:
        """
        Fold-change (ì¦í­ ë°°ìˆ˜) ê´€ì ì˜ ì§€í‘œ
        
        qPCRì—ì„œ Ct ì°¨ì´ 1 cycle = 2ë°° ì°¨ì´
        Î”Ct = log2(fold_change)
        
        Returns:
            dict: {
                'Mean Fold Error': í‰ê·  ë°°ìˆ˜ ì˜¤ì°¨,
                'Median Fold Error': ì¤‘ì•™ê°’ ë°°ìˆ˜ ì˜¤ì°¨,
                'P(Fold Error < 1.5x)': 1.5ë°° ì´ë‚´ ë¹„ìœ¨,
                ...
            }
        """
        # Ct ì°¨ì´ë¥¼ fold-changeë¡œ ë³€í™˜
        fold_errors = 2 ** self.abs_errors
        
        metrics = {
            'Mean Fold Error': np.mean(fold_errors),
            'Median Fold Error': np.median(fold_errors),
            'P(Fold Error < 1.5x)': (fold_errors < 1.5).sum() / len(fold_errors),
            'P(Fold Error < 2.0x)': (fold_errors < 2.0).sum() / len(fold_errors),
            'P(Fold Error < 3.0x)': (fold_errors < 3.0).sum() / len(fold_errors),
        }
        
        return metrics
    
    def calculate_ct_range_performance(self, bins: list = None) -> pd.DataFrame:
        """
        Ct êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
        
        Args:
            bins: Ct êµ¬ê°„ ê²½ê³„ (ê¸°ë³¸ê°’: [0, 10, 15, 20, 25, 30, 35, 40])
        
        Returns:
            DataFrame: êµ¬ê°„ë³„ MAE, RMSE, P(â‰¤0.5), P(â‰¤1.0)
        """
        if bins is None:
            bins = [0, 10, 15, 20, 25, 30, 35, 40]
        
        ct_ranges = pd.cut(self.y_true, bins=bins, include_lowest=True)
        
        results = []
        
        for ct_range in ct_ranges.cat.categories:
            mask = ct_ranges == ct_range
            
            if mask.sum() == 0:
                continue
            
            range_errors = self.abs_errors[mask]
            range_true = self.y_true[mask]
            range_pred = self.y_pred[mask]
            
            mae = np.mean(range_errors)
            rmse = np.sqrt(np.mean((range_pred - range_true) ** 2))
            p_05 = (range_errors <= 0.5).sum() / len(range_errors)
            p_10 = (range_errors <= 1.0).sum() / len(range_errors)
            
            results.append({
                'Ct Range': str(ct_range),
                'Count': mask.sum(),
                'MAE': mae,
                'RMSE': rmse,
                'P(â‰¤0.5)': p_05,
                'P(â‰¤1.0)': p_10
            })
        
        return pd.DataFrame(results)
    
    def get_all_metrics(self) -> Dict[str, any]:
        """ëª¨ë“  ì§€í‘œë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
        metrics = {}
        
        # 1. ê¸°ë³¸ ì§€í‘œ
        metrics.update(self.calculate_basic_metrics())
        
        # 2. í—ˆìš© ë²”ìœ„ ì§€í‘œ
        metrics.update(self.calculate_tolerance_metrics())
        
        # 3. Fold-change ì§€í‘œ
        metrics.update(self.calculate_fold_change_metrics())
        
        return metrics
    
    def plot_error_distribution(self) -> go.Figure:
        """ì˜¤ì°¨ ë¶„í¬ ì‹œê°í™” (íˆìŠ¤í† ê·¸ë¨ + í—ˆìš© ë²”ìœ„ í‘œì‹œ)"""
        fig = go.Figure()
        
        # íˆìŠ¤í† ê·¸ë¨
        fig.add_trace(go.Histogram(
            x=self.errors,
            nbinsx=50,
            name='Error Distribution',
            marker_color='steelblue',
            opacity=0.7
        ))
        
        # í—ˆìš© ë²”ìœ„ í‘œì‹œ
        for threshold, color in [(0.5, 'green'), (1.0, 'orange')]:
            fig.add_vline(
                x=threshold, 
                line_dash="dash", 
                line_color=color,
                annotation_text=f"+{threshold}",
                annotation_position="top"
            )
            fig.add_vline(
                x=-threshold, 
                line_dash="dash", 
                line_color=color,
                annotation_text=f"-{threshold}",
                annotation_position="top"
            )
        
        fig.update_layout(
            title="Prediction Error Distribution with Tolerance Thresholds",
            xaxis_title="Error (Predicted - True)",
            yaxis_title="Count",
            showlegend=False,
            height=400
        )
        
        return fig
    
    def plot_cumulative_error(self) -> go.Figure:
        """ëˆ„ì  ì˜¤ì°¨ ë¶„í¬ (CDF)"""
        sorted_errors = np.sort(self.abs_errors)
        cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=sorted_errors,
            y=cumulative * 100,
            mode='lines',
            name='Cumulative Distribution',
            line=dict(color='steelblue', width=2)
        ))
        
        # í—ˆìš© ë²”ìœ„ í‘œì‹œ
        for threshold, color in [(0.5, 'green'), (1.0, 'orange'), (2.0, 'red')]:
            pct = (self.abs_errors <= threshold).sum() / len(self.abs_errors) * 100
            
            fig.add_vline(
                x=threshold,
                line_dash="dash",
                line_color=color,
                annotation_text=f"{threshold} cycle ({pct:.1f}%)",
                annotation_position="top right"
            )
        
        fig.update_layout(
            title="Cumulative Error Distribution",
            xaxis_title="Absolute Error (cycles)",
            yaxis_title="Cumulative Percentage (%)",
            height=400
        )
        
        return fig
    
    def create_performance_summary_table(self) -> pd.DataFrame:
        """ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”"""
        all_metrics = self.get_all_metrics()
        
        summary = pd.DataFrame([
            {'Metric': 'Mean Absolute Error (MAE)', 'Value': f"{all_metrics['MAE']:.3f} cycles"},
            {'Metric': 'Root Mean Squared Error (RMSE)', 'Value': f"{all_metrics['RMSE']:.3f} cycles"},
            {'Metric': 'RÂ² Score', 'Value': f"{all_metrics['RÂ²']:.4f}"},
            {'Metric': '', 'Value': ''},  # êµ¬ë¶„ì„ 
            {'Metric': 'P(|error| â‰¤ 0.5 cycle)', 'Value': f"{all_metrics['P(|error| â‰¤ 0.5)']:.1%}"},
            {'Metric': 'P(|error| â‰¤ 1.0 cycle)', 'Value': f"{all_metrics['P(|error| â‰¤ 1.0)']:.1%}"},
            {'Metric': 'P(|error| â‰¤ 2.0 cycle)', 'Value': f"{all_metrics['P(|error| â‰¤ 2.0)']:.1%}"},
            {'Metric': '', 'Value': ''},  # êµ¬ë¶„ì„ 
            {'Metric': 'Mean Fold Error', 'Value': f"{all_metrics['Mean Fold Error']:.2f}x"},
            {'Metric': 'P(Fold Error < 1.5x)', 'Value': f"{all_metrics['P(Fold Error < 1.5x)']:.1%}"},
            {'Metric': 'P(Fold Error < 2.0x)', 'Value': f"{all_metrics['P(Fold Error < 2.0x)']:.1%}"},
        ])
        
        return summary


# ===== Streamlit Integration Example =====
def render_enhanced_performance_page(y_true, y_pred):
    """
    ê¸°ì¡´ Performance í˜ì´ì§€ì— ì¶”ê°€í•  í–¥ìƒëœ ë©”íŠ¸ë¦­
    """
    import streamlit as st
    
    st.header("ğŸ“Š Performance Metrics (Enhanced)")
    
    # ì„±ëŠ¥ ê³„ì‚°
    perf = PerformanceMetrics(y_true, y_pred)
    
    # 1. ìš”ì•½ í…Œì´ë¸”
    st.subheader("Performance Summary")
    summary_table = perf.create_performance_summary_table()
    st.dataframe(summary_table, use_container_width=True, hide_index=True)
    
    # 2. ì‹œê°í™”
    col1, col2 = st.columns(2)
    
    with col1:
        st.plotly_chart(perf.plot_error_distribution(), use_container_width=True)
    
    with col2:
        st.plotly_chart(perf.plot_cumulative_error(), use_container_width=True)
    
    # 3. Ct êµ¬ê°„ë³„ ì„±ëŠ¥
    st.subheader("Performance by Ct Range")
    ct_range_perf = perf.calculate_ct_range_performance()
    
    st.dataframe(
        ct_range_perf.style.format({
            'MAE': '{:.3f}',
            'RMSE': '{:.3f}',
            'P(â‰¤0.5)': '{:.1%}',
            'P(â‰¤1.0)': '{:.1%}'
        }).background_gradient(subset=['MAE', 'RMSE'], cmap='RdYlGn_r'),
        use_container_width=True,
        hide_index=True
    )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    np.random.seed(42)
    y_true = np.random.uniform(10, 35, 1000)
    y_pred = y_true + np.random.normal(0, 0.8, 1000)
    
    perf = PerformanceMetrics(y_true, y_pred)
    
    print("=== All Metrics ===")
    for k, v in perf.get_all_metrics().items():
        print(f"{k}: {v:.4f}")
    
    print("\n=== Ct Range Performance ===")
    print(perf.calculate_ct_range_performance())
