# app/streamlit_app.py
from __future__ import annotations

import os
import io
import re
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path
import urllib.request
import urllib.error
import plotly.express as px
import numpy as np
import pandas as pd
import streamlit as st
import xgboost as xgb
import pyarrow.dataset as ds
import argparse

# âœ… set_page_configëŠ” ë°˜ë“œì‹œ 1ë²ˆë§Œ, ê·¸ë¦¬ê³  ìµœìƒë‹¨ì—ì„œ
st.set_page_config(page_title="CPHOTONICS | Early Ct Predictor", layout="wide")

# ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œê°€ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
if 'show_data_catalog' not in st.session_state:
    st.session_state.show_data_catalog = False

PROJECT_ROOT = Path(__file__).resolve().parents[1]

# âœ… ê²½ë¡œëŠ” PROJECT_ROOT ê¸°ì¤€ìœ¼ë¡œ
ASSETS_DIR = PROJECT_ROOT / "assets"
CATALOG_PATH = ASSETS_DIR / "data_catalog.json"
QC_DIR = PROJECT_ROOT / "outputs" / "qc"  # âœ… QC_DIR ì •ì˜ ì¶”ê°€

OUTPUTS_DIR = PROJECT_ROOT / "outputs" / "qc_performance_analysis"
MODELS_DIR = PROJECT_ROOT / "data" / "models" / "by_cutoff"
UPLOAD_DIR = PROJECT_ROOT / "data" / "uploads"

# ========================================
# GitHub Releaseì—ì„œ QC ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ
# ========================================
def load_data_catalog(catalog_path):
    try:
        with open(catalog_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def load_data_catalog(path: Path) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except Exception as e:
        st.error(f"Failed to read data_catalog.json: {e}")
        return {}

catalog = load_data_catalog(CATALOG_PATH)

def find_file_url_in_catalog(catalog: dict, filename: str) -> str | None:
    for item in catalog.get("files", []):
        if item.get("filename") == filename:
            return item.get("url")
    return None

def download_to_path(url: str, dst_path):
    dst_path.parent.mkdir(parents=True, exist_ok=True)
    # GitHub release assetì€ 302 redirectê°€ ëœ° ìˆ˜ ìˆì–´ì„œ urlretrieve/urllibê°€ ì•ˆì •ì 
    urllib.request.urlretrieve(url, dst_path)

def load_data_catalog_json(catalog_path):
    if not catalog_path.exists():
        return None
    with open(catalog_path, "r", encoding="utf-8") as f:
        return json.load(f)

def ensure_asset_download(url: str, dst_path):
    dst_path.parent.mkdir(parents=True, exist_ok=True)
    if dst_path.exists() and dst_path.stat().st_size > 0:
        return False  # already exists

    # GitHub release assetì€ redirectê°€ ìˆì„ ìˆ˜ ìˆì–´ urllibê°€ ì•ˆì „í•¨
    with urllib.request.urlopen(url) as r, open(dst_path, "wb") as f:
        f.write(r.read())
    return True

def download_qc_data_from_github():
    """
    GitHub Releaseì—ì„œ QC ê´€ë ¨ parquet íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œ
    
    ì‚¬ìš©ë²•:
    1. GitHubì—ì„œ Release ìƒì„±
    2. QC parquet íŒŒì¼ë“¤ì„ Releaseì— ì²¨ë¶€
    3. ì´ í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
    """
    # âœ… ì—¬ê¸°ì— ì‹¤ì œ GitHub Release URL ì…ë ¥
    GITHUB_RELEASE_URL = "https://github.com/YOUR_USERNAME/YOUR_REPO/releases/download/v1.0.0/"
    
    QC_DIR.mkdir(parents=True, exist_ok=True)
    
    # ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ ëª©ë¡
    files_to_download = [
        "master_catalog.parquet",
        "excluded_report.parquet",
    ]
    
    for filename in files_to_download:
        local_path = QC_DIR / filename
        
        # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if local_path.exists():
            continue
        
        url = GITHUB_RELEASE_URL + filename
        
        try:
            st.info(f"Downloading {filename} from GitHub Release...")
            urllib.request.urlretrieve(url, local_path)
            st.success(f"âœ… Downloaded: {filename}")
        except urllib.error.HTTPError as e:
            st.warning(f"âš ï¸ Failed to download {filename}: {e}")
        except Exception as e:
            st.warning(f"âš ï¸ Error downloading {filename}: {e}")


# Streamlit Cloudì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ ìë™ ë‹¤ìš´ë¡œë“œ
def running_on_streamlit_cloud() -> bool:
    return str(PROJECT_ROOT).startswith("/mount/src") or os.environ.get("STREAMLIT_RUNTIME_ENV") == "cloud"


if running_on_streamlit_cloud():
    # Cloudì—ì„œëŠ” QC ë°ì´í„°ë¥¼ GitHub Releaseì—ì„œ ë‹¤ìš´ë¡œë“œ
    qc_local_path = QC_DIR / "master_catalog.parquet"

    if not qc_local_path.exists():
        if not catalog:
            st.error("data_catalog.json not loaded (catalog is None/empty).")
        else:
            found = False
            for item in catalog.get("files", []):
                if item.get("filename") == "master_catalog.parquet":
                    ensure_asset_download(item["url"], qc_local_path)
                    found = True
                    break

            if not found:
                st.error("master_catalog.parquet entry not found in assets/data_catalog.json")


# âœ… cutoff ë¨¼ì € ì •ì˜
cutoff = int(st.sidebar.selectbox("Cutoff", [10, 20, 24, 30, 40], index=1))

OPS_DIR = PROJECT_ROOT / "outputs" / "qc_performance_analysis"
OPS_DIR.mkdir(parents=True, exist_ok=True)

ops_filename = f"ops_decisions_cutoff_{cutoff}.parquet"
parquet_path = OPS_DIR / ops_filename
csv_path     = OPS_DIR / f"ops_decisions_cutoff_{cutoff}.csv"

# âœ… Streamlit Cloudì—ì„œ ops decisions parquet ìë™ ë‹¤ìš´ë¡œë“œ
if running_on_streamlit_cloud():
    if not parquet_path.exists():
        ops_url = find_file_url_in_catalog(catalog, ops_filename)
        if ops_url:
            try:
                download_to_path(ops_url, parquet_path)
            except Exception as e:
                st.warning(f"Failed to download ops decisions ({ops_filename}): {e}")
        else:
            st.warning(
                f"Ops decisions file for cutoff={cutoff} not found in data_catalog.json: {ops_filename}"
            )

# ê¸°ì¡´ Data Catalog í‘œì‹œ ë¶€ë¶„ (if st.session_state.show_data_catalog:) ì „ì²´ë¥¼ ì•„ë˜ë¡œ êµì²´

if st.session_state.show_data_catalog:
    st.header("ğŸ“Š Data Quality Control & Catalog")
    st.markdown("QC ìƒíƒœ(PASS/FAIL/FLAG), Ct bin, excluded ì‚¬ìœ ë¥¼ í•œ ë²ˆì— ì •ë¦¬/ë‹¤ìš´ë¡œë“œí•˜ëŠ” í˜ì´ì§€")

    # 1. master_catalog ë¡œë“œ
    @st.cache_data
    def load_master_catalog():
        path = QC_DIR / "master_catalog.parquet"
        if path.exists():
            return pd.read_parquet(path)
        else:
            st.error("master_catalog.parquet not found. Cloudì—ì„œëŠ” GitHub Releaseì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
            return pd.DataFrame()

    df = load_master_catalog()
    # ì»¬ëŸ¼ í™•ì¸ & ì•ˆì „ ì²˜ë¦¬
    if "exclusion_reason" not in df.columns or df["exclusion_reason"].isna().all():
        df["exclusion_reason"] = "No specific reason"  # N/A ëŒ€ì‹  ì˜ë¯¸ìˆëŠ” ê°’
    
    # qc_status, ct_bin ì•ˆì „ ì²˜ë¦¬
    df["qc_status"] = df["qc_status"].fillna("UNKNOWN").astype(str)
    df["ct_bin"] = df["ct_bin"].fillna("UNKNOWN").astype(str)
    
    # Exclusion Reasons ì°¨íŠ¸ ìˆ˜ì • (N/A ì œì™¸í•˜ê³  ì‹¤ì œ ì´ìœ ë§Œ)
    excluded_df = df[(df["qc_status"] != "PASS") & (df["exclusion_reason"] != "N/A") & (df["exclusion_reason"] != "No specific reason")]
    if not excluded_df.empty:
        reasons = excluded_df["exclusion_reason"].value_counts().head(10).reset_index()
        reasons = reasons[reasons["exclusion_reason"] != "N/A"]  # ê°•ì œ í•„í„°
        fig_ex = px.bar(reasons, x="count", y="exclusion_reason", orientation="h",
                        title="Top 10 Exclusion Reasons")
        fig_ex.update_layout(height=500)
        st.plotly_chart(fig_ex, use_container_width=True)
    else:
        st.info("Excluded ìƒ˜í”Œì´ ì—†ê±°ë‚˜ exclusion_reasonì´ ëª¨ë‘ N/Aì…ë‹ˆë‹¤.")
        
    if df.empty:
        st.stop()

    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ (ì—ëŸ¬ ë°©ì§€)
    required_cols = ["qc_status", "ct_bin"]
    if "exclusion_reason" not in df.columns:
        df["exclusion_reason"] = "N/A"

    # 2. Summary Statistics
    total = len(df)
    pass_c = len(df[df["qc_status"] == "PASS"])
    fail_c = len(df[df["qc_status"] == "FAIL"])
    flag_c = len(df[df["qc_status"] == "FLAG"])
    usable = pass_c
    excluded = total - usable

    col1, col2, col3, col4, col5, col6 = st.columns(6)
    col1.metric("Total Wells", f"{total:,}")
    col2.metric("âœ… PASS", f"{pass_c:,}", f"{pass_c/total*100:.1f}%")
    col3.metric("âŒ FAIL", f"{fail_c:,}", f"{fail_c/total*100:.1f}%")
    col4.metric("âš ï¸ FLAG", f"{flag_c:,}", f"{flag_c/total*100:.1f}%")
    col5.metric("ğŸŸ¢ Usable", f"{usable:,}", f"{usable/total*100:.1f}%")
    col6.metric("ğŸ”´ Excluded", f"{excluded:,}", f"{excluded/total*100:.1f}%")

    st.divider()

    # 3. Visualizations
        # 3. Visualizations
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("QC Status Distribution")
        status_counts = df["qc_status"].value_counts()
        if not status_counts.empty:
            fig_pie = px.pie(
                status_counts.reset_index(),
                values="count", names="qc_status",
                color_discrete_map={"PASS": "#00FF00", "FAIL": "#FF0000", "FLAG": "#FFA500", "UNKNOWN": "#808080"}
            )
            fig_pie.update_layout(showlegend=True)
            st.plotly_chart(fig_pie, use_container_width=True)
        else:
            st.info("QC Status ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("Ct Bin Distribution")
        ct_order = sorted(df["ct_bin"].dropna().unique())
        if not ct_order:
            st.info("Ct Bin ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            fig_ct = px.bar(
                df["ct_bin"].value_counts().reindex(ct_order).reset_index(),
                x="ct_bin", y="count"
            )
            st.plotly_chart(fig_ct, use_container_width=True)

    st.subheader("QC Status by Ct Bin")
    stacked = df.groupby(["ct_bin", "qc_status"]).size().reset_index(name="count")
    if not stacked.empty:
        stacked = stacked.sort_values("ct_bin")
        fig_stacked = px.bar(
            stacked, x="ct_bin", y="count", color="qc_status",
            color_discrete_map={"PASS": "#00FF00", "FAIL": "#FF0000", "FLAG": "#FFA500"},
            title="QC Status by Ct Bin"
        )
        st.plotly_chart(fig_stacked, use_container_width=True)
    else:
        st.info("QC Status by Ct Bin ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    excluded_df = df[df["qc_status"] != "PASS"]
    if not excluded_df.empty:
        st.subheader("ğŸ” Exclusion Analysis - Top 10 Reasons")
        reasons = excluded_df["exclusion_reason"].value_counts().head(10).reset_index()
        if not reasons.empty and reasons["count"].sum() > 0:
            fig_ex = px.bar(reasons, x="count", y="exclusion_reason", orientation="h",
                            title="Top 10 Exclusion Reasons")
            fig_ex.update_layout(height=500)
            st.plotly_chart(fig_ex, use_container_width=True)
        else:
            st.info("Excluded ìƒ˜í”Œì´ ì—†ê±°ë‚˜ exclusion_reasonì´ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    else:
        st.info("Excluded ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

    # 4. Filterable Table
    st.subheader("ğŸ“‹ Master Catalog (Filterable & Sortable)")
    try:
        from st_aggrid import AgGrid, GridOptionsBuilder
        gb = GridOptionsBuilder.from_dataframe(df)
        gb.configure_default_column(groupable=True, sortable=True, filterable=True, editable=False)
        gb.configure_column("qc_status", rowGroup=True)
        gb.configure_column("ct_bin", rowGroup=True)
        grid_options = gb.build()
        AgGrid(df, gridOptions=grid_options, height=600, fit_columns_on_grid_load=True)
    except ImportError:
        st.warning("AgGrid not available. Using basic table.")
        st.dataframe(df, use_container_width=True)

    # 5. Download Buttons
    st.subheader("ğŸ’¾ Download Reports")
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            "Download Master Catalog (CSV)",
            df.to_csv(index=False).encode('utf-8'),
            "master_catalog_full.csv",
            "text/csv"
        )
    with col2:
        st.download_button(
            "Download Excluded Report (CSV)",
            excluded_df.to_csv(index=False).encode('utf-8'),
            "excluded_report.csv",
            "text/csv"
        )

    # 6. ì–´ë‘ìš´ í…Œë§ˆ (ê²€ì€ìƒ‰ ë°°ê²½)
    st.markdown("""
    <style>
        .css-1d391kg {background-color: #0e1117;}
        .css-1y0t9cy {color: white;}
        section[data-testid="stSidebar"] {background-color: #262730;}
        .css-1cpxl2t {color: white;}
        h1, h2, h3, h4 {color: white !important;}
    </style>
    """, unsafe_allow_html=True)

    st.stop()

ops = None
try:
    if parquet_path.exists():
        ops = pd.read_parquet(parquet_path)
        st.success(f"Loaded ops decisions (parquet): {parquet_path.name}")
    elif csv_path.exists():
        ops = pd.read_csv(csv_path, encoding="utf-8")
        st.success(f"Loaded ops decisions (csv): {csv_path.name}")
    else:
        st.warning(f"Ops decisions not found: {parquet_path} (or {csv_path})")
except Exception as e:
    st.error(f"Failed to load ops decisions: {e}")
    st.caption(f"Checked: {parquet_path} , {csv_path}")

@st.cache_data(show_spinner=False)
def scan_data_files() -> set[str]:
    base = PROJECT_ROOT / "data"   # <- ìƒëŒ€ê²½ë¡œ ë§ê³  PROJECT_ROOT ê¸°ì¤€ ì¶”ì²œ
    if not base.exists():
        return set()
    return {str(p.relative_to(PROJECT_ROOT)).replace("\\", "/")
            for p in base.rglob("*") if p.is_file()}

@st.cache_data(show_spinner=False)
def load_catalog() -> list[dict]:
    if not CATALOG_PATH.exists():
        return []
    data = json.loads(CATALOG_PATH.read_text(encoding="utf-8"))
    if isinstance(data, dict):
        return data.get("files", [])
    return data if isinstance(data, list) else []

available = scan_data_files()
catalog = load_catalog()

if not CATALOG_PATH.exists():
    st.warning(f"Catalog not found: {CATALOG_PATH}")
elif not catalog:
    st.warning("Catalog metadata is empty. Check assets/data_catalog.json")
else:
    rows = []
    for item in catalog:
        path = item.get("path", "")
        rows.append({
            "id": item.get("id", ""),
            "path": path,
            "available_on_cloud": path in available,
            "description": item.get("description", "")
        })
    st.dataframe(pd.DataFrame(rows), use_container_width=True)

# âœ… ì¤‘ë³µ decision_from_qc í•¨ìˆ˜ ì œê±° (í•˜ë‚˜ë§Œ ìœ ì§€)
def decision_from_qc(qc_status: str) -> str:
    """QC ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš´ì˜ ê²°ì •"""
    qc_status = str(qc_status).upper().strip()
    if qc_status == "PASS":
        return "PREDICT"
    if qc_status == "FLAG":
        return "WARN"
    return "RERUN"


# -------------------------
# Utilities
# -------------------------
def get_reports_root() -> Path:
    # 1) ê°€ì¥ ìš°ì„ : ë ˆí¬ ë£¨íŠ¸ì˜ reports/ (Streamlit Cloud ë°°í¬ìš©)
    p = Path("reports")
    if p.exists():
        return p

    # 2) (ë ˆê±°ì‹œ/ë¡œì»¬) app/data/reports ê°™ì€ ìœ„ì¹˜ë¥¼ ì“°ë˜ ê²½ìš° ëŒ€ë¹„
    p2 = Path(__file__).resolve().parent / "data" / "reports"
    if p2.exists():
        return p2

    # 3) ë§ˆì§€ë§‰ fallback
    return Path("reports")


REPORTS_ROOT = get_reports_root()


def has_canonical_master_long() -> bool:
    return (PROJECT_ROOT / "data" / "canonical" / "master_long.parquet").exists()


def running_on_streamlit_cloud() -> bool:
    # streamlit cloudëŠ” ë³´í†µ /mount/src ì•„ë˜ì—ì„œ ì‹¤í–‰ë¨
    return str(PROJECT_ROOT).startswith("/mount/src")

can_retrain = has_canonical_master_long() and (not running_on_streamlit_cloud())
if running_on_streamlit_cloud():
    st.warning(
        "Streamlit Cloudì—ëŠ” í•™ìŠµ ë°ì´í„°(data/canonical/master_long.parquet)ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ìš”.\n"
        "ì„œë²„/ë¡œì»¬ì—ì„œ í•™ìŠµì„ ëŒë ¤ reports/ ê²°ê³¼ë¬¼ì„ ì €ì¥(ë˜ëŠ” ì»¤ë°‹)í•œ ë’¤ Cloudì—ì„œëŠ” ê·¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ìš´ì˜í•˜ì„¸ìš”."
    )
elif not has_canonical_master_long():
    st.warning("í•™ìŠµ ë°ì´í„° master_long.parquetê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ìš”.")


def get_active_model_id() -> str:
    p = REPORTS_ROOT / "active_model.txt"
    mid = p.read_text().strip() if p.exists() else "model_server_latest_xgb"
    mid = Path(mid).name
    return mid


def _line_y_eq_x(df: pd.DataFrame):
    # y=x ë¼ì¸ ê·¸ë¦¬ê¸° ìœ„í•œ DataFrame
    if df.empty:
        return pd.DataFrame({"x":[0,1], "y":[0,1]})
    lo = float(min(df["true_ct"].min(), df["pred_ct"].min()))
    hi = float(max(df["true_ct"].max(), df["pred_ct"].max()))
    pad = (hi - lo) * 0.05 if hi > lo else 1.0
    lo -= pad; hi += pad
    return pd.DataFrame({"x":[lo, hi], "y":[lo, hi]})

def plot_pred_vs_true_facets(pred_long: pd.DataFrame, cutoffs: list[int], ncol: int = 4) -> None:
    import altair as alt

    df = pred_long.dropna(subset=["cutoff", "true_ct", "pred_ct"]).copy()
    df["cutoff"] = df["cutoff"].astype(int)
    df = df[df["cutoff"].isin([int(c) for c in cutoffs])].copy()
    if df.empty:
        st.info("ì„ íƒí•œ cutoffì— í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ì–´ìš”.")
        return

    # (1) ì‚°ì ë„
    base = alt.Chart(df).mark_circle(size=60, opacity=0.75).encode(
        x=alt.X("true_ct:Q", title="True Ct/Cq"),
        y=alt.Y("pred_ct:Q", title="Pred Ct/Cq"),
        tooltip=["run_id", "well_id", "cutoff", "true_ct", "pred_ct"],
    )

    # (2) y=x ëŒ€ê°ì„ : â˜… ê°™ì€ dfë¥¼ ì“°ë˜ transformìœ¼ë¡œ 2ì ì§œë¦¬ ë¼ì¸ ìƒì„±
    #     (cutoff facetë§ˆë‹¤ lo/hi ê³„ì‚°ë˜ë„ë¡ aggregate -> calculate -> fold)
    diag = (
        alt.Chart(df)
        .transform_aggregate(
            min_true="min(true_ct)",
            min_pred="min(pred_ct)",
            max_true="max(true_ct)",
            max_pred="max(pred_ct)",
            groupby=["cutoff"],  # facet ë‹¨ìœ„ë¡œ ê°ê° lo/hi ë§Œë“¤ê¸°
        )
        .transform_calculate(
            lo="datum.min_true < datum.min_pred ? datum.min_true : datum.min_pred",
            hi="datum.max_true > datum.max_pred ? datum.max_true : datum.max_pred",
        )
        .transform_fold(["lo", "hi"], as_=["k", "v"])
        .transform_calculate(x="datum.v", y="datum.v")
        .mark_line()
        .encode(x="x:Q", y="y:Q")
    )

    chart = alt.layer(diag, base).facet(
        facet=alt.Facet("cutoff:N", title=None),
        columns=ncol,
    ).resolve_scale(
        x="independent",
        y="independent",
    ).properties(
        title="Pred vs True across Cycle Cutoffs"
    )

    st.altair_chart(chart, use_container_width=True)

import re

def normalize_well(x: object) -> str:
    """
    B2, b2, ' B2 '  -> 'B02'
    D07 -> 'D07'
    """
    s = str(x).strip().upper()
    m = re.fullmatch(r"([A-H])\s*0*([0-9]{1,2})", s)
    if not m:
        return s
    row = m.group(1)
    col = int(m.group(2))
    return f"{row}{col:02d}"


import altair as alt

def perf_accuracy_fraction_vs_cutoff(pred: pd.DataFrame, tol: float = 2.0) -> pd.DataFrame:
    """
    |pred-true| <= tol ë¹„ìœ¨ì„ cutoffë³„ë¡œ ê³„ì‚°
    """
    df = pred.dropna(subset=["cutoff", "true_ct", "pred_ct"]).copy()
    df["abs_err"] = (df["pred_ct"] - df["true_ct"]).abs()
    out = df.groupby("cutoff").apply(lambda g: (g["abs_err"] <= tol).mean()).reset_index(name="acc_frac")
    out["cutoff"] = out["cutoff"].astype(int)
    return out.sort_values("cutoff")

def plot_error_by_true_ct_scatter(
    pred: pd.DataFrame,
    cutoff: int,
    tol: float = 2.0,
    bin_width: float = 2.0,
) -> None:
    """
    ì¹œì ˆ ë²„ì „ Bias Plot:
    - y=0 ê¸°ì¤€ì„  (ê³¼ëŒ€/ê³¼ì†Œì˜ˆì¸¡ ë°”ë¡œ í•´ì„)
    - Â±tol band (ì‹¤ë¬´ í—ˆìš© ì˜¤ì°¨ ëŒ€ì—­)
    - True Ct êµ¬ê°„(bin)ë³„ í‰ê·  error ë¼ì¸ (biasê°€ ì–´ë””ì„œ ìƒê¸°ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ)
    """
    import altair as alt

    df = pred[pred["cutoff"] == int(cutoff)].dropna(subset=["true_ct", "pred_ct"]).copy()
    if df.empty:
        st.info("í•´ë‹¹ cutoffì— scatterë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ì–´ìš”.")
        return

    df["err"] = df["pred_ct"] - df["true_ct"]

    # x-range ì¡ê¸°
    x_min = float(df["true_ct"].min())
    x_max = float(df["true_ct"].max())
    pad = (x_max - x_min) * 0.03 if x_max > x_min else 1.0
    x_min -= pad
    x_max += pad

    # (A) Â±tol band ë°ì´í„°
    band_df = pd.DataFrame(
        {"x": [x_min, x_max], "y1": [-float(tol), -float(tol)], "y2": [float(tol), float(tol)]}
    )

    # (B) binë³„ í‰ê·  error (bias ë¼ì¸)
    bw = float(bin_width)
    if bw <= 0:
        bw = 2.0

    tmp = df[["true_ct", "err"]].copy()
    tmp["bin"] = np.floor(tmp["true_ct"] / bw) * bw
    grp = (
        tmp.groupby("bin")
        .agg(mean_err=("err", "mean"), n=("err", "size"))
        .reset_index()
        .sort_values("bin")
    )
    grp["bin_center"] = grp["bin"] + bw / 2.0

    # -----------------
    # ì°¨íŠ¸ ë ˆì´ì–´ êµ¬ì„±
    # -----------------

    # 1) tol band (ì—°í•œ ì˜ì—­)
    band = (
        alt.Chart(band_df)
        .mark_area(opacity=0.12)
        .encode(
            x=alt.X("x:Q", title="True Ct/Cq"),
            y=alt.Y("y1:Q", title="Error (pred - true)"),
            y2="y2:Q",
            tooltip=[
                alt.Tooltip("y1:Q", title="-tol"),
                alt.Tooltip("y2:Q", title="+tol"),
            ],
        )
    )

    # 2) y=0 ê¸°ì¤€ì„ 
    zero_line = (
        alt.Chart(pd.DataFrame({"y": [0.0]}))
        .mark_rule(strokeDash=[6, 4], opacity=0.6)
        .encode(y="y:Q")
    )

    # 3) ì (ìƒ˜í”Œë³„ error)
    points = (
        alt.Chart(df)
        .mark_circle(size=55, opacity=0.65)
        .encode(
            x=alt.X("true_ct:Q", title="True Ct/Cq"),
            y=alt.Y("err:Q", title="Error (pred - true)"),
            tooltip=[
                alt.Tooltip("run_id:N", title="run_id"),
                alt.Tooltip("well_id:N", title="well_id"),
                alt.Tooltip("true_ct:Q", title="true"),
                alt.Tooltip("pred_ct:Q", title="pred"),
                alt.Tooltip("err:Q", title="err (pred-true)"),
            ],
        )
    )

    # 4) bin í‰ê·  bias ë¼ì¸ + í¬ì¸íŠ¸
    bias_line = (
        alt.Chart(grp)
        .mark_line(point=True, opacity=0.9)
        .encode(
            x=alt.X("bin_center:Q"),
            y=alt.Y("mean_err:Q"),
            tooltip=[
                alt.Tooltip("bin_center:Q", title="Ct bin center"),
                alt.Tooltip("mean_err:Q", title="mean err"),
                alt.Tooltip("n:Q", title="n"),
            ],
        )
    )

    chart = (
        alt.layer(band, zero_line, points, bias_line)
        .properties(height=340)
        .interactive()
    )

    st.altair_chart(chart, use_container_width=True)
    st.caption(
        f"í•´ì„ íŒ: y=0 ìœ„(+)=ê³¼ëŒ€ì˜ˆì¸¡(ëŠ¦ê²Œ ë‚˜ì˜¨ë‹¤ê³  íŒë‹¨), ì•„ë˜(-)=ê³¼ì†Œì˜ˆì¸¡(ë¹¨ë¦¬ ë‚˜ì˜¨ë‹¤ê³  íŒë‹¨). "
        f"ì—°í•œ ì˜ì—­ì€ Â±{float(tol):.1f} ì˜¤ì°¨ ëŒ€ì—­, êµµì€ ì„ ì€ Ct êµ¬ê°„(bin={float(bin_width):.1f})ë³„ í‰ê·  ì˜¤ì°¨(=bias)ì…ë‹ˆë‹¤."
    )


def plot_pred_vs_true_hard_colored(df_cut: pd.DataFrame, hard_ids: set[tuple[str, str]] | None = None,
                                  highlight: tuple[str, str] | None = None) -> None:
    import altair as alt

    df = df_cut.dropna(subset=["true_ct", "pred_ct"]).copy()
    if df.empty:
        st.info("scatterë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ì–´ìš”.")
        return

    # hard ì—¬ë¶€
    if hard_ids is None:
        df["group"] = "Inlier"
    else:
        df["group"] = df.apply(lambda r: "Hard" if (str(r["run_id"]), str(r["well_id"])) in hard_ids else "Inlier", axis=1)

    # ì„ íƒ ìƒ˜í”Œ ê°•ì¡°
    if highlight is not None:
        hr, hw = highlight
        df["is_selected"] = (df["run_id"].astype(str) == str(hr)) & (df["well_id"].astype(str) == str(hw))
        df.loc[df["is_selected"], "group"] = "Selected"
    else:
        df["is_selected"] = False

    base = alt.Chart(df).mark_circle(size=70, opacity=0.85).encode(
        x=alt.X("true_ct:Q", title="True Ct/Cq"),
        y=alt.Y("pred_ct:Q", title="Pred Ct/Cq"),
        color=alt.Color("group:N", title="Group"),
        tooltip=["run_id", "well_id", "true_ct", "pred_ct", "abs_err"],
    )

    # y=x ì„ : ê°™ì€ dfë¥¼ ì“°ëŠ” transform ë°©ì‹(Altair ì•ˆì „)
    diag = (
        alt.Chart(df)
        .transform_aggregate(
            min_true="min(true_ct)",
            min_pred="min(pred_ct)",
            max_true="max(true_ct)",
            max_pred="max(pred_ct)",
        )
        .transform_calculate(
            lo="datum.min_true < datum.min_pred ? datum.min_true : datum.min_pred",
            hi="datum.max_true > datum.max_pred ? datum.max_true : datum.max_pred",
        )
        .transform_fold(["lo", "hi"], as_=["k", "v"])
        .transform_calculate(x="datum.v", y="datum.v")
        .mark_line()
        .encode(x="x:Q", y="y:Q")
    )

    st.altair_chart(alt.layer(diag, base).properties(height=380, title="Hard Samples highlighted on Pred vs True"), use_container_width=True)

def plot_uploaded_curve_preview(df_long: pd.DataFrame, cutoff: int, max_wells: int = 6) -> None:
    """ì—…ë¡œë“œí•œ df_longì—ì„œ ëª‡ ê°œ wellë§Œ ë½‘ì•„ ê³¡ì„  preview (ë™ì )"""
    if df_long.empty:
        st.info("df_longì´ ë¹„ì–´ìˆì–´ìš”.")
        return

    wells = sorted(df_long["Well"].dropna().unique().tolist())[:max_wells]
    sub = df_long[df_long["Well"].isin(wells)].copy()
    sub["segment"] = np.where(sub["Cycle"] <= int(cutoff), "early(<=cutoff)", "late")

    chart = (
        alt.Chart(sub)
        .mark_line()
        .encode(
            x=alt.X("Cycle:Q", title="Cycle"),
            y=alt.Y("Fluor:Q", title="Fluor"),
            color=alt.Color("Well:N", legend=alt.Legend(title="Well")),
            tooltip=["Well", "Cycle", "Fluor", "segment"],
        )
        .properties(height=320)
        .interactive()
    )

    vline = (
        alt.Chart(pd.DataFrame({"Cycle": [int(cutoff)]}))
        .mark_rule(strokeDash=[6, 4])
        .encode(x="Cycle:Q")
    )

    st.altair_chart(chart + vline, use_container_width=True)
    st.caption(f"ë¯¸ë¦¬ë³´ê¸°: {len(wells)}ê°œ wellë§Œ í‘œì‹œ (ì „ì²´ {df_long['Well'].nunique()} wells ì¤‘)")

def plot_pred_ct_hist(pred_df: pd.DataFrame) -> None:
    """ì˜ˆì¸¡ Ct ë¶„í¬ íˆìŠ¤í† ê·¸ë¨(ë™ì )"""
    if pred_df.empty or "pred_ct" not in pred_df.columns:
        return

    hist = (
        alt.Chart(pred_df)
        .mark_bar()
        .encode(
            x=alt.X("pred_ct:Q", bin=alt.Bin(maxbins=25), title="Predicted Ct"),
            y=alt.Y("count():Q", title="Count"),
            tooltip=[alt.Tooltip("count():Q", title="count")],
        )
        .properties(height=280)
    )
    st.altair_chart(hist, use_container_width=True)

def plot_cv_vs_ct(df_long: pd.DataFrame, pred_df: pd.DataFrame, cutoff: int) -> None:
    """
    ê°„ë‹¨í•œ í’ˆì§ˆì§€í‘œ(CV) vs Ct (ë™ì )
    - early êµ¬ê°„(<=cutoff)ì—ì„œ Fluorì˜ CV(std/mean)ë¥¼ ê³„ì‚°í•´ì„œ pred_ctì™€ ì—°ê²°
    """
    if df_long.empty or pred_df.empty:
        return

    early = df_long[df_long["Cycle"] <= int(cutoff)].copy()
    g = early.groupby(["run_id", "Well"])["Fluor"]
    cv = (g.std() / (g.mean().replace(0, np.nan))).reset_index()
    cv.rename(columns={"Fluor": "cv_early"}, inplace=True)

    m = pred_df.merge(cv, on=["run_id", "Well"], how="left")
    m = m.dropna(subset=["pred_ct", "cv_early"]).copy()
    if m.empty:
        st.info("CV vs Ctë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ë¶€ì¡±í•´ìš”.")
        return

    scat = (
        alt.Chart(m)
        .mark_circle(size=60)
        .encode(
            x=alt.X("pred_ct:Q", title="Predicted Ct"),
            y=alt.Y("cv_early:Q", title="CV (early <= cutoff)"),
            tooltip=["Well", "pred_ct", "cv_early"],
        )
        .properties(height=300)
        .interactive()
    )
    st.altair_chart(scat, use_container_width=True)
    st.caption("CVëŠ” early êµ¬ê°„ Fluorì˜ std/mean ê¸°ë°˜(ê°„ë‹¨ ë²„ì „).")


def get_best_cutoff_from_report() -> int | None:
    """train_report.csvì—ì„œ mae_test(ë˜ëŠ” mae) ìµœì†Œ cutoffë¥¼ ë°˜í™˜"""
    report_path = REPORTS_ROOT / "train_report.csv"
    if not report_path.exists():
        return None

    rep = pd.read_csv(report_path)
    cols = {str(c).lower(): c for c in rep.columns}
    cutoff_col = cols.get("cutoff")
    mae_col = cols.get("mae") or cols.get("mae_test")

    if not cutoff_col or not mae_col or rep.empty:
        return None

    best_row = rep.loc[rep[mae_col].idxmin()]
    return int(best_row[cutoff_col])

def _safe_stem(name: str) -> str:
    s = Path(name).stem
    s = re.sub(r"[^a-zA-Z0-9_\-]+", "_", s)
    return s[:80] if s else "uploaded"


def discover_cutoffs(models_dir: Path) -> list[int]:
    cutoffs: list[int] = []
    for p in models_dir.glob("ct_xgb_cutoff_*.json"):
        m = re.search(r"cutoff_(\d+)\.json$", p.name)
        if m:
            cutoffs.append(int(m.group(1)))
    return sorted(set(cutoffs))


@st.cache_resource
def load_booster(cutoff: int) -> xgb.Booster:
    model_path = MODELS_DIR / f"ct_xgb_cutoff_{cutoff}.json"
    booster = xgb.Booster()
    booster.load_model(str(model_path))
    return booster


def load_meta(cutoff: int) -> dict:
    meta_path = MODELS_DIR / f"ct_xgb_cutoff_{cutoff}.meta.json"
    if meta_path.exists():
        return json.loads(meta_path.read_text(encoding="utf-8"))
    return {}


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    return df.drop(columns=cols) if cols else df


def infer_long_df(df: pd.DataFrame, run_id: str) -> pd.DataFrame:
    """
    ì—…ë¡œë“œ í…Œì´ë¸”ì„ ìµœëŒ€í•œ ê´€ëŒ€í•˜ê²Œ long í˜•íƒœë¡œ ë³€í™˜.
    ìµœì¢… ë°˜í™˜ ì»¬ëŸ¼: Cycle, Fluor, Well, run_id, well_uid

    ì§€ì› í¬ë§·:
      A) long: (Well, Cycle, Fluor/RFU/Signal)
      B) wide-1: (Well + cycle columns "1","2",... ë˜ëŠ” "Cycle 1"...)
      C) wide-2: (Cycle + well columns "C3","C5","A01"... )  <-- ë„ˆê°€ ë§í•œ ì—‘ì…€ í˜•íƒœ
    """
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    df = _drop_unnamed(df)

    cols_lower = {str(c).strip().lower(): c for c in df.columns}
    has_cycle = "cycle" in cols_lower

    # ---- A) long í˜•íƒœ: Cycle + (Fluor/RFU/Signal)
    fluor_key = None
    for k in ("fluor", "rfu", "signal"):
        if k in cols_lower:
            fluor_key = k
            break

    if has_cycle and fluor_key is not None:
        cycle_col = cols_lower["cycle"]
        fluor_col = cols_lower[fluor_key]
        well_col = (
            cols_lower.get("well")
            or cols_lower.get("well position")
            or cols_lower.get("well_position")
        )

        if well_col is None:
            # Wellì´ ì—†ìœ¼ë©´ í–‰ ë²ˆí˜¸ë¡œ ì„ì‹œ well ë¶€ì—¬
            df["Well"] = [f"R{i:03d}" for i in range(1, len(df) + 1)]
            well_col = "Well"

        out = df[[well_col, cycle_col, fluor_col]].copy()
        out.columns = ["Well", "Cycle", "Fluor"]

    # ---- C) Cycle + ì—¬ëŸ¬ well ì»¬ëŸ¼ (Cycleì´ í–‰)
    elif has_cycle:
        cycle_col = cols_lower["cycle"]
        well_cols = [c for c in df.columns if c != cycle_col]
        if not well_cols:
            raise ValueError("Cycle ì»¬ëŸ¼ì€ ìˆëŠ”ë° well ì»¬ëŸ¼(C3, C5, A01 ë“±)ì´ ì—†ì–´.")

        long = df.melt(
            id_vars=[cycle_col],
            value_vars=well_cols,
            var_name="Well",
            value_name="Fluor",
        )
        long.rename(columns={cycle_col: "Cycle"}, inplace=True)
        out = long[["Well", "Cycle", "Fluor"]].copy()

    # ---- B) Well + cycle ì»¬ëŸ¼ë“¤ (wide)
    else:
        well_col = None
        for cand in ["Well", "well", "WELL"]:
            if cand in df.columns:
                well_col = cand
                break
        if well_col is None:
            raise ValueError("Well ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆì–´. (ì˜ˆ: Well, well)")

        cycle_cols: list[str] = []
        for c in df.columns:
            if c == well_col:
                continue
            if re.fullmatch(r"\d+", str(c).strip()):
                cycle_cols.append(c)
            elif re.search(r"cycle\s*\d+", str(c).strip(), flags=re.IGNORECASE):
                cycle_cols.append(c)

        if not cycle_cols:
            raise ValueError("longë„ ì•„ë‹ˆê³  wide(Well+cycle cols)ë„ ì•„ë‹Œ ê²ƒ ê°™ì•„. (Cycle+well colsë„ ì•„ë‹˜)")

        tmp = df[[well_col] + cycle_cols].copy()
        long = tmp.melt(id_vars=[well_col], var_name="Cycle", value_name="Fluor")
        long["Cycle"] = long["Cycle"].astype(str).str.extract(r"(\d+)").astype(int)
        long.rename(columns={well_col: "Well"}, inplace=True)
        out = long[["Well", "Cycle", "Fluor"]].copy()

    # ---- ì •ë¦¬
    out = out.dropna(subset=["Well", "Cycle", "Fluor"]).copy()
    out["Cycle"] = pd.to_numeric(out["Cycle"], errors="coerce")
    out["Fluor"] = pd.to_numeric(out["Fluor"], errors="coerce")
    out = out.dropna(subset=["Cycle", "Fluor"]).copy()

    out["Cycle"] = out["Cycle"].astype(int)
    out["Well"] = out["Well"].astype(str).str.strip()

    out["run_id"] = run_id
    out["well_uid"] = out["run_id"].astype(str) + ":" + out["Well"].astype(str)

    return out[["Cycle", "Fluor", "Well", "run_id", "well_uid"]]


def predict_ct(df_long: pd.DataFrame, cutoff: int) -> pd.DataFrame:
    booster = load_booster(cutoff)
    X, meta = build_x_from_long(df_long, cutoff=cutoff)

    # feature_names mismatch ë°©ì§€ (metaì— ìˆìœ¼ë©´ ì‚¬ìš©)
    m = load_meta(cutoff)
    feat_cols = m.get("feat_cols") or m.get("feature_cols")
    if feat_cols:
        dmat = xgb.DMatrix(X, feature_names=list(feat_cols))
    else:
        dmat = xgb.DMatrix(X)

    pred = booster.predict(dmat)

    out = meta.copy()
    out["pred_ct"] = pred.astype(float)
    out["cutoff_used"] = cutoff
    return out.sort_values(["run_id", "Well"]).reset_index(drop=True)


def run_retrain(min_cutoff: int, max_cutoff: int) -> tuple[int, str]:
    if running_on_streamlit_cloud():
        return 2, "Streamlit Cloudì—ì„œëŠ” canonical ë°ì´í„°ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„œë²„/ë¡œì»¬ì—ì„œ í•™ìŠµ í›„ reports/ë§Œ ë°°í¬í•˜ì„¸ìš”."
    """
    í˜„ì¬ ì„œë²„ì— ìˆëŠ” canonical/master_long.parquet ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì „ì²´ ì¬í•™ìŠµ.
    (Streamlit ë²„íŠ¼ì—ì„œë„ GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ env ì „ë‹¬)
    """
    cmd = [
        sys.executable,
        "-m",
        "core.step3_train_and_save_models",
        "--min_cutoff",
        str(min_cutoff),
        "--max_cutoff",
        str(max_cutoff),
    ]

    env = dict(os.environ)

    # âœ… GPUë¥¼ ì“°ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨ (ì˜ˆ: 1ë²ˆ GPU ê³ ì •)
    env.setdefault("CUDA_VISIBLE_DEVICES", "1")

    p = subprocess.run(
        cmd,
        cwd=str(PROJECT_ROOT),
        capture_output=True,
        text=True,
        env=env,
    )
    log = (p.stdout or "") + "\n" + (p.stderr or "")
    return p.returncode, log

def split_excel_sheets(obj):
    """
    objê°€ dict(sheet_name -> df)ì¼ ë•Œ
    - curve_df: Cycle ì»¬ëŸ¼ ìˆëŠ” ì‹œíŠ¸(ìš°ì„  SYBR)
    - truth_df: Well + (Cq/Ct/true_ct) ìˆëŠ” ì‹œíŠ¸(ìš°ì„  Sheet1)
    """
    if not isinstance(obj, dict):
        return obj, None, None, None  # (curve_df, truth_df, curve_sheet, truth_sheet)

    # í›„ë³´ ìš°ì„ ìˆœìœ„
    curve_priority = ["SYBR", "Amplification", "Data", "Raw"]
    truth_priority = ["Sheet1", "Ct", "Cq", "Truth", "Result"]

    def norm_cols(df):
        return [str(c).strip().lower() for c in df.columns]

    # 1) curve sheet ì°¾ê¸°
    curve_df = None
    curve_sheet = None

    for nm in curve_priority:
        if nm in obj:
            cols = norm_cols(obj[nm])
            if "cycle" in cols:
                curve_df = obj[nm]
                curve_sheet = nm
                break

    if curve_df is None:
        for nm, df in obj.items():
            cols = norm_cols(df)
            if "cycle" in cols:
                curve_df = df
                curve_sheet = nm
                break

    # fallback: ì²« ì‹œíŠ¸
    if curve_df is None:
        curve_sheet = next(iter(obj.keys()))
        curve_df = obj[curve_sheet]

    # 2) truth sheet ì°¾ê¸°
    truth_df = None
    truth_sheet = None
    truth_keys = {"cq", "ct", "true_ct", "truect"}

    for nm in truth_priority:
        if nm in obj:
            cols = set(norm_cols(obj[nm]))
            if "well" in cols and len(cols & truth_keys) > 0:
                truth_df = obj[nm]
                truth_sheet = nm
                break

    if truth_df is None:
        for nm, df in obj.items():
            cols = set(norm_cols(df))
            if "well" in cols and len(cols & truth_keys) > 0:
                truth_df = df
                truth_sheet = nm
                break

    return curve_df, truth_df, curve_sheet, truth_sheet


def read_uploaded_table(up):
    name = (up.name or "").lower()
    raw = up.getvalue() if hasattr(up, "getvalue") else up.read()
    buf = io.BytesIO(raw)

    if name.endswith((".xlsx", ".xls")):
        # âœ… ëª¨ë“  ì‹œíŠ¸ ì½ê¸° (dict[str, DataFrame])
        return pd.read_excel(buf, sheet_name=None)
    return pd.read_csv(buf)


def sync_train_report_to_parquet(rep: pd.DataFrame) -> str:
    """
    train_report.csv(rep)ë¥¼ Performance í˜ì´ì§€ê°€ ì½ëŠ” parquetë¡œ ì €ì¥í•œë‹¤.

    ì €ì¥ ìœ„ì¹˜:
      <repo>/reports/<model_id>/metrics_by_cutoff.parquet
    ê·¸ë¦¬ê³ :
      <repo>/reports/active_model.txt ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
    """
    model_id = "model_server_latest_xgb"

    outdir = REPORTS_ROOT / model_id
    outdir.mkdir(parents=True, exist_ok=True)
    (REPORTS_ROOT / "active_model.txt").write_text(model_id, encoding="utf-8")
    cols = {str(c).lower(): c for c in rep.columns}
    cutoff_col = cols.get("cutoff")
    mae_col = cols.get("mae") or cols.get("mae_test")
    rmse_col = cols.get("rmse") or cols.get("rmse_test")

    if not (cutoff_col and mae_col and rmse_col):
        print("Missing cols:", {"cutoff": cutoff_col, "mae": mae_col, "rmse": rmse_col})
        print("Available:", list(rep.columns))
        return model_id

    rep2 = rep[[cutoff_col, mae_col, rmse_col]].copy()
    rep2 = rep2.rename(columns={cutoff_col: "cutoff", mae_col: "mae_test", rmse_col: "rmse_test"})

    # optional extras
    for extra in ["n_curves", "n_runs"]:
        if extra in cols:
            rep2[extra] = rep[cols[extra]].values

    rep2.to_parquet(outdir / "metrics_by_cutoff.parquet", index=False)

    (PROJECT_ROOT / "reports").mkdir(exist_ok=True)
    (PROJECT_ROOT / "reports" / "active_model.txt").write_text(model_id, encoding="utf-8")

    return model_id

def show_train_report() -> None:
    st.subheader("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ (ì„œë²„ í•™ìŠµ ê¸°ì¤€)")
    report_path = REPORTS_ROOT / "train_report.csv"
    if not report_path.exists():
        st.info("train_report.csvê°€ ì•„ì§ ì—†ì–´ìš”. ì¬í•™ìŠµ ì‹¤í–‰ í›„ ìƒì„±ë©ë‹ˆë‹¤.")
        return

    rep = pd.read_csv(report_path)

    # âœ… colsë¥¼ ë¨¼ì € ë§Œë“ ë‹¤ (ì—¬ê¸°ê°€ í•µì‹¬)
    cols = {str(c).lower(): c for c in rep.columns}
    cutoff_col = cols.get("cutoff")
    mae_col = cols.get("mae") or cols.get("mae_test")
    rmse_col = cols.get("rmse") or cols.get("rmse_test")
    ncurves_col = cols.get("n_curves")

    # Performance í˜ì´ì§€ìš© parquet ì €ì¥
    mid = sync_train_report_to_parquet(rep)
    st.caption(f"âœ… Performanceìš© ë¦¬í¬íŠ¸ ì €ì¥: reports/{mid}/metrics_by_cutoff.parquet")

    # âœ… ì¶”ì²œ cutoff ì¹´ë“œ (cols ë§Œë“  ë’¤ì—!)
    if cutoff_col and mae_col and rmse_col:
        best_row = rep.loc[rep[mae_col].idxmin()]
        c1, c2, c3 = st.columns(3)
        c1.metric("âœ… ì¶”ì²œ cutoff (MAE ìµœì†Œ)", int(best_row[cutoff_col]))
        c2.metric("ìµœì†Œ MAE", round(float(best_row[mae_col]), 4))
        c3.metric("í•´ë‹¹ RMSE", round(float(best_row[rmse_col]), 4))
        st.divider()

    # =========================
    # Figure ì¤‘ì‹¬ Performance
    # =========================
    import altair as alt

    # ë³´ê¸° ì˜µì…˜
    show_table = st.toggle("í‘œ(ì›ë³¸ rep)ë„ ê°™ì´ ë³´ê¸°", value=False)

    # ì»¬ëŸ¼ ì •ê·œí™”í•´ì„œ ì“°ê¸° í¸í•˜ê²Œ
    rep2 = rep.copy()
    rep2 = rep2.rename(columns={
        cutoff_col: "cutoff",
        mae_col: "mae",
        rmse_col: "rmse",
    })
    if ncurves_col:
        rep2 = rep2.rename(columns={ncurves_col: "n_curves"})
    if "n_runs" in cols:
        rep2 = rep2.rename(columns={cols["n_runs"]: "n_runs"})

    rep2["cutoff"] = pd.to_numeric(rep2["cutoff"], errors="coerce")
    rep2["mae"] = pd.to_numeric(rep2["mae"], errors="coerce")
    rep2["rmse"] = pd.to_numeric(rep2["rmse"], errors="coerce")
    if "n_curves" in rep2.columns:
        rep2["n_curves"] = pd.to_numeric(rep2["n_curves"], errors="coerce")
    if "n_runs" in rep2.columns:
        rep2["n_runs"] = pd.to_numeric(rep2["n_runs"], errors="coerce")

    rep2 = rep2.dropna(subset=["cutoff", "mae", "rmse"]).sort_values("cutoff").reset_index(drop=True)

    # ---- ìš”ì•½ ì¹´ë“œ(ì´ë¯¸ ìœ„ì—ì„œ metric ì°ê³  ìˆì–´ë„, ì—¬ê¸°ì„œ ë” ì§ê´€ì ìœ¼ë¡œ ë³´ê°• ê°€ëŠ¥) ----
    best_i = int(rep2["mae"].idxmin())
    best_row2 = rep2.loc[best_i]

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("âœ… Best cutoff (MAE ìµœì†Œ)", int(best_row2["cutoff"]))
    c2.metric("ìµœì†Œ MAE", round(float(best_row2["mae"]), 4))
    c3.metric("í•´ë‹¹ RMSE", round(float(best_row2["rmse"]), 4))
    if "n_curves" in rep2.columns and pd.notna(best_row2.get("n_curves", np.nan)):
        c4.metric("n_curves", int(best_row2["n_curves"]))
    elif "n_runs" in rep2.columns and pd.notna(best_row2.get("n_runs", np.nan)):
        c4.metric("n_runs", int(best_row2["n_runs"]))
    else:
        c4.metric("rows", int(len(rep2)))

    st.divider()

    # ---- (1) MAE / RMSE vs Cutoff: ì¸í„°ë™í‹°ë¸Œ ë¼ì¸ ----
    metric_choice = st.radio(
        "ë³´ê¸° ì„ íƒ",
        ["MAE vs Cutoff", "RMSE vs Cutoff", "MAE+RMSE ë‘˜ ë‹¤(ê²¹ì³ë³´ê¸°)"],
        horizontal=True,
    )

    base = alt.Chart(rep2).encode(
        x=alt.X("cutoff:Q", title="Cutoff"),
    )

    hover = alt.selection_point(fields=["cutoff"], on="mouseover", nearest=True, empty=False)

    def line_with_points(ycol: str, title: str):
        line = base.mark_line().encode(
            y=alt.Y(f"{ycol}:Q", title=title),
            tooltip=[alt.Tooltip("cutoff:Q", title="cutoff"), alt.Tooltip(f"{ycol}:Q", title=title)],
        )
        pts = base.mark_circle(size=70).encode(
            y=alt.Y(f"{ycol}:Q"),
            opacity=alt.condition(hover, alt.value(1.0), alt.value(0.15)),
            tooltip=[alt.Tooltip("cutoff:Q", title="cutoff"), alt.Tooltip(f"{ycol}:Q", title=title)],
        ).add_params(hover)

        vline = alt.Chart(rep2).mark_rule(strokeDash=[6, 4]).encode(
            x="cutoff:Q",
            opacity=alt.condition(hover, alt.value(0.6), alt.value(0.0)),
        ).transform_filter(hover)

        return (line + pts + vline).properties(height=320)

    if metric_choice == "MAE vs Cutoff":
        st.altair_chart(line_with_points("mae", "MAE"), use_container_width=True)

    elif metric_choice == "RMSE vs Cutoff":
        st.altair_chart(line_with_points("rmse", "RMSE"), use_container_width=True)

    else:
        # ê²¹ì³ë³´ê¸°(ë¡± í¬ë§·ìœ¼ë¡œ ë³€í™˜)
        longm = rep2.melt(id_vars=["cutoff"], value_vars=["mae", "rmse"], var_name="metric", value_name="value")
        longm["metric"] = longm["metric"].map({"mae": "MAE", "rmse": "RMSE"})

        hover2 = alt.selection_point(fields=["cutoff"], on="mouseover", nearest=True, empty=False)

        chart = alt.Chart(longm).encode(
            x=alt.X("cutoff:Q", title="Cutoff"),
            y=alt.Y("value:Q", title="Error"),
            tooltip=[
                alt.Tooltip("cutoff:Q", title="cutoff"),
                alt.Tooltip("metric:N", title="metric"),
                alt.Tooltip("value:Q", title="value"),
            ],
            strokeDash="metric:N",
        )

        line = chart.mark_line()
        pts = chart.mark_circle(size=70).encode(
            opacity=alt.condition(hover2, alt.value(1.0), alt.value(0.15)),
        ).add_params(hover2)

        vline = alt.Chart(rep2).mark_rule(strokeDash=[6, 4]).encode(
            x="cutoff:Q",
            opacity=alt.condition(hover2, alt.value(0.6), alt.value(0.0)),
        ).transform_filter(hover2)

        st.altair_chart((line + pts + vline).properties(height=320), use_container_width=True)

    st.divider()

    # ---- (2) n_curves vs Cutoff (ìˆì„ ë•Œë§Œ) ----
    if "n_curves" in rep2.columns and rep2["n_curves"].notna().any():
        st.markdown("#### ğŸ“¦ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ (#Curves vs Cutoff)")
        cov = alt.Chart(rep2).mark_line().encode(
            x=alt.X("cutoff:Q", title="Cutoff"),
            y=alt.Y("n_curves:Q", title="#Curves"),
            tooltip=[alt.Tooltip("cutoff:Q"), alt.Tooltip("n_curves:Q")],
        ).properties(height=220)
        st.altair_chart(cov, use_container_width=True)

    # =========================
    # (ì¶”ê°€) predictions_long ê¸°ë°˜ ë™ì  ì„±ëŠ¥ figure
    # =========================
    model_id = get_active_model_id()
    
    pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"


    if pred_path.exists():
        pred_long = pd.read_parquet(pred_path)

        st.markdown("### ğŸ“Œ ì¶”ê°€ ì„±ëŠ¥ Figure (ì„œë²„ í‰ê°€ ë¡œê·¸ ê¸°ë°˜)")

        tol = st.slider("ì •í™•ë„ ê¸°ì¤€ |error| <= ?", 0.5, 5.0, 2.0, 0.5, key="perf_tol")
        acc_df = perf_accuracy_fraction_vs_cutoff(pred_long, tol=float(tol))

        acc_chart = (
            alt.Chart(acc_df)
            .mark_line(point=True)
            .encode(
                x=alt.X("cutoff:Q", title="Cutoff"),
                y=alt.Y("acc_frac:Q", title=f"Accuracy Fraction (|err|<= {tol})"),
                tooltip=["cutoff", "acc_frac"],
            )
            .properties(height=260)
        )
        st.altair_chart(acc_chart, use_container_width=True)
        
        # =========================
        # (NEW) Pred vs True (Cutoff stepë³„ Small Multiples)
        # =========================
        st.markdown("### ğŸ“Œ Pred vs True (Cutoff stepë³„ Small Multiples)")

        step = st.radio("cutoff step", [3, 5], horizontal=True, key="pvst_step")
        cmin = int(pred_long["cutoff"].min())
        cmax = int(pred_long["cutoff"].max())
        rng = st.slider("cutoff ë²”ìœ„", min_value=cmin, max_value=cmax, value=(cmin, cmax), step=1, key="pvst_rng")
        cols_per_row = st.slider("í•œ ì¤„ì— ëª‡ ê°œ?", 2, 6, 4, 1, key="pvst_cols")

        cut_list = [c for c in range(rng[0], rng[1] + 1) if (c - rng[0]) % int(step) == 0]
        plot_pred_vs_true_facets(pred_long, cut_list, ncol=int(cols_per_row))

        st.divider()

        cutoff_sel = st.selectbox(
            "Scatter ë³¼ cutoff ì„ íƒ",
            sorted(pred_long["cutoff"].dropna().unique().astype(int).tolist()),
            key="perf_cutoff_sel",
        )
        st.markdown("#### Error vs True Ct (Bias í™•ì¸)")
        # âœ… ì¹œì ˆ ë²„ì „ ì˜µì…˜ (í‚¤ ì¤‘ë³µ ë°©ì§€ìš©ìœ¼ë¡œ perf_ prefix)
        bias_tol = st.slider("Bias í—ˆìš© ëŒ€ì—­(Â±tol)", 0.5, 5.0, 2.0, 0.5, key="perf_bias_tol")
        bias_binw = st.slider("Ct bin í­(í‰ê·  bias ê³„ì‚°)", 1.0, 6.0, 2.0, 0.5, key="perf_bias_binw")
        plot_error_by_true_ct_scatter(
            pred_long,
            cutoff=int(cutoff_sel),
            tol=float(bias_tol),
            bin_width=float(bias_binw),
        )

        dfc = pred_long[pred_long["cutoff"] == int(cutoff_sel)].dropna(subset=["true_ct", "pred_ct"]).copy()
        dfc["abs_err"] = (dfc["pred_ct"] - dfc["true_ct"]).abs()
        hist = (
            alt.Chart(dfc)
            .mark_bar()
            .encode(
                x=alt.X("abs_err:Q", bin=alt.Bin(maxbins=30), title="|Error|"),
                y=alt.Y("count():Q", title="Count"),
                tooltip=[alt.Tooltip("count():Q", title="count")],
            )
            .properties(height=240)
        )
        st.altair_chart(hist, use_container_width=True)

    else:
        st.info("ì¶”ê°€ figureë¥¼ ê·¸ë¦¬ë ¤ë©´ predictions_long.parquetê°€ í•„ìš”í•´ìš”. (Retrain í›„ ìƒì„±ë˜ëŠ” íŒŒì¼)")

    # ---- (ì˜µì…˜) í‘œ ----
    if show_table:
        st.markdown("#### ì›ë³¸ í…Œì´ë¸”")
        st.dataframe(rep, use_container_width=True)

def try_eval_if_truth_exists(df_raw: pd.DataFrame, pred_df: pd.DataFrame, truth_df: pd.DataFrame | None = None) -> None:
    # âœ… truth_dfê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„ ìœ¼ë¡œ í‰ê°€
    src = truth_df if truth_df is not None else df_raw

    true_col = None
    for cand in ["true_ct", "TrueCt", "trueCt", "ct", "Ct", "CT", "Cq", "cq", "CQ"]:
        if cand in src.columns:
            true_col = cand
            break

    if true_col is None:
        st.info("ì—…ë¡œë“œ íŒŒì¼ì— ì •ë‹µ Ct/Cq ì»¬ëŸ¼(true_ct/ct/cq ë“±)ì´ ì—†ì–´ì„œ ì¦‰ì„ í‰ê°€ëŠ” ìƒëµí–ˆì–´ìš”.")
        return

    well_key = None
    for w in ["Well", "well", "WELL"]:
        if w in src.columns:
            well_key = w
            break

    eval_df = pred_df.copy()

    if well_key is not None and "Well" in eval_df.columns:
        truth2 = src[[well_key, true_col]].copy()
        truth2.columns = ["Well", "true_ct"]
        
        # âœ… Well í‘œì¤€í™” (í•µì‹¬)
        truth2["Well"] = truth2["Well"].map(normalize_well)
        eval_df["Well"] = eval_df["Well"].map(normalize_well)
        
        eval_df = eval_df.merge(truth2, on="Well", how="left")

    else:
        eval_df["true_ct"] = pd.to_numeric(src[true_col], errors="coerce").values[: len(eval_df)]

    eval_df["true_ct"] = pd.to_numeric(eval_df["true_ct"], errors="coerce")
    eval_df = eval_df.dropna(subset=["true_ct", "pred_ct"]).copy()
    if len(eval_df) == 0:
        st.warning("ì •ë‹µ Ct ì»¬ëŸ¼ì€ ì°¾ì•˜ëŠ”ë°, predì™€ ë§¤ì¹­ëœ ê°’ì´ ì—†ì–´ìš”. Well ì´ë¦„ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì¤˜.")
        return

    eval_df["err"] = eval_df["pred_ct"] - eval_df["true_ct"]
    mae = float(np.mean(np.abs(eval_df["err"])))
    rmse = float(np.sqrt(np.mean(eval_df["err"] ** 2)))

    st.markdown("### âœ… ì—…ë¡œë“œ ë°ì´í„° ì¦‰ì„ í‰ê°€")
    st.write({"MAE": mae, "RMSE": rmse, "n": int(len(eval_df))})

    st.markdown("**Pred vs True (ì‚°ì ë„)**")
    st.scatter_chart(eval_df[["true_ct", "pred_ct"]], x="true_ct", y="pred_ct", height=320)

    st.markdown("**Residual(ì˜¤ì°¨) ë¶„í¬**")
    # value_countsëŠ” íˆìŠ¤í† ê·¸ë¨ ëŠë‚Œì´ ì•½í•´ì„œ, binsë¡œ ê°€ë³ê²Œ
    bins = np.linspace(eval_df["err"].min(), eval_df["err"].max(), 30) if len(eval_df) > 3 else None
    if bins is not None:
        hist, edges = np.histogram(eval_df["err"].values, bins=bins)
        hist_df = pd.DataFrame({"err_bin_left": edges[:-1], "count": hist}).set_index("err_bin_left")
        st.line_chart(hist_df["count"], height=220)
    else:
        st.line_chart(eval_df["err"], height=220)

def load_curve_from_master(run_id: str, well_id: str) -> pd.DataFrame:
    """
    canonical master_long.parquetì—ì„œ (run_id, well_id) í•œ ê³¡ì„ ë§Œ ë¡œë“œ
    í•„ìš”í•œ ì»¬ëŸ¼ëª…ì´ í™˜ê²½ë§ˆë‹¤ ë‹¬ë¼ì„œ ìœ ì—°í•˜ê²Œ ë§¤í•‘í•¨.
    """
    path = PROJECT_ROOT / "data" / "canonical" / "master_long.parquet"
    if not path.exists():
        raise FileNotFoundError(f"master_long.parquet not found: {path}")

    dataset = ds.dataset(str(path))

    # ì»¬ëŸ¼ í›„ë³´ë“¤(í”„ë¡œì íŠ¸ë§ˆë‹¤ ì´ë¦„ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ)
    cols = set(dataset.schema.names)
    cycle_col = "Cycle" if "Cycle" in cols else ("cycle" if "cycle" in cols else None)
    fluor_col = "Fluor" if "Fluor" in cols else ("fluor" if "fluor" in cols else None)
    run_col   = "run_id" if "run_id" in cols else None
    
    # âœ… ì—¬ê¸°ê°€ í•µì‹¬: Wellì´ ìˆìœ¼ë©´ Wellì„ ìµœìš°ì„ ìœ¼ë¡œ
    if "Well" in cols:
        well_col = "Well"
    elif "well_id" in cols:
        well_col = "well_id"
    elif "well_uid" in cols:
        well_col = "well_uid"
    else:
        well_col = None
    
    if not all([cycle_col, fluor_col, run_col, well_col]):
        raise ValueError(f"master_long columns unexpected. found={sorted(cols)[:50]} ...")
    
    # âœ… well_uidë¥¼ ì“°ëŠ” ê²½ìš°ì—ëŠ” run_id:Well í˜•íƒœë¡œ ë§ì¶°ì¤Œ
    well_value = well_id
    if well_col == "well_uid":
        well_value = f"{run_id}:{well_id}"
    
    filt = (ds.field(run_col) == run_id) & (ds.field(well_col) == well_value)
    table = dataset.to_table(filter=filt, columns=[run_col, well_col, cycle_col, fluor_col])
    df = table.to_pandas()
    
    df = df.rename(columns={cycle_col: "Cycle", fluor_col: "Fluor"})
    df = df.sort_values("Cycle").reset_index(drop=True)
    return df


def load_one_curve_from_predictions_row(row) -> pd.DataFrame:
    """
    predictions_long.parquet rowì— curve_cycles_json / curve_fluor_json ì´ ìˆìœ¼ë©´
    ê·¸ê±¸ë¡œ ì›ë³¸ ê³¡ì„ ì„ ë³µì›í•œë‹¤ (Streamlit Cloud fallback).
    Returns: DataFrame with columns ["Cycle","Fluor"] sorted by Cycle
    """
    import json
    cycles_json = row.get("curve_cycles_json", "") if isinstance(row, dict) else getattr(row, "curve_cycles_json", "")
    fluor_json  = row.get("curve_fluor_json", "")  if isinstance(row, dict) else getattr(row, "curve_fluor_json", "")

    if not cycles_json or not fluor_json:
        raise ValueError("curve_cycles_json / curve_fluor_json is empty (retrain on server with curve embedding).")

    cycles = json.loads(cycles_json)
    fluor  = json.loads(fluor_json)

    df = pd.DataFrame({"Cycle": cycles, "Fluor": fluor})
    df = df.dropna().sort_values("Cycle").reset_index(drop=True)
    return df

def show_hard_review() -> None:
    st.subheader("ğŸ§¨ Hard Sample Review (ì„œë²„ í‰ê°€ ë¡œê·¸ ê¸°ë°˜)")

    # active model id
    model_id = get_active_model_id()

    pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"

    if not pred_path.exists():
        st.info(f"predictions_long.parquetê°€ ì—†ì–´ìš”: {pred_path}\nì¬í•™ìŠµì„ í•œ ë²ˆ ì‹¤í–‰í•´ì„œ ìƒì„±í•´ì¤˜.")
        return

    pred = pd.read_parquet(pred_path)
    need_cols = {"run_id", "well_id", "cutoff", "true_ct", "pred_ct"}
    if not need_cols.issubset(set(pred.columns)):
        st.error(f"predictions_long.parquet ì»¬ëŸ¼ì´ ì˜ˆìƒê³¼ ë‹¬ë¼ìš”. í•„ìš”: {need_cols}, í˜„ì¬: {set(pred.columns)}")
        return

    pred = pred.copy()
    pred["abs_err"] = (pred["pred_ct"] - pred["true_ct"]).abs()

    c_list = sorted(pred["cutoff"].dropna().unique().astype(int).tolist())
    if not c_list:
        st.warning("cutoff ê°’ì´ ë¹„ì–´ìˆì–´ìš”.")
        return

    col1, col2, col3 = st.columns([1.0, 1.0, 1.5])
    with col1:
        best_cutoff = get_best_cutoff_from_report()
        if best_cutoff in c_list:
            default_idx = c_list.index(best_cutoff)
        else:
            default_idx = min(len(c_list)-1, 0)
    
        cutoff = st.selectbox("cutoff ì„ íƒ", c_list, index=default_idx)
    with col2:
        topk = st.slider("Hard Top-K", min_value=5, max_value=200, value=30, step=5)
    with col3:
        err_thr = st.number_input("ë˜ëŠ” |error| ì„ê³„ê°’", value=0.0, step=0.5, help="0ì´ë©´ Top-K ê¸°ì¤€ë§Œ ì‚¬ìš©")

    df = pred[pred["cutoff"] == int(cutoff)].copy()
    if err_thr > 0:
        hard = df[df["abs_err"] >= float(err_thr)].sort_values("abs_err", ascending=False)
    else:
        hard = df.sort_values("abs_err", ascending=False).head(int(topk))
        
    # =========================
    # 0) ì „ì²´ í›„ë³´êµ° ëŒ€ë¹„ hard ì„ ì • ì´ìœ /ë­í¬ ë§Œë“¤ê¸°
    # =========================
    df = df.copy()
    df["err"] = df["pred_ct"] - df["true_ct"]
    df = df.sort_values("abs_err", ascending=False).reset_index(drop=True)
    df["rank_abs_err"] = np.arange(1, len(df) + 1)  # 1ì´ ê°€ì¥ hard
    
    # hard set í‘œì‹œ
    if err_thr > 0:
        rule_text = f"|error| â‰¥ {float(err_thr):.3f}"
        df["is_hard"] = df["abs_err"] >= float(err_thr)
    else:
        rule_text = f"Top-{int(topk)} by |error|"
        df["is_hard"] = df["rank_abs_err"] <= int(topk)
    
    hard = df[df["is_hard"]].copy()
    hard = hard.sort_values("abs_err", ascending=False).reset_index(drop=True)
    
    st.markdown("## ğŸ¯ Pred vs Trueì—ì„œ Hard ê°•ì¡° ë³´ê¸°")

    # hard id set ë§Œë“¤ê¸° (run_id, well_id)
    hard_ids = set(zip(hard["run_id"].astype(str), hard["well_id"].astype(str)))
    
    # (ì•„ì§ ì„ íƒ ìƒ˜í”Œ rid/wid ë§Œë“¤ê¸° ì „ì´ë©´ highlight=Noneìœ¼ë¡œ ë¨¼ì € ê·¸ë¦¬ê³ ,
    #  ì„ íƒ ìƒ˜í”Œ ì„ íƒ í›„ì— ë‹¤ì‹œ í•œ ë²ˆ ê·¸ë ¤ë„ ë¨)
    plot_pred_vs_true_hard_colored(
        df_cut=df,                 # í•´ë‹¹ cutoff ì „ì²´
        hard_ids=hard_ids,
        highlight=None
    )
    
    st.caption("ì„ (y=x)ì—ì„œ ë§ì´ ë²—ì–´ë‚œ ì ë“¤ì„ Hardë¡œ í‘œì‹œí•´ì„œ 'ìš°ë¦¬ê°€ ì´ê±¸ ë¶„ì„ ì¤‘'ì´ë¼ëŠ” ëŠë‚Œì„ ì¤ë‹ˆë‹¤.")
    
    # =========================
    # 1) Hard ì„ ì • ìš”ì•½ ì¹´ë“œ (ì „ì²´ ëŒ€ë¹„)
    # =========================
    st.markdown("## ğŸ” Hard ì„ ì • ìš”ì•½")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Cutoff", int(cutoff))
    c2.metric("ì „ì²´ í›„ë³´(í•´ë‹¹ cutoff)", int(len(df)))
    c3.metric("Hard í›„ë³´", int(len(hard)))
    hard_ratio = (len(hard) / len(df) * 100.0) if len(df) else 0.0
    c4.metric("Hard ë¹„ìœ¨", f"{hard_ratio:.1f}%")
    st.caption(f"Hard ì„ ì • ê¸°ì¤€: **{rule_text}**")
    st.divider()
    
    # =========================
    # 2) ì „ì²´ ë¶„í¬ì—ì„œ hard ìœ„ì¹˜ ë³´ê¸° (íˆìŠ¤í† ê·¸ë¨)
    # =========================
    st.markdown("## ğŸ“Š ì „ì²´ í›„ë³´ ëŒ€ë¹„ Hard ìœ„ì¹˜")
    import altair as alt
    
    hist_base = alt.Chart(df).mark_bar().encode(
        x=alt.X("abs_err:Q", bin=alt.Bin(maxbins=40), title="|error| (abs_err)"),
        y=alt.Y("count():Q", title="Count"),
        tooltip=[alt.Tooltip("count():Q", title="Count")]
    )
    
    layers = [hist_base]
    
    # threshold ë¼ì¸ ë˜ëŠ” topk ì»· ë¼ì¸
    if err_thr > 0:
        thr_df = pd.DataFrame({"abs_err_thr": [float(err_thr)]})
        thr_line = alt.Chart(thr_df).mark_rule(strokeDash=[6,4]).encode(x="abs_err_thr:Q")
        layers.append(thr_line)
    else:
        # TopKì˜ ë§ˆì§€ë§‰ abs_err ê°’ì„ ì»·ìœ¼ë¡œ í‘œì‹œ
        if len(hard) > 0:
            kth = float(hard["abs_err"].min())
            kth_df = pd.DataFrame({"abs_err_kth": [kth]})
            kth_line = alt.Chart(kth_df).mark_rule(strokeDash=[6,4]).encode(x="abs_err_kth:Q")
            layers.append(kth_line)
    
    st.altair_chart(alt.layer(*layers).properties(height=220), use_container_width=True)
    st.divider()
    
    # í‘œëŠ” ì ‘ì–´ì„œ ë³´ê¸° ì˜µì…˜
    with st.expander("ğŸ“‹ Hard í›„ë³´ í‘œ(ì ‘ê¸°/í¼ì¹˜ê¸°)", expanded=False):
        st.dataframe(
            hard[["run_id", "well_id", "true_ct", "pred_ct", "abs_err", "rank_abs_err"]],
            use_container_width=True,
            height=260
        )


    st.caption(f"model_id={model_id} / cutoff={cutoff} / candidates={len(hard)}")
    st.dataframe(hard[["run_id", "well_id", "true_ct", "pred_ct", "abs_err"]], use_container_width=True, height=320)

    # ---- ì„ íƒ ----
    if len(hard) == 0:
        return
    
    items = hard.reset_index(drop=True)  # 0..N-1 ì¸ë±ìŠ¤ ê³ ì •
    
    # âœ… ì„¸ì…˜ ì¸ë±ìŠ¤ í‚¤ (ì •ìˆ˜)
    if "hard_pick_i" not in st.session_state:
        st.session_state["hard_pick_i"] = 0
    
    # âœ… TopK/threshold ë°”ë€Œì–´ items ê¸¸ì´ê°€ ë°”ë€Œì–´ë„ ì•ˆì „í•˜ê²Œ clamp
    st.session_state["hard_pick_i"] = max(
        0, min(int(st.session_state["hard_pick_i"]), len(items) - 1)
    )
    
    st.caption(f"ğŸ“Œ {st.session_state['hard_pick_i']+1} / {len(items)}")
    
    def _fmt(i: int) -> str:
        r = items.iloc[i]
        return f"{r['run_id']} | {r['well_id']} | {r['abs_err']:.3f}"
    
    pick_i = st.selectbox(
        "ê²€í† í•  ìƒ˜í”Œ ì„ íƒ (run_id | well_id | abs_err)",
        options=list(range(len(items))),
        index=int(st.session_state["hard_pick_i"]),
        format_func=_fmt,
        key="hard_pick_i_selectbox",
    )
    
    # âœ… ì„ íƒê°’ ë™ê¸°í™” (ì‚¬ìš©ìê°€ ì§ì ‘ ë“œë¡­ë‹¤ìš´ìœ¼ë¡œ ë°”ê¿”ë„ idx ê°±ì‹ )
    st.session_state["hard_pick_i"] = int(pick_i)
    
    rid = str(items.loc[pick_i, "run_id"])
    wid = str(items.loc[pick_i, "well_id"])
    
    # =========================
    # 3) ì„ íƒ ìƒ˜í”Œì´ hardë¡œ ë“¤ì–´ì˜¨ ì´ìœ  ì„¤ëª…
    # =========================
    chosen = df[(df["run_id"] == rid) & (df["well_id"] == wid)]
    if len(chosen) > 0:
        chosen = chosen.iloc[0]
        st.markdown("### ğŸ§  ì™œ ì´ ìƒ˜í”Œì´ Hardì¸ê°€?")
        reason_lines = [
            f"- Hard ì„ ì • ê¸°ì¤€: **{rule_text}**",
            f"- ì´ ìƒ˜í”Œ |error| = **{float(chosen['abs_err']):.3f}** (err={float(chosen['err']):+.3f})",
            f"- ì „ì²´ í›„ë³´ {len(df)}ê°œ ì¤‘ |error| ìˆœìœ„: **{int(chosen['rank_abs_err'])}ìœ„**",
        ]
        if err_thr > 0:
            reason_lines.append(f"- ì„ê³„ê°’ {float(err_thr):.3f} {'í†µê³¼' if float(chosen['abs_err']) >= float(err_thr) else 'ë¯¸í†µê³¼'}")
        else:
            reason_lines.append(f"- Top-{int(topk)} {'í¬í•¨' if int(chosen['rank_abs_err']) <= int(topk) else 'ë¯¸í¬í•¨'}")
        st.write("\n".join(reason_lines))

    
    sel = items.iloc[int(pick_i)]
    st.markdown("### ì„ íƒ ìƒ˜í”Œ ìš”ì•½")
    st.write(
        {
            "run_id": rid,
            "well_id": wid,
            "cutoff": int(cutoff),
            "true_ct": float(sel["true_ct"]),
            "pred_ct": float(sel["pred_ct"]),
            "abs_err": float(sel["abs_err"]),
        }
    )
    
    # ---- í”Œë¡¯ ----
    st.markdown("### ğŸ“ˆ ì›ë³¸ qPCR ê³¡ì„  ë³´ê¸° (master_long ìš°ì„ , ì—†ìœ¼ë©´ predictions_long JSON fallback)")
    cutoff_i = int(cutoff)

    try:
        curve = None

        # 1) master_long ìˆìœ¼ë©´ ìš°ì„  ë¡œë“œ
        if has_canonical_master_long():
            curve = load_curve_from_master(rid, wid)

        # 2) ì—†ê±°ë‚˜(Cloud) / ëª»ì°¾ì•˜ê±°ë‚˜ / ë¹ˆ dfë©´ -> predictions_longì˜ JSONìœ¼ë¡œ ë³µì›
        if curve is None or len(curve) == 0:
            curve = load_one_curve_from_predictions_row(sel.to_dict())

        if curve is None or len(curve) == 0:
            st.info("ê³¡ì„  ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´. (master_longë„ ì—†ê³ , predictions_long JSONë„ ë¹„ì–´ìˆìŒ)")
        else:
            curve = curve.sort_values("Cycle").reset_index(drop=True)
            curve["segment"] = np.where(curve["Cycle"] <= cutoff_i, "early(<=cutoff)", "late")

            import altair as alt

            line = (
                alt.Chart(curve)
                .mark_line()
                .encode(
                    x=alt.X("Cycle:Q", title="Cycle"),
                    y=alt.Y("Fluor:Q", title="Fluor"),
                    tooltip=["Cycle", "Fluor", "segment"],
                )
            )
            vline = (
                alt.Chart(pd.DataFrame({"Cycle": [cutoff_i]}))
                .mark_rule(strokeDash=[6, 4])
                .encode(x="Cycle:Q")
            )
            st.altair_chart(line + vline, use_container_width=True)

            st.markdown("#### ğŸ” Early êµ¬ê°„ í™•ëŒ€ (<= cutoff)")
            early = curve[curve["Cycle"] <= cutoff_i].copy()
            if len(early) < 2:
                st.info("early êµ¬ê°„ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ í™•ëŒ€ í”Œë¡¯ì„ ëª» ê·¸ë ¤.")
            else:
                eline = (
                    alt.Chart(early)
                    .mark_line()
                    .encode(
                        x=alt.X("Cycle:Q", title="Cycle (early)"),
                        y=alt.Y("Fluor:Q", title="Fluor"),
                        tooltip=["Cycle", "Fluor"],
                    )
                )
                st.altair_chart(eline, use_container_width=True)

    except Exception as e:
        st.warning(f"ê³¡ì„  ë¡œë”©/í”Œë¡¯ ì‹¤íŒ¨: {e}")
    
    # ---- ë¹ ë¥¸ ì´ë™ ----
    st.markdown("### â­ï¸ ë¹ ë¥¸ ì´ë™")
    
    def _next_sample():
        # ë²„íŠ¼ ì½œë°±ì—ì„œë§Œ idx ì¦ê°€ (ì—¬ê¸°ì„œ setí•˜ë©´ Streamlitì´ ìì—°ìŠ¤ëŸ½ê²Œ reruní•¨)
        i = int(st.session_state.get("hard_pick_i", 0))
        if i < len(items) - 1:
            st.session_state["hard_pick_i"] = i + 1
            st.session_state["hard_pick_i_selectbox"] = i + 1  # selectboxë„ ê°™ì´ ë°€ì–´ì¤Œ(ì•ˆì „)
        else:
            st.session_state["hard_pick_i"] = len(items) - 1
    
    st.button("ë‹¤ìŒ ìƒ˜í”Œ ë³´ê¸°", type="secondary", key="btn_next_hard", on_click=_next_sample)

# -------------------------
# UI
# -------------------------
st.caption("ì—…ë¡œë“œí•œ qPCR curve ë°ì´í„°ë¡œ Ctë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜, ì„œë²„ì— ëˆ„ì ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”.")

# -------------------------
# Sidebar (cutoff / retrain range)
# -------------------------
cutoffs = discover_cutoffs(MODELS_DIR)
if not cutoffs:
    st.error(f"ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆì–´: {MODELS_DIR} (ct_xgb_cutoff_*.json ì—†ìŒ)")
    st.stop()
with st.sidebar:
    st.title("CPHOTONICS | Early Ct Predictor")  # ì•± ì œëª© ì‚¬ì´ë“œë°”ì—
    if st.button("ğŸ“Š Data Quality Control & Catalog", type="primary"):
        st.session_state.show_data_catalog = True
    if st.button("ğŸ”™ Back to Main"):
        st.session_state.show_data_catalog = False    
    st.divider()
    st.subheader("ì¬í•™ìŠµ (ì„œë²„ ë°ì´í„° ê¸°ì¤€)")
    min_c = st.number_input("min_cutoff", min_value=1, max_value=200, value=10, step=1, key="sidebar_min_c")
    max_c = st.number_input("max_cutoff", min_value=1, max_value=200, value=40, step=1, key="sidebar_max_c")

cutoff = int(cutoff)
min_c = int(min_c)
max_c = int(max_c)

tabs = st.tabs(["ğŸ“ˆ Performance", "ğŸ§ª Predict (Upload)", "ğŸ§¨ Hard Review", "ğŸ›  Retrain(Admin)"])

with tabs[0]:
    show_train_report()   # ìµœì†Œí•œ ì´ê±°ë¼ë„

with tabs[1]:
    st.subheader("ğŸ§ª Predict (Upload)")
    up = st.file_uploader("qPCR íŒŒì¼ ì—…ë¡œë“œ (csv/xlsx)", type=["csv", "xlsx", "xls"])
    if up is None:
        st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì˜ˆì¸¡ì„ ì§„í–‰í•´ìš”.")
    else:
        run_id = _safe_stem(up.name) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")

        # âœ… 1) ì—…ë¡œë“œ ì½ê¸°
        raw_obj = read_uploaded_table(up)

        # âœ… 2) ì—‘ì…€ì´ë©´ ì‹œíŠ¸ ë¶„ë¦¬, CSVë©´ ê·¸ëŒ€ë¡œ
        df_curve, df_truth, curve_sheet, truth_sheet = split_excel_sheets(raw_obj)
        if isinstance(raw_obj, dict):
            st.write("ğŸ“„ Sheets:", list(raw_obj.keys()))
            for nm, df_ in raw_obj.items():
                st.write(f"--- {nm} ---")
                st.write("columns:", [str(c) for c in df_.columns])

        # âœ… 3) ë¯¸ë¦¬ë³´ê¸°
        if curve_sheet is not None:
            st.caption(f"ì—…ë¡œë“œ curve ì‹œíŠ¸: {curve_sheet}")
        st.dataframe(df_curve.head(30), use_container_width=True)

        if df_truth is not None:
            st.caption(f"ì •ë‹µ Ct/Cq ì‹œíŠ¸ ê°ì§€ë¨: {truth_sheet}")
            st.dataframe(df_truth.head(30), use_container_width=True)

        # âœ… 4) long ë³€í™˜ì€ curve_dfë¡œë§Œ!
        try:
            df_long = infer_long_df(df_curve, run_id=run_id)
        except Exception as e:
            st.error(f"ì—…ë¡œë“œ íŒŒì¼ì„ long í˜•íƒœë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            st.stop()
        
        # =========================
        # (NEW) Multi-cutoff Sweep (ì—…ë¡œë“œ ë°ì´í„°)
        # =========================
        do_sweep = st.toggle("ì—¬ëŸ¬ cutoffë¡œ í•œ ë²ˆì— ë¹„êµ(3/5 step)", value=False, key="do_sweep_upload")


        if do_sweep:
            step2 = st.radio("step", [3, 5], horizontal=True, key="upload_step")
            sweep_min = st.number_input("min cutoff", 1, 200, 10, 1, key="upload_min")
            sweep_max = st.number_input("max cutoff", 1, 200, 40, 1, key="upload_max")

            sweep_cutoffs = list(range(int(sweep_min), int(sweep_max) + 1, int(step2)))

            missing = []
            preds_all = []
            
            with st.spinner(f"Sweep ì˜ˆì¸¡ ì¤‘... (cutoffs={sweep_cutoffs})"):
                for c in sweep_cutoffs:
                    try:
                        p = predict_ct(df_long, cutoff=int(c))
                        p["cutoff"] = int(c)
            
                        # ì—…ë¡œë“œ ì˜ˆì¸¡ ê²°ê³¼ì— well_id ì—†ì„ ìˆ˜ ìˆì–´ì„œ ì•ˆì „í•˜ê²Œ ìƒì„±
                        if "well_id" not in p.columns:
                            if "Well" in p.columns:
                                p["well_id"] = p["Well"].astype(str)
                            elif "well_uid" in p.columns:
                                p["well_id"] = p["well_uid"].astype(str)
            
                        preds_all.append(p)
            
                    except Exception as e:
                        # ëª¨ë¸ íŒŒì¼ ì—†ëŠ” cutoff(=No such file...)ë©´ ì—¬ê¸°ë¡œ ë“¤ì–´ì˜´
                        missing.append((int(c), str(e)))
            
            if missing:
                st.warning("ì¼ë¶€ cutoffëŠ” ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ì„œ ìŠ¤í‚µí–ˆì–´: " + ", ".join(str(c) for c, _ in missing))
            
            if not preds_all:
                st.error("ì„ íƒí•œ cutoff ë²”ìœ„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ì–´. min/maxë¥¼ ì¤„ì—¬ì¤˜.")
                st.stop()
            
            preds_all = pd.concat(preds_all, ignore_index=True)


            # ---- truth ìˆìœ¼ë©´ cutoffë³„ ì„±ëŠ¥(MAE/RMSE) ----
            if df_truth is not None:
                tmp = preds_all.copy()

                # true ì»¬ëŸ¼ ì°¾ê¸°
                true_col = None
                for cand in ["true_ct", "ct", "Ct", "Cq", "cq", "CQ", "CT"]:
                    if cand in df_truth.columns:
                        true_col = cand
                        break

                # well ì»¬ëŸ¼ ì°¾ê¸°
                wcol = None
                for cand in ["Well", "well", "WELL"]:
                    if cand in df_truth.columns:
                        wcol = cand
                        break

                if true_col and wcol:
                    truth2 = df_truth[[wcol, true_col]].copy()
                    truth2.columns = ["Well", "true_ct"]

                    # âœ… Well í‘œì¤€í™”(ë§¤ì¹­ í•µì‹¬)
                    truth2["Well"] = truth2["Well"].map(normalize_well)
                    if "Well" in tmp.columns:
                        tmp["Well"] = tmp["Well"].map(normalize_well)
                    elif "well_id" in tmp.columns:
                        tmp["well_id"] = tmp["well_id"].map(normalize_well)

                    # merge í‚¤: tmpì— Wellì´ ìˆìœ¼ë©´ Wellë¡œ, ì—†ìœ¼ë©´ well_idë¡œ
                    if "Well" in tmp.columns:
                        tmp = tmp.merge(truth2, on="Well", how="left")
                    else:
                        tmp = tmp.merge(truth2.rename(columns={"Well": "well_id"}), on="well_id", how="left")

                    tmp["err"] = tmp["pred_ct"] - tmp["true_ct"]
                    g = tmp.dropna(subset=["true_ct", "pred_ct"]).groupby("cutoff")

                    perf = g.apply(lambda x: pd.Series({
                        "mae": float(np.mean(np.abs(x["err"]))),
                        "rmse": float(np.sqrt(np.mean(x["err"] ** 2))),
                        "n": int(len(x)),
                    })).reset_index()

                    st.markdown("#### cutoffë³„ MAE/RMSE")
                    pm = perf.melt(id_vars=["cutoff"], value_vars=["mae", "rmse"], var_name="metric", value_name="value")
                    st.altair_chart(
                        alt.Chart(pm).mark_line(point=True).encode(
                            x="cutoff:Q",
                            y="value:Q",
                            strokeDash="metric:N",
                            tooltip=["cutoff", "metric", "value", "n:Q"],
                        ).properties(height=260),
                        use_container_width=True
                    )

                    # ---- Pred vs True small multiples (sweep) ----
                    st.markdown("#### Pred vs True (Sweep Small Multiples)")
                    ncol = st.slider("í•œ ì¤„ì— ëª‡ ê°œ?", 2, 6, 4, 1, key="upload_sweep_cols")

                    # plot_pred_vs_true_facetsê°€ ìš”êµ¬í•˜ëŠ” ì»¬ëŸ¼ ë§ì¶”ê¸°
                    if "run_id" not in tmp.columns:
                        tmp["run_id"] = run_id
                    if "well_id" not in tmp.columns:
                        tmp["well_id"] = tmp["Well"].astype(str)

                    plot_pred_vs_true_facets(tmp.rename(columns={"pred_ct": "pred_ct", "true_ct": "true_ct"}), sweep_cutoffs, ncol=int(ncol))
                else:
                    st.info("Sweep ì„±ëŠ¥ì„ ê³„ì‚°í•˜ë ¤ë©´ truth ì‹œíŠ¸ì— Well + (ct/cq/true_ct) ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•´ìš”.")

            st.divider()

        # âœ… 5) ì˜ˆì¸¡
        pred_df = predict_ct(df_long, cutoff=int(cutoff))
        st.success("ì˜ˆì¸¡ ì™„ë£Œ!")

        # =========================
        # (NEW) Prediction ì‹ ë¢°ë„/ì •ë‹µë¥ (í™•ë¥ ) ìš”ì•½
        # =========================
        st.markdown("### âœ… ì˜ˆì¸¡ ì‹ ë¢°ë„ / ëª‡ ê°œ ë§ì·„ëŠ”ì§€")
        
        tol_u = st.slider(
            "ì •ë‹µ íŒì • ê¸°ì¤€ (|pred-true| <= tol)",
            0.5, 5.0, 2.0, 0.5,
            key="upload_tol_summary",
        )
        
        # --- ì„œë²„ ë¡œê·¸ ê¸°ë°˜ 'ì˜ˆìƒ ì •ë‹µë¥ (í™•ë¥ )' ê³„ì‚° (ìˆìœ¼ë©´) ---
        active_path = PROJECT_ROOT / "reports" / "active_model.txt"
        model_id = active_path.read_text().strip() if active_path.exists() else "model_server_latest_xgb"
        
        pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"

        expected_rate = None
        if pred_path.exists():
            try:
                server_pred_long = pd.read_parquet(pred_path)
                acc_df_srv = perf_accuracy_fraction_vs_cutoff(server_pred_long, tol=float(tol_u))
                hit = acc_df_srv[acc_df_srv["cutoff"] == int(cutoff)]
                if not hit.empty:
                    expected_rate = float(hit["acc_frac"].iloc[0])
            except Exception:
                expected_rate = None
        
        # --- ì—…ë¡œë“œ íŒŒì¼ truthê°€ ìˆìœ¼ë©´ "ë§ì¶˜ ê°œìˆ˜" ê³„ì‚° ---
        def _build_eval_df_with_truth(pred_df: pd.DataFrame, df_truth: pd.DataFrame) -> pd.DataFrame:
            # true_ct column ì°¾ê¸°
            true_col = None
            for cand in ["true_ct", "TrueCt", "trueCt", "ct", "Ct", "CT", "Cq", "cq", "CQ"]:
                if cand in df_truth.columns:
                    true_col = cand
                    break
            if true_col is None:
                return pd.DataFrame()
        
            # Well column ì°¾ê¸°
            wcol = None
            for cand in ["Well", "well", "WELL"]:
                if cand in df_truth.columns:
                    wcol = cand
                    break
            if wcol is None:
                return pd.DataFrame()
        
            truth2 = df_truth[[wcol, true_col]].copy()
            truth2.columns = ["Well", "true_ct"]
        
            out = pred_df.copy()
            if "Well" not in out.columns:
                return pd.DataFrame()
        
            # Well í‘œì¤€í™”
            truth2["Well"] = truth2["Well"].map(normalize_well)
            out["Well"] = out["Well"].map(normalize_well)
        
            out = out.merge(truth2, on="Well", how="left")
            out["pred_ct"] = pd.to_numeric(out["pred_ct"], errors="coerce")
            out["true_ct"] = pd.to_numeric(out["true_ct"], errors="coerce")
            out = out.dropna(subset=["pred_ct", "true_ct"]).copy()
            if out.empty:
                return pd.DataFrame()
        
            out["abs_err"] = (out["pred_ct"] - out["true_ct"]).abs()
            out["within_tol"] = out["abs_err"] <= float(tol_u)
            return out
        
        eval_df = pd.DataFrame()
        if "pred_df" in locals() and pred_df is not None and df_truth is not None:
            eval_df = _build_eval_df_with_truth(pred_df, df_truth)
        else:
            st.info("ì˜ˆì¸¡ ê²°ê³¼(pred_df) ë˜ëŠ” truth(df_truth)ê°€ ì•„ì§ ì—†ì–´ì„œ ì‹ ë¢°ë„ ê³„ì‚°ì„ ìƒëµí–ˆì–´.")

        # --- UI ì¶œë ¥ (truth ìˆìœ¼ë©´ ì‹¤ì œ ì •ë‹µë¥  / ì—†ìœ¼ë©´ ì˜ˆìƒ ì •ë‹µë¥ ) ---
        cA, cB, cC, cD = st.columns(4)
        
        if not eval_df.empty:
            n_total = int(len(eval_df))
            n_hit = int(eval_df["within_tol"].sum())
            hit_rate = (n_hit / n_total) if n_total else 0.0
            mae_u = float(eval_df["abs_err"].mean())
            rmse_u = float(np.sqrt(np.mean((eval_df["pred_ct"] - eval_df["true_ct"]) ** 2)))
        
            cA.metric("ë§ì¶˜ ê°œìˆ˜", f"{n_hit} / {n_total}")
            cB.metric(f"ì •ë‹µë¥ (Â±{tol_u:g})", f"{hit_rate*100:.1f}%")
            cC.metric("MAE(ì—…ë¡œë“œ)", f"{mae_u:.3f}")
            cD.metric("RMSE(ì—…ë¡œë“œ)", f"{rmse_u:.3f}")
        
            if expected_rate is not None:
                st.caption(f"ì°¸ê³ : ì„œë²„ í‰ê°€ ë¡œê·¸ ê¸°ì¤€ ì´ cutoffì˜ **ì˜ˆìƒ ì •ë‹µë¥ (Â±{tol_u:g}) â‰ˆ {expected_rate*100:.1f}%**")
        
            st.progress(hit_rate)
            st.caption("í•´ì„: ì •ë‹µë¥ ì€ **|pred-true| <= tol** ë§Œì¡± ë¹„ìœ¨ì…ë‹ˆë‹¤.")
        
            # (ì˜µì…˜) Wellë³„ ì˜¤ì°¨ ë§‰ëŒ€ + ìƒ‰ìƒìœ¼ë¡œ ë§/í‹€ í‘œì‹œ
            with st.expander("ğŸ” Wellë³„ ì˜¤ì°¨(ë§/í‹€ ìƒ‰ìƒ) ë³´ê¸°", expanded=False):
                import altair as alt
                bar = (
                    alt.Chart(eval_df)
                    .mark_bar()
                    .encode(
                        x=alt.X("Well:N", sort=alt.SortField("abs_err", order="descending"), title="Well"),
                        y=alt.Y("abs_err:Q", title="|Error|"),
                        color=alt.Color("within_tol:N", title=f"within Â±{tol_u:g}", legend=None),
                        tooltip=["Well", "true_ct", "pred_ct", "abs_err", "within_tol"],
                    )
                    .properties(height=260)
                )
                st.altair_chart(bar, use_container_width=True)
        
        else:
            # truth ì—†ì„ ë•Œ: ì˜ˆìƒ ì •ë‹µë¥ ë§Œ
            if expected_rate is not None:
                cA.metric(f"ì˜ˆìƒ ì •ë‹µë¥ (Â±{tol_u:g})", f"{expected_rate*100:.1f}%")
                cB.metric("cutoff", int(cutoff))
                cC.metric("Wells", int(df_long["Well"].nunique()) if "Well" in df_long.columns else int(df_long["well_uid"].nunique()))
                cD.metric("ê¸°ì¤€", "ì„œë²„ ë¡œê·¸ ê¸°ë°˜")
                st.progress(expected_rate)
                st.caption(
                    f"truth(ì •ë‹µ Ct/Cq)ì´ ì—†ì–´ì„œ ì—…ë¡œë“œ ë°ì´í„°ì˜ 'ë§ì¶˜ ê°œìˆ˜'ëŠ” ê³„ì‚° ë¶ˆê°€. "
                    f"ëŒ€ì‹  ì„œë²„ í‰ê°€ ë¡œê·¸ì—ì„œ **ì´ cutoffê°€ Â±{tol_u:g} ì•ˆì— ë“¤ì–´ê°ˆ í™•ë¥ (ì˜ˆìƒ ì •ë‹µë¥ )** ì„ ë³´ì—¬ì¤˜ìš”."
                )
            else:
                st.info(
                    "truth(ì •ë‹µ Ct/Cq) ì‹œíŠ¸ë„ ì—†ê³ , ì„œë²„ í‰ê°€ ë¡œê·¸(predictions_long.parquet)ë„ ì—†ì–´ì„œ "
                    "ì •ë‹µë¥ /í™•ë¥  ìš”ì•½ì„ í‘œì‹œí•  ìˆ˜ ì—†ì–´ìš”."
                )

        # ---- ìš”ì•½ ì¹´ë“œ ----
        c1, c2, c3 = st.columns(3)
        c1.metric("Cutoff", int(cutoff))
        c2.metric("Wells", int(pred_df["Well"].nunique()) if "Well" in pred_df.columns else len(pred_df))
        c3.metric("Pred Ct (min ~ max)", f"{pred_df['pred_ct'].min():.3f} ~ {pred_df['pred_ct'].max():.3f}")

        st.divider()

        # âœ… í‘œëŠ” ê¸°ë³¸ ìˆ¨ê¹€(ì›í•˜ë©´ ì¼œê¸°)
        show_tables = st.toggle("í‘œë„ ê°™ì´ ë³´ê¸°(ë””ë²„ê¹…ìš©)", value=False, key="pred_show_tables")

        # ---- figure 1) ì—…ë¡œë“œ ê³¡ì„  preview ----
        st.markdown("### ğŸ“ˆ ì—…ë¡œë“œ ê³¡ì„  Preview")
        plot_uploaded_curve_preview(df_long, cutoff=int(cutoff), max_wells=6)

        # ---- figure 2) Pred Ct ë¶„í¬ ----
        st.markdown("### ğŸ“Š Pred Ct ë¶„í¬")
        plot_pred_ct_hist(pred_df)

        # ---- figure 3) í’ˆì§ˆì§€í‘œ(CV) vs Ct ----
        st.markdown("### ğŸ§ª í’ˆì§ˆì§€í‘œ(CV) vs Ct")
        plot_cv_vs_ct(df_long, pred_df, cutoff=int(cutoff))

        # ---- í‘œëŠ” ì˜µì…˜ ----
        if show_tables:
            st.markdown("### ğŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸”")
            st.dataframe(pred_df, use_container_width=True)

        # âœ… 6) truth ìˆìœ¼ë©´ ì¦‰ì„ í‰ê°€ (ì¤‘ìš”: df_truth ë„˜ê¸°ê¸°!)
        try_eval_if_truth_exists(df_curve, pred_df, truth_df=df_truth)

        st.divider()
        
        # =========================
        # (A) Pred vs True + error color (Upload)
        # =========================
        st.markdown("### ğŸ¯ Pred vs True (ì—…ë¡œë“œ ë°ì´í„°, ì˜¤ì°¨ ìƒ‰ìƒ í‘œì‹œ)")
        
        if df_truth is not None:
            # eval_df ë§Œë“¤ê¸°: try_eval_if_truth_existsì™€ ë™ì¼í•œ ë°©ì‹(Well normalize)
            true_col = None
            for cand in ["true_ct", "TrueCt", "trueCt", "ct", "Ct", "CT", "Cq", "cq", "CQ"]:
                if cand in df_truth.columns:
                    true_col = cand
                    break
        
            wcol = None
            for cand in ["Well", "well", "WELL"]:
                if cand in df_truth.columns:
                    wcol = cand
                    break
        
            if true_col and wcol:
                truth2 = df_truth[[wcol, true_col]].copy()
                truth2.columns = ["Well", "true_ct"]
                truth2["Well"] = truth2["Well"].map(normalize_well)
        
                tmp_eval = pred_df.copy()
                tmp_eval["Well"] = tmp_eval["Well"].map(normalize_well)
                tmp_eval = tmp_eval.merge(truth2, on="Well", how="left")
        
                tmp_eval["true_ct"] = pd.to_numeric(tmp_eval["true_ct"], errors="coerce")
                tmp_eval["pred_ct"] = pd.to_numeric(tmp_eval["pred_ct"], errors="coerce")
                tmp_eval = tmp_eval.dropna(subset=["true_ct", "pred_ct"]).copy()
        
                if not tmp_eval.empty:
                    tmp_eval["err"] = tmp_eval["pred_ct"] - tmp_eval["true_ct"]
                    tmp_eval["abs_err"] = tmp_eval["err"].abs()
        
                    import altair as alt
                    line_df = _line_y_eq_x(tmp_eval.rename(columns={"true_ct": "true_ct", "pred_ct": "pred_ct"}))
        
                    base = alt.Chart(tmp_eval).mark_circle(size=70, opacity=0.85).encode(
                        x=alt.X("true_ct:Q", title="True Ct/Cq"),
                        y=alt.Y("pred_ct:Q", title="Pred Ct/Cq"),
                        color=alt.Color("abs_err:Q", title="|Error|"),
                        tooltip=["Well", "true_ct", "pred_ct", "err", "abs_err"]
                    ).interactive()
        
                    diag = alt.Chart(line_df.rename(columns={"x": "true_ct", "y": "pred_ct"})).mark_line().encode(
                        x="true_ct:Q", y="pred_ct:Q"
                    )
        
                    st.altair_chart((diag + base).properties(height=360), use_container_width=True)
                    st.caption("âœ… ì ì´ ëŒ€ê°ì„ (y=x)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ˆì¸¡ì´ ì˜ ë§ëŠ” ê±°ì•¼. ìƒ‰ì´ ì§„í• ìˆ˜ë¡(|Error| í¼) ë” ë§ì´ í‹€ë¦° ìƒ˜í”Œ.")
                else:
                    st.info("truthëŠ” ìˆëŠ”ë° predì™€ ë§¤ì¹­ëœ í–‰ì´ ì—†ì–´(Well ë§¤ì¹­ í™•ì¸ í•„ìš”).")
            else:
                st.info("truth ì‹œíŠ¸ì—ì„œ Well/true_ct(Ct/Cq) ì»¬ëŸ¼ì„ ëª» ì°¾ì•„ì„œ Pred vs True ì°¨íŠ¸ëŠ” ìƒëµí–ˆì–´.")
        else:
            st.info("truth ì‹œíŠ¸ê°€ ì—†ì–´ì„œ Pred vs True(ì •ë‹µ ê¸°ë°˜) ì°¨íŠ¸ëŠ” ìƒëµí–ˆì–´.")
        
        # =========================
        # (B) Upload Hard-like Review (Top-K |error|)
        # =========================
        st.markdown("### ğŸ§¨ Upload Hard Review (Top-K |error|)")
        
        if df_truth is not None and 'tmp_eval' in locals() and (tmp_eval is not None) and (not tmp_eval.empty):
            topk_u = st.slider("Hard Top-K (ì—…ë¡œë“œ)", 5, 50, 15, 5, key="pred_upload_hard_topk")
        
            hard_u = tmp_eval.sort_values("abs_err", ascending=False).head(int(topk_u)).reset_index(drop=True)
        
            c1, c2, c3 = st.columns(3)
            c1.metric("n (eval)", int(len(tmp_eval)))
            c2.metric("Hard Top-K", int(len(hard_u)))
            c3.metric("Worst |error|", f"{float(hard_u['abs_err'].iloc[0]):.3f}")
        
            with st.expander("ğŸ“‹ Hard í›„ë³´ í‘œ(ì—…ë¡œë“œ)", expanded=False):
                st.dataframe(hard_u[["Well", "true_ct", "pred_ct", "err", "abs_err"]], use_container_width=True, height=280)
        
            # ì„ íƒí•´ì„œ ê³¡ì„  ë³´ê¸°
            def _fmt_u(i: int) -> str:
                r = hard_u.iloc[i]
                return f"{r['Well']} | |err|={r['abs_err']:.3f} (err={r['err']:+.3f})"
        
            pick_u = st.selectbox(
                "ê²€í† í•  Hard ìƒ˜í”Œ ì„ íƒ(ì—…ë¡œë“œ)",
                options=list(range(len(hard_u))),
                format_func=_fmt_u,
                key="pred_upload_hard_pick",
            )
        
            well_pick = str(hard_u.loc[int(pick_u), "Well"])
        
            st.markdown("#### ğŸ“ˆ ì„ íƒ Hard ìƒ˜í”Œì˜ ì›ë³¸ ê³¡ì„ (ì—…ë¡œë“œ df_long)")
            curve_sel = df_long[df_long["Well"].astype(str) == str(well_pick)].copy()
            if curve_sel.empty:
                st.info("ì„ íƒ wellì˜ curveë¥¼ df_longì—ì„œ ëª» ì°¾ì•˜ì–´.")
            else:
                import altair as alt
                curve_sel = curve_sel.sort_values("Cycle").reset_index(drop=True)
                cutoff_i = int(cutoff)
                curve_sel["segment"] = np.where(curve_sel["Cycle"] <= cutoff_i, "early(<=cutoff)", "late")
        
                line = alt.Chart(curve_sel).mark_line().encode(
                    x=alt.X("Cycle:Q", title="Cycle"),
                    y=alt.Y("Fluor:Q", title="Fluor"),
                    tooltip=["Cycle", "Fluor", "segment"]
                )
                vline = alt.Chart(pd.DataFrame({"Cycle": [cutoff_i]})).mark_rule(strokeDash=[6,4]).encode(x="Cycle:Q")
                st.altair_chart(line + vline, use_container_width=True)
        
                st.markdown("#### ğŸ” Early í™•ëŒ€(<=cutoff)")
                early = curve_sel[curve_sel["Cycle"] <= cutoff_i].copy()
                if len(early) >= 2:
                    eline = alt.Chart(early).mark_line().encode(
                        x=alt.X("Cycle:Q", title="Cycle (early)"),
                        y=alt.Y("Fluor:Q", title="Fluor"),
                        tooltip=["Cycle", "Fluor"]
                    )
                    st.altair_chart(eline, use_container_width=True)
        else:
            st.info("truth ê¸°ë°˜ í‰ê°€ê°€ ì—†ì–´ì„œ(ë˜ëŠ” ë§¤ì¹­ ì‹¤íŒ¨) ì—…ë¡œë“œ Hard ReviewëŠ” ìƒëµí–ˆì–´.")

        # =========================
        # (C) Pred stability across cutoffs (ì—°ê²°ì„ )
        #  - Multi-cutoff Sweepì„ ì¼°ì„ ë•Œë§Œ ì˜ë¯¸ ìˆìŒ
        # =========================
        st.markdown("### ğŸ”— Cutoffì— ë”°ë¥¸ ì˜ˆì¸¡ ë³€í™”(ì„  ì—°ê²°)")
        
        if 'preds_all' in locals() and isinstance(preds_all, pd.DataFrame) and (not preds_all.empty):
            # í•œ ë²ˆì— ë„ˆë¬´ ë§ìœ¼ë©´ ë³´ê¸° í˜ë“¤ì–´ì„œ, ë³´ì—¬ì¤„ well ìˆ˜ ì œí•œ
            max_w = st.slider("í‘œì‹œí•  Well ê°œìˆ˜(ìƒìœ„)", 5, 40, 15, 5, key="pred_stab_maxw")
        
            show_wells = preds_all["Well"].astype(str).unique().tolist()[:int(max_w)]
            stab = preds_all[preds_all["Well"].astype(str).isin(show_wells)].copy()
        
            # truthê°€ ìˆìœ¼ë©´ ê°™ì´ ê·¸ë¦¬ê¸°(ê°€ëŠ¥í•˜ë©´)
            if df_truth is not None:
                true_col = None
                for cand in ["true_ct", "ct", "Ct", "Cq", "cq", "CQ"]:
                    if cand in df_truth.columns:
                        true_col = cand; break
                wcol = None
                for cand in ["Well", "well", "WELL"]:
                    if cand in df_truth.columns:
                        wcol = cand; break
        
                if true_col and wcol:
                    truth2 = df_truth[[wcol, true_col]].copy()
                    truth2.columns = ["Well", "true_ct"]
                    truth2["Well"] = truth2["Well"].map(normalize_well)
        
                    stab["Well"] = stab["Well"].map(normalize_well)
                    stab = stab.merge(truth2, on="Well", how="left")
        
            import altair as alt
            base = alt.Chart(stab.dropna(subset=["cutoff", "pred_ct"])).encode(
                x=alt.X("cutoff:Q", title="Cutoff"),
                y=alt.Y("pred_ct:Q", title="Pred Ct"),
                color=alt.Color("Well:N", legend=None),
                tooltip=["Well", "cutoff", "pred_ct"] + (["true_ct"] if "true_ct" in stab.columns else [])
            )
        
            lines = base.mark_line(point=True).properties(height=320)
        
            if "true_ct" in stab.columns and stab["true_ct"].notna().any():
                # TrueëŠ” cutoffë§ˆë‹¤ ë³€í•˜ì§€ ì•Šìœ¼ë‹ˆê¹Œ ì ì„ ìœ¼ë¡œ ê°™ì´ ë³´ì—¬ì£¼ë©´ â€œì •ë‹µ ëŒ€ë¹„ í”ë“¤ë¦¼â€ì´ ë°”ë¡œ ë³´ì„
                true_line = alt.Chart(stab.dropna(subset=["cutoff","true_ct"])).mark_line(strokeDash=[6,4]).encode(
                    x="cutoff:Q",
                    y="true_ct:Q",
                    detail="Well:N",
                    color=alt.value("gray"),
                )
                st.altair_chart((lines + true_line).interactive(), use_container_width=True)
                st.caption("ì‹¤ì„ =Pred ë³€í™”, ì ì„ =True(ê³ ì •). Predê°€ cutoffì— ë”°ë¼ ì•ˆì •ì ì´ë©´ ì‹¤ì„ ì´ ì ì„  ì£¼ë³€ì—ì„œ í¬ê²Œ í”ë“¤ë¦¬ì§€ ì•Šì•„.")
            else:
                st.altair_chart(lines.interactive(), use_container_width=True)
                st.caption("truthê°€ ì—†ì–´ì„œ True ê¸°ì¤€ì„ ì€ ìƒëµí–ˆì–´. ê·¸ë˜ë„ cutoffì— ë”°ë¥¸ ì˜ˆì¸¡ ì•ˆì •ì„±ì€ í™•ì¸ ê°€ëŠ¥.")
        else:
            st.info("Multi-cutoff Sweepì„ ì¼œë©´(ê·¸ë¦¬ê³  preds_all ìƒì„±ë˜ë©´) cutoffë³„ ì˜ˆì¸¡ ì—°ê²°ì„ ì´ ìƒê²¨.")

                    
        # =========================
        # (ì¶”ê°€) ê·¸ë¦¼ ì¤‘ì‹¬ 4íƒ­ UI (ì›í•˜ë©´ ìœ ì§€)
        # =========================
        import altair as alt

        t1, t2, t3, t4 = st.tabs(["ğŸ“Š Ct Overview", "ğŸ“ˆ Wellë³„ Ct", "ğŸ§¬ Curve ë³´ê¸°", "ğŸ§¾ Data(í‘œ)"])

        with t1:
            hist = (
                alt.Chart(pred_df)
                .mark_bar()
                .encode(
                    x=alt.X("pred_ct:Q", bin=alt.Bin(maxbins=30), title="Predicted Ct"),
                    y=alt.Y("count():Q", title="Count"),
                    tooltip=[alt.Tooltip("count():Q", title="count")],
                )
                .properties(height=280)
            )
            st.altair_chart(hist, use_container_width=True)

        with t2:
            bar = (
                alt.Chart(pred_df)
                .mark_bar()
                .encode(
                    x=alt.X("Well:N", sort=alt.SortField("pred_ct", order="ascending"), title="Well"),
                    y=alt.Y("pred_ct:Q", title="Predicted Ct"),
                    tooltip=["Well", "pred_ct"],
                )
                .properties(height=320)
            )
            st.altair_chart(bar, use_container_width=True)

        with t3:
            wells = pred_df["Well"].astype(str).tolist()
            pick_well = st.selectbox("ê³¡ì„  ë³¼ Well ì„ íƒ", wells, index=0, key="pred_pick_well")

            curve_sel = df_long[df_long["Well"].astype(str) == str(pick_well)].copy()
            if curve_sel.empty:
                st.info("ì„ íƒí•œ Wellì˜ curve ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´.")
            else:
                curve_sel = curve_sel.sort_values("Cycle").copy()
                cutoff_i = int(cutoff)

                line = (
                    alt.Chart(curve_sel)
                    .mark_line()
                    .encode(
                        x=alt.X("Cycle:Q", title="Cycle"),
                        y=alt.Y("Fluor:Q", title="Fluor"),
                        tooltip=["Cycle", "Fluor"],
                    )
                )
                vline = (
                    alt.Chart(pd.DataFrame({"Cycle": [cutoff_i]}))
                    .mark_rule(strokeDash=[6, 4])
                    .encode(x="Cycle:Q")
                )
                st.altair_chart(line + vline, use_container_width=True)

                ct_val = float(pred_df.loc[pred_df["Well"].astype(str) == str(pick_well), "pred_ct"].iloc[0])
                st.caption(f"âœ… Well={pick_well} / Pred Ct={ct_val:.3f} (cutoff={cutoff_i})")

        with t4:
            with st.expander("í‘œë¡œ ë³´ê¸° (ì›ë³¸/ì˜ˆì¸¡ ê²°ê³¼)", expanded=False):
                if isinstance(raw_obj, dict):
                    st.caption("curve ì‹œíŠ¸(ìƒë‹¨ ì¼ë¶€)")
                    st.dataframe(df_curve.head(30), use_container_width=True)
                    if df_truth is not None:
                        st.caption("truth ì‹œíŠ¸(ìƒë‹¨ ì¼ë¶€)")
                        st.dataframe(df_truth.head(30), use_container_width=True)
                else:
                    st.caption("ì—…ë¡œë“œ ì›ë³¸(ìƒë‹¨ ì¼ë¶€)")
                    st.dataframe(df_curve.head(30), use_container_width=True)

                st.caption("ì˜ˆì¸¡ ê²°ê³¼")
                st.dataframe(pred_df, use_container_width=True)


# -------------------------
# Tab 2: Hard Review
# -------------------------
with tabs[2]:
    show_hard_review()

# -------------------------
# Tab 3: Retrain (Admin)
# -------------------------
with tabs[3]:
    st.subheader("2) ëˆ„ì  ë°˜ì˜ í›„ ì¬í•™ìŠµ (ê´€ë¦¬ì ë²„íŠ¼)")

    if running_on_streamlit_cloud():
        st.warning(
            "Streamlit Cloudì—ëŠ” canonical ë°ì´í„°(master_long.parquet)ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ìš”.\n"
            "ì„œë²„/ë¡œì»¬ì—ì„œ í•™ìŠµ í›„ reports/ ê²°ê³¼ë¬¼ë§Œ ë°°í¬í•˜ì„¸ìš”."
        )
        st.stop()  # âœ… ì—¬ê¸°ì„œ íƒ­ ì‹¤í–‰ì„ ëë‚´ë²„ë¦¬ë©´ step3ê°€ ì ˆëŒ€ í˜¸ì¶œë˜ì§€ ì•ŠìŒ

    st.info(
        "ì´ ë²„íŠ¼ì€ **í˜„ì¬ ì„œë²„ì— ì €ì¥ëœ canonical ë°ì´í„°(master_long.parquet)** ê¸°ì¤€ìœ¼ë¡œ "
        "ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ê³  data/models/by_cutoffì— ë®ì–´ì”ë‹ˆë‹¤.\n\n"
        "âš ï¸ ë°ì´í„° ingest(= raw -> canonical)ëŠ” ì´ ë²„íŠ¼ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ìš”. "
        "ìƒˆ raw ë°ì´í„°ë¥¼ canonicalë¡œ ë°˜ì˜í•˜ë ¤ë©´ ingest íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¨¼ì € master_longì„ ì—…ë°ì´íŠ¸í•´ì¤˜ì•¼ í•´ìš”."
    )

    meta = load_meta(int(cutoff))
    if meta:
        with st.expander("ì„ íƒëœ ëª¨ë¸ ë©”íƒ€ ë³´ê¸°"):
            st.json(meta)

    can_retrain = has_canonical_master_long() and (not running_on_streamlit_cloud())

    if not can_retrain:
        st.warning(
            "Streamlit Cloudì—ëŠ” í•™ìŠµ ë°ì´í„°(master_long.parquet)ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ìš”.\n"
            "ë¡œì»¬/ì„œë²„ì—ì„œ í•™ìŠµì„ ëŒë¦° ë’¤, reports/ ê²°ê³¼ë¬¼ë§Œ repoì— ì»¤ë°‹í•´ì„œ ë°°í¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìš´ì˜í•˜ì„¸ìš”."
        )
    
    if st.button("ì¬í•™ìŠµ ì‹¤í–‰", type="secondary", key="btn_retrain", disabled=not can_retrain):
        with st.spinner("ì¬í•™ìŠµ ì¤‘... (ë¡œê·¸ ìƒì„± ì¤‘)"):
            code, log = run_retrain(int(min_c), int(max_c))
    
        st.text_area("í•™ìŠµ ë¡œê·¸", log, height=380)
    
        if code == 0:
            st.success("ì¬í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ íŒŒì¼ì´ ê°±ì‹ ëì–´ìš”.")
            show_train_report()
        else:
            st.error(f"ì¬í•™ìŠµ ì‹¤íŒ¨ (return code={code}) - ë¡œê·¸ë¥¼ í™•ì¸í•´ì¤˜.")
            
try:
    st.caption("VERSION: " + (PROJECT_ROOT / "VERSION.txt").read_text().strip())
except Exception:
    st.caption("VERSION: (missing)")

