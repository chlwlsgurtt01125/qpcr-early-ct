# app/streamlit_app.py
from __future__ import annotations

import os
import io
import re
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path
import urllib.request
import urllib.error
import plotly.express as px
import numpy as np
import pandas as pd
import streamlit as st
import xgboost as xgb
import pyarrow.dataset as ds
import argparse
from scipy.optimize import curve_fit
from scipy.stats import linregress
from dataclasses import dataclass
from enum import Enum

class HardBucket(Enum):
    """Hard Sample ë²„í‚· ì¢…ë¥˜"""
    LATE_AMP = "late_amp"
    HIGH_RANGE = "high_range"
    NOISY = "noisy"
    NON_SIGMOID = "non_sigmoid"
    UNKNOWN = "unknown"
    NORMAL = "normal"


@dataclass
class BucketResult:
    """ë²„í‚· ë¶„ë¥˜ ê²°ê³¼"""
    bucket: HardBucket
    confidence: float
    details: Dict
    is_hard: bool


BUCKET_COLORS = {
    "late_amp": "#FFD700",      # ë…¸ë€ìƒ‰
    "high_range": "#FF4444",    # ë¹¨ê°„ìƒ‰
    "noisy": "#FFA500",         # ì£¼í™©ìƒ‰
    "non_sigmoid": "#9370DB",   # ë³´ë¼ìƒ‰
    "unknown": "#808080",       # íšŒìƒ‰
    "normal": "#00CC66",        # ì´ˆë¡ìƒ‰
    "error": "#000000"          # ê²€ì •ìƒ‰
}

BUCKET_EMOJI = {
    "late_amp": "ğŸŸ¡",
    "high_range": "ğŸ”´",
    "noisy": "ğŸŸ ",
    "non_sigmoid": "ğŸŸ£",
    "unknown": "âšª",
    "normal": "ğŸŸ¢",
    "error": "âš«"
}


# ============================================
# PART 2: Sigmoid Fitting í•¨ìˆ˜
# ============================================

def sigmoid_4pl(x, a, b, c, d):
    """4-Parameter Logistic Sigmoid"""
    return d + (a - d) / (1 + (x / c) ** b)


def fit_sigmoid(cycles: np.ndarray, fluor: np.ndarray) -> Tuple[float, np.ndarray, Dict]:
    """
    Sigmoid fitting ìˆ˜í–‰
    Returns: (r2, fitted_values, params_dict)
    """
    try:
        a_init = np.min(fluor)
        d_init = np.max(fluor)
        c_init = cycles[len(cycles) // 2]
        b_init = 1.0
        
        popt, _ = curve_fit(
            sigmoid_4pl, cycles, fluor,
            p0=[a_init, b_init, c_init, d_init],
            bounds=([0, 0.1, 1, 0], [np.inf, 50, 100, np.inf]),
            maxfev=5000
        )
        
        fitted = sigmoid_4pl(cycles, *popt)
        params = {"a": popt[0], "b": popt[1], "c": popt[2], "d": popt[3]}
        
        ss_res = np.sum((fluor - fitted) ** 2)
        ss_tot = np.sum((fluor - np.mean(fluor)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
        
        return r2, fitted, params
        
    except Exception as e:
        return 0.0, np.zeros_like(fluor), {"error": str(e)}

def sigmoid_4pl(x, a, b, c, d):
    """4-Parameter Logistic Sigmoid"""
    return d + (a - d) / (1 + (x / c) ** b)


def fit_sigmoid(cycles: np.ndarray, fluor: np.ndarray) -> Tuple[float, np.ndarray, Dict]:
    """
    Sigmoid fitting ìˆ˜í–‰
    Returns: (r2, fitted_values, params_dict)
    """
    try:
        a_init = np.min(fluor)
        d_init = np.max(fluor)
        c_init = cycles[len(cycles) // 2]
        b_init = 1.0
        
        popt, _ = curve_fit(
            sigmoid_4pl, cycles, fluor,
            p0=[a_init, b_init, c_init, d_init],
            bounds=([0, 0.1, 1, 0], [np.inf, 50, 100, np.inf]),
            maxfev=5000
        )
        
        fitted = sigmoid_4pl(cycles, *popt)
        params = {"a": popt[0], "b": popt[1], "c": popt[2], "d": popt[3]}
        
        ss_res = np.sum((fluor - fitted) ** 2)
        ss_tot = np.sum((fluor - np.mean(fluor)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
        
        return r2, fitted, params
        
    except Exception as e:
        return 0.0, np.zeros_like(fluor), {"error": str(e)}

# âœ… set_page_configëŠ” ë°˜ë“œì‹œ 1ë²ˆë§Œ, ê·¸ë¦¬ê³  ìµœìƒë‹¨ì—ì„œ
st.set_page_config(page_title="CPHOTONICS | Early Ct Predictor", layout="wide")
PROJECT_ROOT = Path(__file__).resolve().parents[1]

# âœ… ê²½ë¡œëŠ” PROJECT_ROOT ê¸°ì¤€ìœ¼ë¡œ
ASSETS_DIR = PROJECT_ROOT / "assets"
CATALOG_PATH = ASSETS_DIR / "data_catalog.json"
QC_DIR = PROJECT_ROOT / "outputs" / "qc"  # âœ… QC_DIR ì •ì˜ ì¶”ê°€

OUTPUTS_DIR = PROJECT_ROOT / "outputs" / "qc_performance_analysis"
MODELS_DIR = PROJECT_ROOT / "data" / "models" / "by_cutoff"
UPLOAD_DIR = PROJECT_ROOT / "data" / "uploads"

# ========================================
# GitHub Releaseì—ì„œ QC ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ
# ========================================
def load_data_catalog(catalog_path):
    try:
        with open(catalog_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def load_data_catalog(path: Path) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except Exception as e:
        st.error(f"Failed to read data_catalog.json: {e}")
        return {}

catalog = load_data_catalog(CATALOG_PATH)

def find_file_url_in_catalog(catalog: dict, filename: str) -> str | None:
    for item in catalog.get("files", []):
        if item.get("filename") == filename:
            return item.get("url")
    return None

def download_to_path(url: str, dst_path):
    dst_path.parent.mkdir(parents=True, exist_ok=True)
    # GitHub release assetì€ 302 redirectê°€ ëœ° ìˆ˜ ìˆì–´ì„œ urlretrieve/urllibê°€ ì•ˆì •ì 
    urllib.request.urlretrieve(url, dst_path)

def load_data_catalog_json(catalog_path):
    if not catalog_path.exists():
        return None
    with open(catalog_path, "r", encoding="utf-8") as f:
        return json.load(f)

def ensure_asset_download(url: str, dst_path):
    dst_path.parent.mkdir(parents=True, exist_ok=True)
    if dst_path.exists() and dst_path.stat().st_size > 0:
        return False  # already exists

    # GitHub release assetì€ redirectê°€ ìˆì„ ìˆ˜ ìˆì–´ urllibê°€ ì•ˆì „í•¨
    with urllib.request.urlopen(url) as r, open(dst_path, "wb") as f:
        f.write(r.read())
    return True

def download_qc_data_from_github():
    """
    GitHub Releaseì—ì„œ QC ê´€ë ¨ parquet íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œ
    
    ì‚¬ìš©ë²•:
    1. GitHubì—ì„œ Release ìƒì„±
    2. QC parquet íŒŒì¼ë“¤ì„ Releaseì— ì²¨ë¶€
    3. ì´ í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
    """
    # âœ… ì—¬ê¸°ì— ì‹¤ì œ GitHub Release URL ì…ë ¥
    GITHUB_RELEASE_URL = "https://github.com/YOUR_USERNAME/YOUR_REPO/releases/download/v1.0.0/"
    
    QC_DIR.mkdir(parents=True, exist_ok=True)
    
    # ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ ëª©ë¡
    files_to_download = [
        "master_catalog.parquet",
        "excluded_report.parquet",
    ]
    
    for filename in files_to_download:
        local_path = QC_DIR / filename
        
        # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if local_path.exists():
            continue
        
        url = GITHUB_RELEASE_URL + filename
        
        try:
            st.info(f"Downloading {filename} from GitHub Release...")
            urllib.request.urlretrieve(url, local_path)
            st.success(f"âœ… Downloaded: {filename}")
        except urllib.error.HTTPError as e:
            st.warning(f"âš ï¸ Failed to download {filename}: {e}")
        except Exception as e:
            st.warning(f"âš ï¸ Error downloading {filename}: {e}")


# Streamlit Cloudì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ ìë™ ë‹¤ìš´ë¡œë“œ
def running_on_streamlit_cloud() -> bool:
    return str(PROJECT_ROOT).startswith("/mount/src") or os.environ.get("STREAMLIT_RUNTIME_ENV") == "cloud"


if running_on_streamlit_cloud():
    # Cloudì—ì„œëŠ” QC ë°ì´í„°ë¥¼ GitHub Releaseì—ì„œ ë‹¤ìš´ë¡œë“œ
    qc_local_path = QC_DIR / "master_catalog.parquet"

    if not qc_local_path.exists():
        if not catalog:
            st.error("data_catalog.json not loaded (catalog is None/empty).")
        else:
            found = False
            for item in catalog.get("files", []):
                if item.get("filename") == "master_catalog.parquet":
                    ensure_asset_download(item["url"], qc_local_path)
                    found = True
                    break

            if not found:
                st.error("master_catalog.parquet entry not found in assets/data_catalog.json")


# âœ… cutoff ë¨¼ì € ì •ì˜
cutoff = int(st.sidebar.selectbox("Cutoff", [10, 20, 24, 30, 40], index=1))

OPS_DIR = PROJECT_ROOT / "outputs" / "qc_performance_analysis"
OPS_DIR.mkdir(parents=True, exist_ok=True)

ops_filename = f"ops_decisions_cutoff_{cutoff}.parquet"
parquet_path = OPS_DIR / ops_filename
csv_path     = OPS_DIR / f"ops_decisions_cutoff_{cutoff}.csv"

# âœ… Streamlit Cloudì—ì„œ ops decisions parquet ìë™ ë‹¤ìš´ë¡œë“œ
if running_on_streamlit_cloud():
    if not parquet_path.exists():
        ops_url = find_file_url_in_catalog(catalog, ops_filename)
        if ops_url:
            try:
                download_to_path(ops_url, parquet_path)
            except Exception as e:
                st.warning(f"Failed to download ops decisions ({ops_filename}): {e}")
        else:
            st.warning(
                f"Ops decisions file for cutoff={cutoff} not found in data_catalog.json: {ops_filename}"
            )
if 'show_data_catalog' not in st.session_state:
    st.session_state.show_data_catalog = False


if st.session_state.show_data_catalog:
    st.header("ğŸ“Š Data Quality Control & Catalog")
    st.markdown("QC ìƒíƒœ(PASS/FAIL/FLAG), Ct bin, excluded ì‚¬ìœ ë¥¼ í•œ ë²ˆì— ì •ë¦¬/ë‹¤ìš´ë¡œë“œí•˜ëŠ” í˜ì´ì§€")

    # 1. master_catalog ë¡œë“œ
    @st.cache_data
    def load_master_catalog():
        path = QC_DIR / "master_catalog.parquet"
        if path.exists():
            return pd.read_parquet(path)
        else:
            st.error("master_catalog.parquet not found. Cloudì—ì„œëŠ” GitHub Releaseì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
            return pd.DataFrame()

    df = load_master_catalog()
    # ì»¬ëŸ¼ í™•ì¸ & ì•ˆì „ ì²˜ë¦¬
    if "exclusion_reason" not in df.columns or df["exclusion_reason"].isna().all():
        df["exclusion_reason"] = "No specific reason"  # N/A ëŒ€ì‹  ì˜ë¯¸ìˆëŠ” ê°’
    
    # qc_status, ct_bin ì•ˆì „ ì²˜ë¦¬
    df["qc_status"] = df["qc_status"].fillna("UNKNOWN").astype(str)
    df["ct_bin"] = df["ct_bin"].fillna("UNKNOWN").astype(str)
    
    # Exclusion Reasons ì°¨íŠ¸ ìˆ˜ì • (N/A ì œì™¸í•˜ê³  ì‹¤ì œ ì´ìœ ë§Œ)
    excluded_df = df[(df["qc_status"] != "PASS") & (df["exclusion_reason"] != "N/A") & (df["exclusion_reason"] != "No specific reason")]
    if not excluded_df.empty:
        reasons = excluded_df["exclusion_reason"].value_counts().head(10).reset_index()
        reasons = reasons[reasons["exclusion_reason"] != "N/A"]  # ê°•ì œ í•„í„°
        fig_ex = px.bar(reasons, x="count", y="exclusion_reason", orientation="h",
                        title="Top 10 Exclusion Reasons")
        fig_ex.update_layout(height=500)
        st.plotly_chart(fig_ex, use_container_width=True)
    else:
        st.info("Excluded ìƒ˜í”Œì´ ì—†ê±°ë‚˜ exclusion_reasonì´ ëª¨ë‘ N/Aì…ë‹ˆë‹¤.")
        
    if df.empty:
        st.stop()

    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ (ì—ëŸ¬ ë°©ì§€)
    required_cols = ["qc_status", "ct_bin"]
    if "exclusion_reason" not in df.columns:
        df["exclusion_reason"] = "N/A"

    # 2. Summary Statistics
    total = len(df)
    pass_c = len(df[df["qc_status"] == "PASS"])
    fail_c = len(df[df["qc_status"] == "FAIL"])
    flag_c = len(df[df["qc_status"] == "FLAG"])
    usable = pass_c
    excluded = total - usable

    col1, col2, col3, col4, col5, col6 = st.columns(6)
    col1.metric("Total Wells", f"{total:,}")
    col2.metric("âœ… PASS", f"{pass_c:,}", f"{pass_c/total*100:.1f}%")
    col3.metric("âŒ FAIL", f"{fail_c:,}", f"{fail_c/total*100:.1f}%")
    col4.metric("âš ï¸ FLAG", f"{flag_c:,}", f"{flag_c/total*100:.1f}%")
    col5.metric("ğŸŸ¢ Usable", f"{usable:,}", f"{usable/total*100:.1f}%")
    col6.metric("ğŸ”´ Excluded", f"{excluded:,}", f"{excluded/total*100:.1f}%")

    st.divider()

    # 3. Visualizations
        # 3. Visualizations
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("QC Status Distribution")
        status_counts = df["qc_status"].value_counts()
        if not status_counts.empty:
            fig_pie = px.pie(
                status_counts.reset_index(),
                values="count", names="qc_status",
                color_discrete_map={"PASS": "#00FF00", "FAIL": "#FF0000", "FLAG": "#FFA500", "UNKNOWN": "#808080"}
            )
            fig_pie.update_layout(showlegend=True)
            st.plotly_chart(fig_pie, use_container_width=True)
        else:
            st.info("QC Status ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("Ct Bin Distribution")
        ct_order = sorted(df["ct_bin"].dropna().unique())
        if not ct_order:
            st.info("Ct Bin ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            fig_ct = px.bar(
                df["ct_bin"].value_counts().reindex(ct_order).reset_index(),
                x="ct_bin", y="count"
            )
            st.plotly_chart(fig_ct, use_container_width=True)

    st.subheader("QC Status by Ct Bin")
    stacked = df.groupby(["ct_bin", "qc_status"]).size().reset_index(name="count")
    if not stacked.empty:
        stacked = stacked.sort_values("ct_bin")
        fig_stacked = px.bar(
            stacked, x="ct_bin", y="count", color="qc_status",
            color_discrete_map={"PASS": "#00FF00", "FAIL": "#FF0000", "FLAG": "#FFA500"},
            title="QC Status by Ct Bin"
        )
        st.plotly_chart(fig_stacked, use_container_width=True)
    else:
        st.info("QC Status by Ct Bin ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    excluded_df = df[df["qc_status"] != "PASS"].copy()
    if not excluded_df.empty:
        st.subheader("ğŸ” Exclusion Analysis - Top 10 Reasons")
        # ì‹¤ì œ ì´ìœ ë§Œ í•„í„° (N/A ì œì™¸)
        reasons = excluded_df[
            (excluded_df["exclusion_reason"] != "N/A") &
            (excluded_df["exclusion_reason"] != "No specific reason") &
            (excluded_df["exclusion_reason"].notna())
        ]["exclusion_reason"].value_counts().head(10).reset_index()
        
        if not reasons.empty and reasons["count"].sum() > 0:
            fig_ex = px.bar(reasons, x="count", y="exclusion_reason", orientation="h",
                            title="Top 10 Exclusion Reasons")
            fig_ex.update_layout(height=500, showlegend=False)
            st.plotly_chart(fig_ex, use_container_width=True)
        else:
            st.info("ì‹¤ì œ exclusion reasonì´ ì—†ìŠµë‹ˆë‹¤ (ëŒ€ë¶€ë¶„ N/A). QC ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì´ìœ ë¥¼ ë” ìì„¸íˆ ê¸°ë¡í•˜ì„¸ìš”.")
    else:
        st.info("Excluded ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")

    # 4. Filterable Table
    st.subheader("ğŸ“‹ Master Catalog (Filterable & Sortable)")
    try:
        from st_aggrid import AgGrid, GridOptionsBuilder
        gb = GridOptionsBuilder.from_dataframe(df)
        gb.configure_default_column(groupable=True, sortable=True, filterable=True, editable=False)
        gb.configure_column("qc_status", rowGroup=True)
        gb.configure_column("ct_bin", rowGroup=True)
        grid_options = gb.build()
        AgGrid(df, gridOptions=grid_options, height=600, fit_columns_on_grid_load=True)
    except ImportError:
        st.warning("AgGrid not available. Using basic table.")
        st.dataframe(df, use_container_width=True)

    # 5. Download Buttons
    st.subheader("ğŸ’¾ Download Reports")
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            "Download Master Catalog (CSV)",
            df.to_csv(index=False).encode('utf-8'),
            "master_catalog_full.csv",
            "text/csv"
        )
    with col2:
        st.download_button(
            "Download Excluded Report (CSV)",
            excluded_df.to_csv(index=False).encode('utf-8'),
            "excluded_report.csv",
            "text/csv"
        )

    # 6. ì–´ë‘ìš´ í…Œë§ˆ (ê²€ì€ìƒ‰ ë°°ê²½)
    st.markdown("""
    <style>
        .css-1d391kg {background-color: #0e1117;}
        .css-1y0t9cy {color: white;}
        section[data-testid="stSidebar"] {background-color: #262730;}
        .css-1cpxl2t {color: white;}
        h1, h2, h3, h4 {color: white !important;}
    </style>
    """, unsafe_allow_html=True)

    st.stop()

ops = None
try:
    if parquet_path.exists():
        ops = pd.read_parquet(parquet_path)
    elif csv_path.exists():
        ops = pd.read_csv(csv_path, encoding="utf-8")
    
    else:
        st.warning(f"Ops decisions not found: {parquet_path} (or {csv_path})")
except Exception as e:
    st.error(f"Failed to load ops decisions: {e}")
    st.caption(f"Checked: {parquet_path} , {csv_path}")

# âœ… ì¤‘ë³µ decision_from_qc í•¨ìˆ˜ ì œê±° (í•˜ë‚˜ë§Œ ìœ ì§€)
def decision_from_qc(qc_status: str) -> str:
    """QC ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš´ì˜ ê²°ì •"""
    qc_status = str(qc_status).upper().strip()
    if qc_status == "PASS":
        return "PREDICT"
    if qc_status == "FLAG":
        return "WARN"
    return "RERUN"
# ============================================
# PART 3: ê°œë³„ ë²„í‚· íŒì • í•¨ìˆ˜
# ============================================

def check_late_amplification(true_ct: Optional[float], pred_ct: float, threshold: float = 35.0) -> Tuple[bool, float, Dict]:
    """Late Amplification ì²´í¬"""
    ct_value = true_ct if true_ct is not None else pred_ct
    is_late = ct_value > threshold
    confidence = min(1.0, (ct_value - threshold) / 10.0) if is_late else 0.0
    
    return is_late, confidence, {
        "true_ct": true_ct, "pred_ct": pred_ct, 
        "threshold": threshold, "ct_used": ct_value
    }


def check_high_range(fluor: np.ndarray, max_thr: float = 50000, min_thr: float = -100) -> Tuple[bool, float, Dict]:
    """ê³¼ëŒ€ ë ˆì¸ì§€ ì²´í¬"""
    f_max, f_min = np.max(fluor), np.min(fluor)
    is_high = f_max > max_thr or f_min < min_thr
    
    confidence = 0.0
    if f_max > max_thr:
        confidence = max(confidence, min(1.0, (f_max - max_thr) / max_thr))
    if f_min < min_thr:
        confidence = max(confidence, min(1.0, abs(f_min - min_thr) / 1000))
    
    return is_high, confidence, {
        "fluor_max": float(f_max), "fluor_min": float(f_min),
        "max_threshold": max_thr, "min_threshold": min_thr
    }


def check_noisy(cycles: np.ndarray, fluor: np.ndarray, cutoff: int, 
                cv_thr: float = 0.15, snr_thr: float = 3.0) -> Tuple[bool, float, Dict]:
    """ë…¸ì´ì¦ˆ ì²´í¬ (CV, SNR)"""
    early_mask = cycles <= cutoff
    early_fluor = fluor[early_mask]
    
    if len(early_fluor) < 3:
        return False, 0.0, {"error": "early êµ¬ê°„ ë°ì´í„° ë¶€ì¡±"}
    
    early_mean = np.mean(early_fluor)
    early_std = np.std(early_fluor)
    cv = early_std / early_mean if early_mean != 0 else 0
    
    late_fluor = fluor[cycles > cutoff] if np.any(cycles > cutoff) else early_fluor
    signal = np.max(late_fluor) - np.mean(early_fluor)
    noise = early_std if early_std > 0 else 1e-6
    snr = signal / noise
    
    is_high_cv = cv > cv_thr
    is_low_snr = snr < snr_thr
    is_noisy = is_high_cv or is_low_snr
    
    confidence = 0.0
    if is_high_cv:
        confidence = max(confidence, min(1.0, (cv - cv_thr) / cv_thr))
    if is_low_snr:
        confidence = max(confidence, min(1.0, (snr_thr - snr) / snr_thr))
    
    return is_noisy, confidence, {
        "cv_early": float(cv), "cv_threshold": cv_thr,
        "snr": float(snr), "snr_threshold": snr_thr
    }


def check_non_sigmoid(cycles: np.ndarray, fluor: np.ndarray, r2_thr: float = 0.95) -> Tuple[bool, float, Dict]:
    """ë¹„ì‹œê·¸ëª¨ì´ë“œ ì²´í¬"""
    r2, fitted, params = fit_sigmoid(cycles, fluor)
    
    baseline = np.percentile(fluor, 10)
    plateau = np.percentile(fluor, 90)
    fold_change = plateau / baseline if baseline > 0 else 1.0
    
    slope, _, r_value, _, _ = linregress(cycles, fluor)
    is_increasing = slope > 0 and r_value > 0.5
    
    is_low_r2 = r2 < r2_thr
    is_flat = fold_change < 2.0
    is_non_sigmoid = is_low_r2 or is_flat or (not is_increasing)
    
    confidence = 0.0
    if is_low_r2:
        confidence = max(confidence, min(1.0, (r2_thr - r2) / r2_thr))
    if is_flat:
        confidence = max(confidence, min(1.0, (2.0 - fold_change) / 2.0))
    
    return is_non_sigmoid, confidence, {
        "r2_sigmoid": float(r2), "r2_threshold": r2_thr,
        "fold_change": float(fold_change), "slope": float(slope)
    }


# ============================================
# PART 4: í†µí•© ë¶„ë¥˜ í•¨ìˆ˜
# ============================================

def classify_hard_sample(
    curve_df: pd.DataFrame,
    true_ct: Optional[float],
    pred_ct: float,
    abs_error: float,
    cutoff: int,
    error_threshold: float = 2.0,
    late_amp_threshold: float = 35.0,
    fluor_max_threshold: float = 50000,
    fluor_min_threshold: float = -100,
    cv_threshold: float = 0.15,
    snr_threshold: float = 3.0,
    r2_threshold: float = 0.95,
) -> BucketResult:
    """Hard Sample í†µí•© ë¶„ë¥˜"""
    
    # Hard ì—¬ë¶€ íŒì •
    is_hard = abs_error >= error_threshold
    if not is_hard:
        return BucketResult(HardBucket.NORMAL, 0.0, {"abs_error": abs_error}, False)
    
    # ë°ì´í„° ì¶”ì¶œ ë° ì •ë ¬
    cycles = curve_df["Cycle"].values.astype(float)
    fluor = curve_df["Fluor"].values.astype(float)
    sort_idx = np.argsort(cycles)
    cycles, fluor = cycles[sort_idx], fluor[sort_idx]
    
    # ê° ë²„í‚· ì²´í¬
    checks = []
    
    is_late, conf_late, det_late = check_late_amplification(true_ct, pred_ct, late_amp_threshold)
    checks.append((HardBucket.LATE_AMP, is_late, conf_late, det_late))
    
    is_high, conf_high, det_high = check_high_range(fluor, fluor_max_threshold, fluor_min_threshold)
    checks.append((HardBucket.HIGH_RANGE, is_high, conf_high, det_high))
    
    is_noisy, conf_noisy, det_noisy = check_noisy(cycles, fluor, cutoff, cv_threshold, snr_threshold)
    checks.append((HardBucket.NOISY, is_noisy, conf_noisy, det_noisy))
    
    is_non_sig, conf_non_sig, det_non_sig = check_non_sigmoid(cycles, fluor, r2_threshold)
    checks.append((HardBucket.NON_SIGMOID, is_non_sig, conf_non_sig, det_non_sig))
    
    # ê°€ì¥ confidence ë†’ì€ ë²„í‚· ì„ íƒ
    triggered = [(b, c, d) for b, is_triggered, c, d in checks if is_triggered]
    
    if triggered:
        triggered.sort(key=lambda x: x[1], reverse=True)
        best_bucket, best_conf, best_details = triggered[0]
        
        all_details = {
            "primary_bucket": best_bucket.value,
            "all_checks": {
                "late_amp": det_late, "high_range": det_high,
                "noisy": det_noisy, "non_sigmoid": det_non_sig
            },
            "triggered_buckets": [b.value for b, _, _ in triggered],
            "abs_error": abs_error
        }
        
        return BucketResult(best_bucket, best_conf, all_details, True)
    
    # UNKNOWN
    return BucketResult(
        HardBucket.UNKNOWN, 0.5,
        {"abs_error": abs_error, "note": "No specific pattern detected"},
        True
    )


# ============================================
# PART 5: ë²„í‚·ë³„ ê¶Œì¥ì‚¬í•­
# ============================================

def get_bucket_recommendations(bucket: str) -> Dict:
    """ë²„í‚·ë³„ ì›ì¸ ë° ëŒ€ì‘ ì „ëµ"""
    recommendations = {
        "late_amp": {
            "ì›ì¸": "í…œí”Œë¦¿ ë†ë„ê°€ ë§¤ìš° ë‚®ê±°ë‚˜, ì¦í­ íš¨ìœ¨ì´ ë–¨ì–´ì§",
            "ëª¨ë¸ íŠ¹ì§•": "Early cycleì—ì„œ ì‹ í˜¸ ë³€í™”ê°€ ê±°ì˜ ì—†ì–´ ì˜ˆì¸¡ì´ ì–´ë ¤ì›€",
            "ëŒ€ì‘ ì „ëµ": [
                "Late Ct ìƒ˜í”Œì€ ë³„ë„ ëª¨ë¸ ë˜ëŠ” threshold ì ìš© ê³ ë ¤",
                "Ct > 35 ìƒ˜í”Œì€ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê²½ê³  í‘œì‹œ",
                "ì¬ê²€ì‚¬ ë˜ëŠ” í¬ì„ í›„ ì¬ê²€ì‚¬ ê¶Œì¥"
            ]
        },
        "high_range": {
            "ì›ì¸": "ì¥ë¹„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¬¸ì œ, ìƒ˜í”Œ ì˜¤ì—¼, ë˜ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ì˜¤ë¥˜",
            "ëª¨ë¸ íŠ¹ì§•": "ë¹„ì •ìƒì ì¸ Fluor ë²”ìœ„ë¡œ feature ê°’ì´ ì™œê³¡ë¨",
            "ëŒ€ì‘ ì „ëµ": [
                "ì›ë³¸ ë°ì´í„°ì™€ ì •í•©ì„± í™•ì¸ í•„ìš”",
                "ì¥ë¹„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìƒíƒœ ì ê²€",
                "í•´ë‹¹ ìƒ˜í”Œ ì œì™¸ í›„ ì¬ë¶„ì„ ê³ ë ¤"
            ]
        },
        "noisy": {
            "ì›ì¸": "ë‚®ì€ ì‹œê·¸ë„, ì¥ë¹„ ë…¸ì´ì¦ˆ, ë˜ëŠ” ìƒ˜í”Œ í’ˆì§ˆ ë¬¸ì œ",
            "ëª¨ë¸ íŠ¹ì§•": "Early êµ¬ê°„ ë³€ë™ì´ ì»¤ì„œ feature ì¶”ì¶œì´ ë¶ˆì•ˆì •",
            "ëŒ€ì‘ ì „ëµ": [
                "SNR ê¸°ë°˜ í’ˆì§ˆ í•„í„°ë§ ê°•í™”",
                "Smoothing ì „ì²˜ë¦¬ ì ìš© ê³ ë ¤",
                "Low-quality ìƒ˜í”Œ ì¬ê²€ì‚¬ ê¶Œì¥"
            ]
        },
        "non_sigmoid": {
            "ì›ì¸": "ë¹„ì •ìƒ ì¦í­ (ì–µì œ, ë¹„íŠ¹ì´ì  ì¦í­, primer-dimer ë“±)",
            "ëª¨ë¸ íŠ¹ì§•": "ì •ìƒ S-curve ê°€ì •ì´ ê¹¨ì ¸ ì˜ˆì¸¡ ì •í™•ë„ ì €í•˜",
            "ëŒ€ì‘ ì „ëµ": [
                "Sigmoid RÂ² ê¸°ë°˜ í’ˆì§ˆ í•„í„° ì ìš©",
                "Melting curve ë¶„ì„ìœ¼ë¡œ íŠ¹ì´ì„± í™•ì¸",
                "Primer ì¬ì„¤ê³„ ë˜ëŠ” ì¡°ê±´ ìµœì í™”"
            ]
        },
        "unknown": {
            "ì›ì¸": "ëª…í™•í•œ íŒ¨í„´ ì—†ì´ ì˜ˆì¸¡ ì˜¤ì°¨ ë°œìƒ",
            "ëª¨ë¸ íŠ¹ì§•": "ê¸°ì¡´ ë²„í‚·ìœ¼ë¡œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ì˜¤ì°¨",
            "ëŒ€ì‘ ì „ëµ": [
                "ê°œë³„ ì‚¬ë¡€ ì‹¬ì¸µ ë¶„ì„ í•„ìš”",
                "ìƒˆë¡œìš´ ì˜¤ë¥˜ íŒ¨í„´ ë°œêµ´ ê¸°íšŒ",
                "ì¶”ê°€ feature ì—”ì§€ë‹ˆì–´ë§ ê²€í† "
            ]
        }
    }
    return recommendations.get(bucket, {"ì›ì¸": "Unknown", "ëª¨ë¸ íŠ¹ì§•": "Unknown", "ëŒ€ì‘ ì „ëµ": []})


# ============================================
# PART 6: ë©”ì¸ UI í•¨ìˆ˜
# ============================================

def show_hard_review_with_buckets() -> None:
    """Hard Sample Review - ë²„í‚· ë¶„ë¥˜ í¬í•¨ ë²„ì „"""
    import altair as alt
    
    st.subheader("ğŸ§¨ Hard Sample Review (ë²„í‚· ë¶„ë¥˜)")

    model_id = get_active_model_id()
    pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"

    if not pred_path.exists():
        st.info(f"predictions_long.parquetê°€ ì—†ì–´ìš”: {pred_path}")
        return

    pred = pd.read_parquet(pred_path)
    pred = pred.copy()
    pred["abs_err"] = (pred["pred_ct"] - pred["true_ct"]).abs()

    c_list = sorted(pred["cutoff"].dropna().unique().astype(int).tolist())
    if not c_list:
        st.warning("cutoff ê°’ì´ ë¹„ì–´ìˆì–´ìš”.")
        return

    # ========== ì„¤ì • íŒ¨ë„ ==========
    st.markdown("### âš™ï¸ ì„¤ì •")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        best_cutoff = get_best_cutoff_from_report()
        default_idx = c_list.index(best_cutoff) if best_cutoff in c_list else 0
        cutoff = st.selectbox("Cutoff", c_list, index=default_idx, key="bucket_cutoff")
    
    with col2:
        error_threshold = st.slider("Hard ê¸°ì¤€ |error|", 0.5, 5.0, 2.0, 0.5, key="bucket_err_thr")
    
    with col3:
        topk = st.slider("ìµœëŒ€ í‘œì‹œ ê°œìˆ˜", 10, 200, 50, 10, key="bucket_topk")

    with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
        adv1, adv2 = st.columns(2)
        with adv1:
            late_amp_thr = st.number_input("Late Amp Ct ê¸°ì¤€", value=35.0, step=1.0)
            cv_thr = st.number_input("ë…¸ì´ì¦ˆ CV ê¸°ì¤€", value=0.15, step=0.01)
            r2_thr = st.number_input("ë¹„ì‹œê·¸ëª¨ì´ë“œ RÂ² ê¸°ì¤€", value=0.95, step=0.01)
        with adv2:
            fluor_max_thr = st.number_input("ê³¼ëŒ€ Fluor Max", value=50000.0, step=1000.0)
            snr_thr = st.number_input("ë…¸ì´ì¦ˆ SNR ê¸°ì¤€", value=3.0, step=0.5)

    st.divider()

    # ========== ë¶„ë¥˜ ì‹¤í–‰ ==========
    df = pred[pred["cutoff"] == int(cutoff)].copy()
    df = df.sort_values("abs_err", ascending=False).reset_index(drop=True)
    hard_candidates = df[df["abs_err"] >= error_threshold].head(topk).copy()
    
    if hard_candidates.empty:
        st.success(f"ğŸ‰ |error| >= {error_threshold} ì¸ ìƒ˜í”Œì´ ì—†ì–´ìš”!")
        return

    st.markdown("### ğŸ”„ ë²„í‚· ë¶„ë¥˜ ì¤‘...")
    progress = st.progress(0)
    
    results = []
    for i, (idx, row) in enumerate(hard_candidates.iterrows()):
        try:
            curve_df = load_curve_from_master(str(row["run_id"]), str(row["well_id"]))
            result = classify_hard_sample(
                curve_df=curve_df,
                true_ct=row.get("true_ct"),
                pred_ct=row["pred_ct"],
                abs_error=row["abs_err"],
                cutoff=int(cutoff),
                error_threshold=error_threshold,
                late_amp_threshold=late_amp_thr,
                fluor_max_threshold=fluor_max_thr,
                cv_threshold=cv_thr,
                snr_threshold=snr_thr,
                r2_threshold=r2_thr
            )
            results.append({
                "bucket": result.bucket.value,
                "confidence": result.confidence,
                "details": result.details
            })
        except Exception as e:
            results.append({"bucket": "error", "confidence": 0.0, "details": {"error": str(e)}})
        
        progress.progress((i + 1) / len(hard_candidates))
    
    classified = pd.concat([
        hard_candidates.reset_index(drop=True),
        pd.DataFrame(results)
    ], axis=1)
    progress.empty()

    # ========== 1. ë²„í‚· ë¶„í¬ ==========
    st.markdown("### ğŸ“Š ë²„í‚· ë¶„í¬")
    bucket_counts = classified["bucket"].value_counts()
    
    cols = st.columns(min(len(bucket_counts), 6))
    for i, (bucket, count) in enumerate(bucket_counts.items()):
        pct = count / len(classified) * 100
        with cols[i % len(cols)]:
            st.metric(f"{BUCKET_EMOJI.get(bucket, 'âšª')} {bucket}", f"{count}ê°œ", f"{pct:.1f}%")

    # ========== 2. Scatter ==========
    st.markdown("### ğŸ¯ Pred vs True (ë²„í‚·ë³„)")
    
    scatter = alt.Chart(classified).mark_circle(size=80, opacity=0.8).encode(
        x=alt.X("true_ct:Q", title="True Ct"),
        y=alt.Y("pred_ct:Q", title="Pred Ct"),
        color=alt.Color("bucket:N", scale=alt.Scale(
            domain=list(BUCKET_COLORS.keys()), range=list(BUCKET_COLORS.values())
        )),
        tooltip=["run_id", "well_id", "true_ct", "pred_ct", "abs_err", "bucket"]
    )
    
    x_min, x_max = classified["true_ct"].min(), classified["true_ct"].max()
    diag = alt.Chart(pd.DataFrame({"x": [x_min, x_max], "y": [x_min, x_max]})).mark_line(
        strokeDash=[5, 5], color="gray"
    ).encode(x="x:Q", y="y:Q")
    
    st.altair_chart((diag + scatter).properties(height=400).interactive(), use_container_width=True)

    # ========== 3. Box Plot ==========
    st.markdown("### ğŸ“ˆ ë²„í‚·ë³„ Error ë¶„í¬")
    box = alt.Chart(classified).mark_boxplot(size=40).encode(
        x=alt.X("bucket:N", sort=list(bucket_counts.index)),
        y=alt.Y("abs_err:Q", title="|Error|"),
        color=alt.Color("bucket:N", legend=None, scale=alt.Scale(
            domain=list(BUCKET_COLORS.keys()), range=list(BUCKET_COLORS.values())
        ))
    ).properties(height=300)
    st.altair_chart(box, use_container_width=True)

    # ========== 4. ê¶Œì¥ì‚¬í•­ íƒ­ ==========
    st.markdown("### ğŸ’¡ ë²„í‚·ë³„ ëŒ€ì‘ ì „ëµ")
    active_buckets = [b for b in bucket_counts.index if b not in ["normal", "error"]]
    
    if active_buckets:
        tabs = st.tabs([f"{BUCKET_EMOJI.get(b, 'âšª')} {b}" for b in active_buckets])
        
        for tab, bucket in zip(tabs, active_buckets):
            with tab:
                rec = get_bucket_recommendations(bucket)
                c1, c2 = st.columns(2)
                with c1:
                    st.markdown("**ğŸ” ì›ì¸**")
                    st.info(rec["ì›ì¸"])
                    st.markdown("**ğŸ¤– ëª¨ë¸ íŠ¹ì§•**")
                    st.warning(rec["ëª¨ë¸ íŠ¹ì§•"])
                with c2:
                    st.markdown("**ğŸ›  ëŒ€ì‘ ì „ëµ**")
                    for i, s in enumerate(rec["ëŒ€ì‘ ì „ëµ"], 1):
                        st.markdown(f"{i}. {s}")
                
                st.markdown(f"**ğŸ“‹ {bucket} ìƒ˜í”Œ ëª©ë¡**")
                st.dataframe(
                    classified[classified["bucket"] == bucket][
                        ["run_id", "well_id", "true_ct", "pred_ct", "abs_err", "confidence"]
                    ].sort_values("abs_err", ascending=False),
                    use_container_width=True, height=200
                )

    st.divider()

    # ========== 5. ê°œë³„ ë¶„ì„ ==========
    st.markdown("### ğŸ”¬ ê°œë³„ ìƒ˜í”Œ ë¶„ì„")
    
    def _fmt(i):
        r = classified.iloc[i]
        return f"{BUCKET_EMOJI.get(r['bucket'], 'âšª')} {r['run_id']}:{r['well_id']} | err={r['abs_err']:.2f}"
    
    pick = st.selectbox("ìƒ˜í”Œ ì„ íƒ", range(len(classified)), format_func=_fmt, key="bucket_pick")
    sel = classified.iloc[pick]
    
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Bucket", f"{BUCKET_EMOJI.get(sel['bucket'], 'âšª')} {sel['bucket']}")
    c2.metric("True Ct", f"{sel['true_ct']:.2f}")
    c3.metric("Pred Ct", f"{sel['pred_ct']:.2f}")
    c4.metric("|Error|", f"{sel['abs_err']:.3f}")

    # ê³¡ì„  ì‹œê°í™”
    try:
        curve_df = load_curve_from_master(str(sel["run_id"]), str(sel["well_id"]))
        if not curve_df.empty:
            curve_df = curve_df.sort_values("Cycle").reset_index(drop=True)
            cycles = curve_df["Cycle"].values.astype(float)
            fluor = curve_df["Fluor"].values.astype(float)
            r2, fitted, params = fit_sigmoid(cycles, fluor)
            curve_df["Fitted"] = fitted
            
            base = alt.Chart(curve_df).encode(x=alt.X("Cycle:Q"))
            orig = base.mark_line(color="steelblue").encode(y="Fluor:Q")
            fit_line = base.mark_line(color="red", strokeDash=[5,5]).encode(y="Fitted:Q")
            vline = alt.Chart(pd.DataFrame({"x": [int(cutoff)]})).mark_rule(
                strokeDash=[6,4], color="green"
            ).encode(x="x:Q")
            
            st.altair_chart(
                (orig + fit_line + vline).properties(height=350, title=f"RÂ²={r2:.4f}"),
                use_container_width=True
            )
    except Exception as e:
        st.error(f"ê³¡ì„  ë¡œë”© ì‹¤íŒ¨: {e}")

    # ========== 6. ë‹¤ìš´ë¡œë“œ ==========
    st.divider()
    st.markdown("### ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
    c1, c2 = st.columns(2)
    with c1:
        csv = classified.drop(columns=["details"], errors="ignore").to_csv(index=False)
        st.download_button("ğŸ“¥ ì „ì²´ ê²°ê³¼ CSV", csv.encode(), f"hard_buckets_cutoff{cutoff}.csv")
    with c2:
        summary = classified.groupby("bucket").agg(
            count=("bucket", "size"), mean_err=("abs_err", "mean")
        ).reset_index()
        st.download_button("ğŸ“¥ ìš”ì•½ CSV", summary.to_csv(index=False).encode(), "bucket_summary.csv")

# -------------------------
# Utilities
# -------------------------
def get_reports_root() -> Path:
    # 1) ê°€ì¥ ìš°ì„ : ë ˆí¬ ë£¨íŠ¸ì˜ reports/ (Streamlit Cloud ë°°í¬ìš©)
    p = Path("reports")
    if p.exists():
        return p

    # 2) (ë ˆê±°ì‹œ/ë¡œì»¬) app/data/reports ê°™ì€ ìœ„ì¹˜ë¥¼ ì“°ë˜ ê²½ìš° ëŒ€ë¹„
    p2 = Path(__file__).resolve().parent / "data" / "reports"
    if p2.exists():
        return p2

    # 3) ë§ˆì§€ë§‰ fallback
    return Path("reports")


REPORTS_ROOT = get_reports_root()


def has_canonical_master_long() -> bool:
    return (PROJECT_ROOT / "data" / "canonical" / "master_long.parquet").exists()


def running_on_streamlit_cloud() -> bool:
    # streamlit cloudëŠ” ë³´í†µ /mount/src ì•„ë˜ì—ì„œ ì‹¤í–‰ë¨
    return str(PROJECT_ROOT).startswith("/mount/src")

can_retrain = has_canonical_master_long() and (not running_on_streamlit_cloud())
if running_on_streamlit_cloud():
    pass
elif not has_canonical_master_long():
    pass

def get_active_model_id() -> str:
    p = REPORTS_ROOT / "active_model.txt"
    mid = p.read_text().strip() if p.exists() else "model_server_latest_xgb"
    mid = Path(mid).name
    return mid


def _line_y_eq_x(df: pd.DataFrame):
    # y=x ë¼ì¸ ê·¸ë¦¬ê¸° ìœ„í•œ DataFrame
    if df.empty:
        return pd.DataFrame({"x":[0,1], "y":[0,1]})
    lo = float(min(df["true_ct"].min(), df["pred_ct"].min()))
    hi = float(max(df["true_ct"].max(), df["pred_ct"].max()))
    pad = (hi - lo) * 0.05 if hi > lo else 1.0
    lo -= pad; hi += pad
    return pd.DataFrame({"x":[lo, hi], "y":[lo, hi]})

def plot_pred_vs_true_facets(pred_long: pd.DataFrame, cutoffs: list[int], ncol: int = 4) -> None:
    import altair as alt

    df = pred_long.dropna(subset=["cutoff", "true_ct", "pred_ct"]).copy()
    df["cutoff"] = df["cutoff"].astype(int)
    df = df[df["cutoff"].isin([int(c) for c in cutoffs])].copy()
    if df.empty:
        st.info("ì„ íƒí•œ cutoffì— í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ì–´ìš”.")
        return

    # (1) ì‚°ì ë„
    base = alt.Chart(df).mark_circle(size=60, opacity=0.75).encode(
        x=alt.X("true_ct:Q", title="True Ct/Cq"),
        y=alt.Y("pred_ct:Q", title="Pred Ct/Cq"),
        tooltip=["run_id", "well_id", "cutoff", "true_ct", "pred_ct"],
    )

    # (2) y=x ëŒ€ê°ì„ : â˜… ê°™ì€ dfë¥¼ ì“°ë˜ transformìœ¼ë¡œ 2ì ì§œë¦¬ ë¼ì¸ ìƒì„±
    #     (cutoff facetë§ˆë‹¤ lo/hi ê³„ì‚°ë˜ë„ë¡ aggregate -> calculate -> fold)
    diag = (
        alt.Chart(df)
        .transform_aggregate(
            min_true="min(true_ct)",
            min_pred="min(pred_ct)",
            max_true="max(true_ct)",
            max_pred="max(pred_ct)",
            groupby=["cutoff"],  # facet ë‹¨ìœ„ë¡œ ê°ê° lo/hi ë§Œë“¤ê¸°
        )
        .transform_calculate(
            lo="datum.min_true < datum.min_pred ? datum.min_true : datum.min_pred",
            hi="datum.max_true > datum.max_pred ? datum.max_true : datum.max_pred",
        )
        .transform_fold(["lo", "hi"], as_=["k", "v"])
        .transform_calculate(x="datum.v", y="datum.v")
        .mark_line()
        .encode(x="x:Q", y="y:Q")
    )

    chart = alt.layer(diag, base).facet(
        facet=alt.Facet("cutoff:N", title=None),
        columns=ncol,
    ).resolve_scale(
        x="independent",
        y="independent",
    ).properties(
        title="Pred vs True across Cycle Cutoffs"
    )

    st.altair_chart(chart, use_container_width=True)

import re

def normalize_well(x: object) -> str:
    """
    B2, b2, ' B2 '  -> 'B02'
    D07 -> 'D07'
    """
    s = str(x).strip().upper()
    m = re.fullmatch(r"([A-H])\s*0*([0-9]{1,2})", s)
    if not m:
        return s
    row = m.group(1)
    col = int(m.group(2))
    return f"{row}{col:02d}"


import altair as alt

def perf_accuracy_fraction_vs_cutoff(pred: pd.DataFrame, tol: float = 2.0) -> pd.DataFrame:
    """
    |pred-true| <= tol ë¹„ìœ¨ì„ cutoffë³„ë¡œ ê³„ì‚°
    """
    df = pred.dropna(subset=["cutoff", "true_ct", "pred_ct"]).copy()
    df["abs_err"] = (df["pred_ct"] - df["true_ct"]).abs()
    out = df.groupby("cutoff").apply(lambda g: (g["abs_err"] <= tol).mean()).reset_index(name="acc_frac")
    out["cutoff"] = out["cutoff"].astype(int)
    return out.sort_values("cutoff")

def plot_error_by_true_ct_scatter(
    pred: pd.DataFrame,
    cutoff: int,
    tol: float = 2.0,
    bin_width: float = 2.0,
) -> None:
    """
    ì¹œì ˆ ë²„ì „ Bias Plot:
    - y=0 ê¸°ì¤€ì„  (ê³¼ëŒ€/ê³¼ì†Œì˜ˆì¸¡ ë°”ë¡œ í•´ì„)
    - Â±tol band (ì‹¤ë¬´ í—ˆìš© ì˜¤ì°¨ ëŒ€ì—­)
    - True Ct êµ¬ê°„(bin)ë³„ í‰ê·  error ë¼ì¸ (biasê°€ ì–´ë””ì„œ ìƒê¸°ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ)
    """
    import altair as alt

    df = pred[pred["cutoff"] == int(cutoff)].dropna(subset=["true_ct", "pred_ct"]).copy()
    if df.empty:
        st.info("í•´ë‹¹ cutoffì— scatterë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ì–´ìš”.")
        return

    df["err"] = df["pred_ct"] - df["true_ct"]

    # x-range ì¡ê¸°
    x_min = float(df["true_ct"].min())
    x_max = float(df["true_ct"].max())
    pad = (x_max - x_min) * 0.03 if x_max > x_min else 1.0
    x_min -= pad
    x_max += pad

    # (A) Â±tol band ë°ì´í„°
    band_df = pd.DataFrame(
        {"x": [x_min, x_max], "y1": [-float(tol), -float(tol)], "y2": [float(tol), float(tol)]}
    )

    # (B) binë³„ í‰ê·  error (bias ë¼ì¸)
    bw = float(bin_width)
    if bw <= 0:
        bw = 2.0

    tmp = df[["true_ct", "err"]].copy()
    tmp["bin"] = np.floor(tmp["true_ct"] / bw) * bw
    grp = (
        tmp.groupby("bin")
        .agg(mean_err=("err", "mean"), n=("err", "size"))
        .reset_index()
        .sort_values("bin")
    )
    grp["bin_center"] = grp["bin"] + bw / 2.0

    # -----------------
    # ì°¨íŠ¸ ë ˆì´ì–´ êµ¬ì„±
    # -----------------

    # 1) tol band (ì—°í•œ ì˜ì—­)
    band = (
        alt.Chart(band_df)
        .mark_area(opacity=0.12)
        .encode(
            x=alt.X("x:Q", title="True Ct/Cq"),
            y=alt.Y("y1:Q", title="Error (pred - true)"),
            y2="y2:Q",
            tooltip=[
                alt.Tooltip("y1:Q", title="-tol"),
                alt.Tooltip("y2:Q", title="+tol"),
            ],
        )
    )

    # 2) y=0 ê¸°ì¤€ì„ 
    zero_line = (
        alt.Chart(pd.DataFrame({"y": [0.0]}))
        .mark_rule(strokeDash=[6, 4], opacity=0.6)
        .encode(y="y:Q")
    )

    # 3) ì (ìƒ˜í”Œë³„ error)
    points = (
        alt.Chart(df)
        .mark_circle(size=55, opacity=0.65)
        .encode(
            x=alt.X("true_ct:Q", title="True Ct/Cq"),
            y=alt.Y("err:Q", title="Error (pred - true)"),
            tooltip=[
                alt.Tooltip("run_id:N", title="run_id"),
                alt.Tooltip("well_id:N", title="well_id"),
                alt.Tooltip("true_ct:Q", title="true"),
                alt.Tooltip("pred_ct:Q", title="pred"),
                alt.Tooltip("err:Q", title="err (pred-true)"),
            ],
        )
    )

    # 4) bin í‰ê·  bias ë¼ì¸ + í¬ì¸íŠ¸
    bias_line = (
        alt.Chart(grp)
        .mark_line(point=True, opacity=0.9)
        .encode(
            x=alt.X("bin_center:Q"),
            y=alt.Y("mean_err:Q"),
            tooltip=[
                alt.Tooltip("bin_center:Q", title="Ct bin center"),
                alt.Tooltip("mean_err:Q", title="mean err"),
                alt.Tooltip("n:Q", title="n"),
            ],
        )
    )

    chart = (
        alt.layer(band, zero_line, points, bias_line)
        .properties(height=340)
        .interactive()
    )

    st.altair_chart(chart, use_container_width=True)
    st.caption(
        f"í•´ì„ íŒ: y=0 ìœ„(+)=ê³¼ëŒ€ì˜ˆì¸¡(ëŠ¦ê²Œ ë‚˜ì˜¨ë‹¤ê³  íŒë‹¨), ì•„ë˜(-)=ê³¼ì†Œì˜ˆì¸¡(ë¹¨ë¦¬ ë‚˜ì˜¨ë‹¤ê³  íŒë‹¨). "
        f"ì—°í•œ ì˜ì—­ì€ Â±{float(tol):.1f} ì˜¤ì°¨ ëŒ€ì—­, êµµì€ ì„ ì€ Ct êµ¬ê°„(bin={float(bin_width):.1f})ë³„ í‰ê·  ì˜¤ì°¨(=bias)ì…ë‹ˆë‹¤."
    )


def plot_pred_vs_true_hard_colored(df_cut: pd.DataFrame, hard_ids: set[tuple[str, str]] | None = None,
                                  highlight: tuple[str, str] | None = None) -> None:
    import altair as alt

    df = df_cut.dropna(subset=["true_ct", "pred_ct"]).copy()
    if df.empty:
        st.info("scatterë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ì–´ìš”.")
        return

    # hard ì—¬ë¶€
    if hard_ids is None:
        df["group"] = "Inlier"
    else:
        df["group"] = df.apply(lambda r: "Hard" if (str(r["run_id"]), str(r["well_id"])) in hard_ids else "Inlier", axis=1)

    # ì„ íƒ ìƒ˜í”Œ ê°•ì¡°
    if highlight is not None:
        hr, hw = highlight
        df["is_selected"] = (df["run_id"].astype(str) == str(hr)) & (df["well_id"].astype(str) == str(hw))
        df.loc[df["is_selected"], "group"] = "Selected"
    else:
        df["is_selected"] = False

    base = alt.Chart(df).mark_circle(size=70, opacity=0.85).encode(
        x=alt.X("true_ct:Q", title="True Ct/Cq"),
        y=alt.Y("pred_ct:Q", title="Pred Ct/Cq"),
        color=alt.Color("group:N", title="Group"),
        tooltip=["run_id", "well_id", "true_ct", "pred_ct", "abs_err"],
    )

    # y=x ì„ : ê°™ì€ dfë¥¼ ì“°ëŠ” transform ë°©ì‹(Altair ì•ˆì „)
    diag = (
        alt.Chart(df)
        .transform_aggregate(
            min_true="min(true_ct)",
            min_pred="min(pred_ct)",
            max_true="max(true_ct)",
            max_pred="max(pred_ct)",
        )
        .transform_calculate(
            lo="datum.min_true < datum.min_pred ? datum.min_true : datum.min_pred",
            hi="datum.max_true > datum.max_pred ? datum.max_true : datum.max_pred",
        )
        .transform_fold(["lo", "hi"], as_=["k", "v"])
        .transform_calculate(x="datum.v", y="datum.v")
        .mark_line()
        .encode(x="x:Q", y="y:Q")
    )

    st.altair_chart(alt.layer(diag, base).properties(height=380, title="Hard Samples highlighted on Pred vs True"), use_container_width=True)

def plot_uploaded_curve_preview(df_long: pd.DataFrame, cutoff: int, max_wells: int = 6) -> None:
    """ì—…ë¡œë“œí•œ df_longì—ì„œ ëª‡ ê°œ wellë§Œ ë½‘ì•„ ê³¡ì„  preview (ë™ì )"""
    if df_long.empty:
        st.info("df_longì´ ë¹„ì–´ìˆì–´ìš”.")
        return

    wells = sorted(df_long["Well"].dropna().unique().tolist())[:max_wells]
    sub = df_long[df_long["Well"].isin(wells)].copy()
    sub["segment"] = np.where(sub["Cycle"] <= int(cutoff), "early(<=cutoff)", "late")

    chart = (
        alt.Chart(sub)
        .mark_line()
        .encode(
            x=alt.X("Cycle:Q", title="Cycle"),
            y=alt.Y("Fluor:Q", title="Fluor"),
            color=alt.Color("Well:N", legend=alt.Legend(title="Well")),
            tooltip=["Well", "Cycle", "Fluor", "segment"],
        )
        .properties(height=320)
        .interactive()
    )

    vline = (
        alt.Chart(pd.DataFrame({"Cycle": [int(cutoff)]}))
        .mark_rule(strokeDash=[6, 4])
        .encode(x="Cycle:Q")
    )

    st.altair_chart(chart + vline, use_container_width=True)
    st.caption(f"ë¯¸ë¦¬ë³´ê¸°: {len(wells)}ê°œ wellë§Œ í‘œì‹œ (ì „ì²´ {df_long['Well'].nunique()} wells ì¤‘)")

def plot_pred_ct_hist(pred_df: pd.DataFrame) -> None:
    """ì˜ˆì¸¡ Ct ë¶„í¬ íˆìŠ¤í† ê·¸ë¨(ë™ì )"""
    if pred_df.empty or "pred_ct" not in pred_df.columns:
        return

    hist = (
        alt.Chart(pred_df)
        .mark_bar()
        .encode(
            x=alt.X("pred_ct:Q", bin=alt.Bin(maxbins=25), title="Predicted Ct"),
            y=alt.Y("count():Q", title="Count"),
            tooltip=[alt.Tooltip("count():Q", title="count")],
        )
        .properties(height=280)
    )
    st.altair_chart(hist, use_container_width=True)

def plot_cv_vs_ct(df_long: pd.DataFrame, pred_df: pd.DataFrame, cutoff: int) -> None:
    """
    ê°„ë‹¨í•œ í’ˆì§ˆì§€í‘œ(CV) vs Ct (ë™ì )
    - early êµ¬ê°„(<=cutoff)ì—ì„œ Fluorì˜ CV(std/mean)ë¥¼ ê³„ì‚°í•´ì„œ pred_ctì™€ ì—°ê²°
    """
    if df_long.empty or pred_df.empty:
        return

    early = df_long[df_long["Cycle"] <= int(cutoff)].copy()
    g = early.groupby(["run_id", "Well"])["Fluor"]
    cv = (g.std() / (g.mean().replace(0, np.nan))).reset_index()
    cv.rename(columns={"Fluor": "cv_early"}, inplace=True)

    m = pred_df.merge(cv, on=["run_id", "Well"], how="left")
    m = m.dropna(subset=["pred_ct", "cv_early"]).copy()
    if m.empty:
        st.info("CV vs Ctë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ë¶€ì¡±í•´ìš”.")
        return

    scat = (
        alt.Chart(m)
        .mark_circle(size=60)
        .encode(
            x=alt.X("pred_ct:Q", title="Predicted Ct"),
            y=alt.Y("cv_early:Q", title="CV (early <= cutoff)"),
            tooltip=["Well", "pred_ct", "cv_early"],
        )
        .properties(height=300)
        .interactive()
    )
    st.altair_chart(scat, use_container_width=True)
    st.caption("CVëŠ” early êµ¬ê°„ Fluorì˜ std/mean ê¸°ë°˜(ê°„ë‹¨ ë²„ì „).")


def get_best_cutoff_from_report() -> int | None:
    """train_report.csvì—ì„œ mae_test(ë˜ëŠ” mae) ìµœì†Œ cutoffë¥¼ ë°˜í™˜"""
    report_path = REPORTS_ROOT / "train_report.csv"
    if not report_path.exists():
        return None

    rep = pd.read_csv(report_path)
    cols = {str(c).lower(): c for c in rep.columns}
    cutoff_col = cols.get("cutoff")
    mae_col = cols.get("mae") or cols.get("mae_test")

    if not cutoff_col or not mae_col or rep.empty:
        return None

    best_row = rep.loc[rep[mae_col].idxmin()]
    return int(best_row[cutoff_col])

def _safe_stem(name: str) -> str:
    s = Path(name).stem
    s = re.sub(r"[^a-zA-Z0-9_\-]+", "_", s)
    return s[:80] if s else "uploaded"


def discover_cutoffs(models_dir: Path) -> list[int]:
    cutoffs: list[int] = []
    for p in models_dir.glob("ct_xgb_cutoff_*.json"):
        m = re.search(r"cutoff_(\d+)\.json$", p.name)
        if m:
            cutoffs.append(int(m.group(1)))
    return sorted(set(cutoffs))


@st.cache_resource
def load_booster(cutoff: int) -> xgb.Booster:
    model_path = MODELS_DIR / f"ct_xgb_cutoff_{cutoff}.json"
    booster = xgb.Booster()
    booster.load_model(str(model_path))
    return booster


def load_meta(cutoff: int) -> dict:
    meta_path = MODELS_DIR / f"ct_xgb_cutoff_{cutoff}.meta.json"
    if meta_path.exists():
        return json.loads(meta_path.read_text(encoding="utf-8"))
    return {}


def _drop_unnamed(df: pd.DataFrame) -> pd.DataFrame:
    cols = [c for c in df.columns if str(c).strip().lower().startswith("unnamed")]
    return df.drop(columns=cols) if cols else df


def infer_long_df(df: pd.DataFrame, run_id: str) -> pd.DataFrame:
    """
    ì—…ë¡œë“œ í…Œì´ë¸”ì„ ìµœëŒ€í•œ ê´€ëŒ€í•˜ê²Œ long í˜•íƒœë¡œ ë³€í™˜.
    ìµœì¢… ë°˜í™˜ ì»¬ëŸ¼: Cycle, Fluor, Well, run_id, well_uid

    ì§€ì› í¬ë§·:
      A) long: (Well, Cycle, Fluor/RFU/Signal)
      B) wide-1: (Well + cycle columns "1","2",... ë˜ëŠ” "Cycle 1"...)
      C) wide-2: (Cycle + well columns "C3","C5","A01"... )  <-- ë„ˆê°€ ë§í•œ ì—‘ì…€ í˜•íƒœ
    """
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    df = _drop_unnamed(df)

    cols_lower = {str(c).strip().lower(): c for c in df.columns}
    has_cycle = "cycle" in cols_lower

    # ---- A) long í˜•íƒœ: Cycle + (Fluor/RFU/Signal)
    fluor_key = None
    for k in ("fluor", "rfu", "signal"):
        if k in cols_lower:
            fluor_key = k
            break

    if has_cycle and fluor_key is not None:
        cycle_col = cols_lower["cycle"]
        fluor_col = cols_lower[fluor_key]
        well_col = (
            cols_lower.get("well")
            or cols_lower.get("well position")
            or cols_lower.get("well_position")
        )

        if well_col is None:
            # Wellì´ ì—†ìœ¼ë©´ í–‰ ë²ˆí˜¸ë¡œ ì„ì‹œ well ë¶€ì—¬
            df["Well"] = [f"R{i:03d}" for i in range(1, len(df) + 1)]
            well_col = "Well"

        out = df[[well_col, cycle_col, fluor_col]].copy()
        out.columns = ["Well", "Cycle", "Fluor"]

    # ---- C) Cycle + ì—¬ëŸ¬ well ì»¬ëŸ¼ (Cycleì´ í–‰)
    elif has_cycle:
        cycle_col = cols_lower["cycle"]
        well_cols = [c for c in df.columns if c != cycle_col]
        if not well_cols:
            raise ValueError("Cycle ì»¬ëŸ¼ì€ ìˆëŠ”ë° well ì»¬ëŸ¼(C3, C5, A01 ë“±)ì´ ì—†ì–´.")

        long = df.melt(
            id_vars=[cycle_col],
            value_vars=well_cols,
            var_name="Well",
            value_name="Fluor",
        )
        long.rename(columns={cycle_col: "Cycle"}, inplace=True)
        out = long[["Well", "Cycle", "Fluor"]].copy()

    # ---- B) Well + cycle ì»¬ëŸ¼ë“¤ (wide)
    else:
        well_col = None
        for cand in ["Well", "well", "WELL"]:
            if cand in df.columns:
                well_col = cand
                break
        if well_col is None:
            raise ValueError("Well ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆì–´. (ì˜ˆ: Well, well)")

        cycle_cols: list[str] = []
        for c in df.columns:
            if c == well_col:
                continue
            if re.fullmatch(r"\d+", str(c).strip()):
                cycle_cols.append(c)
            elif re.search(r"cycle\s*\d+", str(c).strip(), flags=re.IGNORECASE):
                cycle_cols.append(c)

        if not cycle_cols:
            raise ValueError("longë„ ì•„ë‹ˆê³  wide(Well+cycle cols)ë„ ì•„ë‹Œ ê²ƒ ê°™ì•„. (Cycle+well colsë„ ì•„ë‹˜)")

        tmp = df[[well_col] + cycle_cols].copy()
        long = tmp.melt(id_vars=[well_col], var_name="Cycle", value_name="Fluor")
        long["Cycle"] = long["Cycle"].astype(str).str.extract(r"(\d+)").astype(int)
        long.rename(columns={well_col: "Well"}, inplace=True)
        out = long[["Well", "Cycle", "Fluor"]].copy()

    # ---- ì •ë¦¬
    out = out.dropna(subset=["Well", "Cycle", "Fluor"]).copy()
    out["Cycle"] = pd.to_numeric(out["Cycle"], errors="coerce")
    out["Fluor"] = pd.to_numeric(out["Fluor"], errors="coerce")
    out = out.dropna(subset=["Cycle", "Fluor"]).copy()

    out["Cycle"] = out["Cycle"].astype(int)
    out["Well"] = out["Well"].astype(str).str.strip()

    out["run_id"] = run_id
    out["well_uid"] = out["run_id"].astype(str) + ":" + out["Well"].astype(str)

    return out[["Cycle", "Fluor", "Well", "run_id", "well_uid"]]


def predict_ct(df_long: pd.DataFrame, cutoff: int) -> pd.DataFrame:
    booster = load_booster(cutoff)
    X, meta = build_x_from_long(df_long, cutoff=cutoff)

    # feature_names mismatch ë°©ì§€ (metaì— ìˆìœ¼ë©´ ì‚¬ìš©)
    m = load_meta(cutoff)
    feat_cols = m.get("feat_cols") or m.get("feature_cols")
    if feat_cols:
        dmat = xgb.DMatrix(X, feature_names=list(feat_cols))
    else:
        dmat = xgb.DMatrix(X)

    pred = booster.predict(dmat)

    out = meta.copy()
    out["pred_ct"] = pred.astype(float)
    out["cutoff_used"] = cutoff
    return out.sort_values(["run_id", "Well"]).reset_index(drop=True)


def run_retrain(min_cutoff: int, max_cutoff: int) -> tuple[int, str]:
    if running_on_streamlit_cloud():
        return 2, "Streamlit Cloudì—ì„œëŠ” canonical ë°ì´í„°ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„œë²„/ë¡œì»¬ì—ì„œ í•™ìŠµ í›„ reports/ë§Œ ë°°í¬í•˜ì„¸ìš”."
    """
    í˜„ì¬ ì„œë²„ì— ìˆëŠ” canonical/master_long.parquet ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì „ì²´ ì¬í•™ìŠµ.
    (Streamlit ë²„íŠ¼ì—ì„œë„ GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ env ì „ë‹¬)
    """
    cmd = [
        sys.executable,
        "-m",
        "core.step3_train_and_save_models",
        "--min_cutoff",
        str(min_cutoff),
        "--max_cutoff",
        str(max_cutoff),
    ]

    env = dict(os.environ)

    # âœ… GPUë¥¼ ì“°ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨ (ì˜ˆ: 1ë²ˆ GPU ê³ ì •)
    env.setdefault("CUDA_VISIBLE_DEVICES", "1")

    p = subprocess.run(
        cmd,
        cwd=str(PROJECT_ROOT),
        capture_output=True,
        text=True,
        env=env,
    )
    log = (p.stdout or "") + "\n" + (p.stderr or "")
    return p.returncode, log

def split_excel_sheets(obj):
    """
    objê°€ dict(sheet_name -> df)ì¼ ë•Œ
    - curve_df: Cycle ì»¬ëŸ¼ ìˆëŠ” ì‹œíŠ¸(ìš°ì„  SYBR)
    - truth_df: Well + (Cq/Ct/true_ct) ìˆëŠ” ì‹œíŠ¸(ìš°ì„  Sheet1)
    """
    if not isinstance(obj, dict):
        return obj, None, None, None  # (curve_df, truth_df, curve_sheet, truth_sheet)

    # í›„ë³´ ìš°ì„ ìˆœìœ„
    curve_priority = ["SYBR", "Amplification", "Data", "Raw"]
    truth_priority = ["Sheet1", "Ct", "Cq", "Truth", "Result"]

    def norm_cols(df):
        return [str(c).strip().lower() for c in df.columns]

    # 1) curve sheet ì°¾ê¸°
    curve_df = None
    curve_sheet = None

    for nm in curve_priority:
        if nm in obj:
            cols = norm_cols(obj[nm])
            if "cycle" in cols:
                curve_df = obj[nm]
                curve_sheet = nm
                break

    if curve_df is None:
        for nm, df in obj.items():
            cols = norm_cols(df)
            if "cycle" in cols:
                curve_df = df
                curve_sheet = nm
                break

    # fallback: ì²« ì‹œíŠ¸
    if curve_df is None:
        curve_sheet = next(iter(obj.keys()))
        curve_df = obj[curve_sheet]

    # 2) truth sheet ì°¾ê¸°
    truth_df = None
    truth_sheet = None
    truth_keys = {"cq", "ct", "true_ct", "truect"}

    for nm in truth_priority:
        if nm in obj:
            cols = set(norm_cols(obj[nm]))
            if "well" in cols and len(cols & truth_keys) > 0:
                truth_df = obj[nm]
                truth_sheet = nm
                break

    if truth_df is None:
        for nm, df in obj.items():
            cols = set(norm_cols(df))
            if "well" in cols and len(cols & truth_keys) > 0:
                truth_df = df
                truth_sheet = nm
                break

    return curve_df, truth_df, curve_sheet, truth_sheet


def read_uploaded_table(up):
    name = (up.name or "").lower()
    raw = up.getvalue() if hasattr(up, "getvalue") else up.read()
    buf = io.BytesIO(raw)

    if name.endswith((".xlsx", ".xls")):
        # âœ… ëª¨ë“  ì‹œíŠ¸ ì½ê¸° (dict[str, DataFrame])
        return pd.read_excel(buf, sheet_name=None)
    return pd.read_csv(buf)


def sync_train_report_to_parquet(rep: pd.DataFrame) -> str:
    """
    train_report.csv(rep)ë¥¼ Performance í˜ì´ì§€ê°€ ì½ëŠ” parquetë¡œ ì €ì¥í•œë‹¤.

    ì €ì¥ ìœ„ì¹˜:
      <repo>/reports/<model_id>/metrics_by_cutoff.parquet
    ê·¸ë¦¬ê³ :
      <repo>/reports/active_model.txt ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
    """
    model_id = "model_server_latest_xgb"

    outdir = REPORTS_ROOT / model_id
    outdir.mkdir(parents=True, exist_ok=True)
    (REPORTS_ROOT / "active_model.txt").write_text(model_id, encoding="utf-8")
    cols = {str(c).lower(): c for c in rep.columns}
    cutoff_col = cols.get("cutoff")
    mae_col = cols.get("mae") or cols.get("mae_test")
    rmse_col = cols.get("rmse") or cols.get("rmse_test")

    if not (cutoff_col and mae_col and rmse_col):
        print("Missing cols:", {"cutoff": cutoff_col, "mae": mae_col, "rmse": rmse_col})
        print("Available:", list(rep.columns))
        return model_id

    rep2 = rep[[cutoff_col, mae_col, rmse_col]].copy()
    rep2 = rep2.rename(columns={cutoff_col: "cutoff", mae_col: "mae_test", rmse_col: "rmse_test"})

    # optional extras
    for extra in ["n_curves", "n_runs"]:
        if extra in cols:
            rep2[extra] = rep[cols[extra]].values

    rep2.to_parquet(outdir / "metrics_by_cutoff.parquet", index=False)

    (PROJECT_ROOT / "reports").mkdir(exist_ok=True)
    (PROJECT_ROOT / "reports" / "active_model.txt").write_text(model_id, encoding="utf-8")

    return model_id

def show_train_report() -> None:
    st.subheader("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ (ì„œë²„ í•™ìŠµ ê¸°ì¤€)")
    report_path = REPORTS_ROOT / "train_report.csv"
    if not report_path.exists():
        st.info("train_report.csvê°€ ì•„ì§ ì—†ì–´ìš”. ì¬í•™ìŠµ ì‹¤í–‰ í›„ ìƒì„±ë©ë‹ˆë‹¤.")
        return

    rep = pd.read_csv(report_path)

    # âœ… colsë¥¼ ë¨¼ì € ë§Œë“ ë‹¤ (ì—¬ê¸°ê°€ í•µì‹¬)
    cols = {str(c).lower(): c for c in rep.columns}
    cutoff_col = cols.get("cutoff")
    mae_col = cols.get("mae") or cols.get("mae_test")
    rmse_col = cols.get("rmse") or cols.get("rmse_test")
    ncurves_col = cols.get("n_curves")

    # Performance í˜ì´ì§€ìš© parquet ì €ì¥
    mid = sync_train_report_to_parquet(rep)
    st.caption(f"âœ… Performanceìš© ë¦¬í¬íŠ¸ ì €ì¥: reports/{mid}/metrics_by_cutoff.parquet")

    # âœ… ì¶”ì²œ cutoff ì¹´ë“œ (cols ë§Œë“  ë’¤ì—!)
    if cutoff_col and mae_col and rmse_col:
        best_row = rep.loc[rep[mae_col].idxmin()]
        c1, c2, c3 = st.columns(3)
        c1.metric("âœ… ì¶”ì²œ cutoff (MAE ìµœì†Œ)", int(best_row[cutoff_col]))
        c2.metric("ìµœì†Œ MAE", round(float(best_row[mae_col]), 4))
        c3.metric("í•´ë‹¹ RMSE", round(float(best_row[rmse_col]), 4))
        st.divider()

    # =========================
    # Figure ì¤‘ì‹¬ Performance
    # =========================
    import altair as alt

    # ë³´ê¸° ì˜µì…˜
    show_table = st.toggle("í‘œ(ì›ë³¸ rep)ë„ ê°™ì´ ë³´ê¸°", value=False)

    # ì»¬ëŸ¼ ì •ê·œí™”í•´ì„œ ì“°ê¸° í¸í•˜ê²Œ
    rep2 = rep.copy()
    rep2 = rep2.rename(columns={
        cutoff_col: "cutoff",
        mae_col: "mae",
        rmse_col: "rmse",
    })
    if ncurves_col:
        rep2 = rep2.rename(columns={ncurves_col: "n_curves"})
    if "n_runs" in cols:
        rep2 = rep2.rename(columns={cols["n_runs"]: "n_runs"})

    rep2["cutoff"] = pd.to_numeric(rep2["cutoff"], errors="coerce")
    rep2["mae"] = pd.to_numeric(rep2["mae"], errors="coerce")
    rep2["rmse"] = pd.to_numeric(rep2["rmse"], errors="coerce")
    if "n_curves" in rep2.columns:
        rep2["n_curves"] = pd.to_numeric(rep2["n_curves"], errors="coerce")
    if "n_runs" in rep2.columns:
        rep2["n_runs"] = pd.to_numeric(rep2["n_runs"], errors="coerce")

    rep2 = rep2.dropna(subset=["cutoff", "mae", "rmse"]).sort_values("cutoff").reset_index(drop=True)

    # ---- ìš”ì•½ ì¹´ë“œ(ì´ë¯¸ ìœ„ì—ì„œ metric ì°ê³  ìˆì–´ë„, ì—¬ê¸°ì„œ ë” ì§ê´€ì ìœ¼ë¡œ ë³´ê°• ê°€ëŠ¥) ----
    best_i = int(rep2["mae"].idxmin())
    best_row2 = rep2.loc[best_i]

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("âœ… Best cutoff (MAE ìµœì†Œ)", int(best_row2["cutoff"]))
    c2.metric("ìµœì†Œ MAE", round(float(best_row2["mae"]), 4))
    c3.metric("í•´ë‹¹ RMSE", round(float(best_row2["rmse"]), 4))
    if "n_curves" in rep2.columns and pd.notna(best_row2.get("n_curves", np.nan)):
        c4.metric("n_curves", int(best_row2["n_curves"]))
    elif "n_runs" in rep2.columns and pd.notna(best_row2.get("n_runs", np.nan)):
        c4.metric("n_runs", int(best_row2["n_runs"]))
    else:
        c4.metric("rows", int(len(rep2)))

    st.divider()

    # ---- (1) MAE / RMSE vs Cutoff: ì¸í„°ë™í‹°ë¸Œ ë¼ì¸ ----
    metric_choice = st.radio(
        "ë³´ê¸° ì„ íƒ",
        ["MAE vs Cutoff", "RMSE vs Cutoff", "MAE+RMSE ë‘˜ ë‹¤(ê²¹ì³ë³´ê¸°)"],
        horizontal=True,
    )

    base = alt.Chart(rep2).encode(
        x=alt.X("cutoff:Q", title="Cutoff"),
    )

    hover = alt.selection_point(fields=["cutoff"], on="mouseover", nearest=True, empty=False)

    def line_with_points(ycol: str, title: str):
        line = base.mark_line().encode(
            y=alt.Y(f"{ycol}:Q", title=title),
            tooltip=[alt.Tooltip("cutoff:Q", title="cutoff"), alt.Tooltip(f"{ycol}:Q", title=title)],
        )
        pts = base.mark_circle(size=70).encode(
            y=alt.Y(f"{ycol}:Q"),
            opacity=alt.condition(hover, alt.value(1.0), alt.value(0.15)),
            tooltip=[alt.Tooltip("cutoff:Q", title="cutoff"), alt.Tooltip(f"{ycol}:Q", title=title)],
        ).add_params(hover)

        vline = alt.Chart(rep2).mark_rule(strokeDash=[6, 4]).encode(
            x="cutoff:Q",
            opacity=alt.condition(hover, alt.value(0.6), alt.value(0.0)),
        ).transform_filter(hover)

        return (line + pts + vline).properties(height=320)

    if metric_choice == "MAE vs Cutoff":
        st.altair_chart(line_with_points("mae", "MAE"), use_container_width=True)

    elif metric_choice == "RMSE vs Cutoff":
        st.altair_chart(line_with_points("rmse", "RMSE"), use_container_width=True)

    else:
        # ê²¹ì³ë³´ê¸°(ë¡± í¬ë§·ìœ¼ë¡œ ë³€í™˜)
        longm = rep2.melt(id_vars=["cutoff"], value_vars=["mae", "rmse"], var_name="metric", value_name="value")
        longm["metric"] = longm["metric"].map({"mae": "MAE", "rmse": "RMSE"})

        hover2 = alt.selection_point(fields=["cutoff"], on="mouseover", nearest=True, empty=False)

        chart = alt.Chart(longm).encode(
            x=alt.X("cutoff:Q", title="Cutoff"),
            y=alt.Y("value:Q", title="Error"),
            tooltip=[
                alt.Tooltip("cutoff:Q", title="cutoff"),
                alt.Tooltip("metric:N", title="metric"),
                alt.Tooltip("value:Q", title="value"),
            ],
            strokeDash="metric:N",
        )

        line = chart.mark_line()
        pts = chart.mark_circle(size=70).encode(
            opacity=alt.condition(hover2, alt.value(1.0), alt.value(0.15)),
        ).add_params(hover2)

        vline = alt.Chart(rep2).mark_rule(strokeDash=[6, 4]).encode(
            x="cutoff:Q",
            opacity=alt.condition(hover2, alt.value(0.6), alt.value(0.0)),
        ).transform_filter(hover2)

        st.altair_chart((line + pts + vline).properties(height=320), use_container_width=True)

    st.divider()

    # ---- (2) n_curves vs Cutoff (ìˆì„ ë•Œë§Œ) ----
    if "n_curves" in rep2.columns and rep2["n_curves"].notna().any():
        st.markdown("#### ğŸ“¦ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ (#Curves vs Cutoff)")
        cov = alt.Chart(rep2).mark_line().encode(
            x=alt.X("cutoff:Q", title="Cutoff"),
            y=alt.Y("n_curves:Q", title="#Curves"),
            tooltip=[alt.Tooltip("cutoff:Q"), alt.Tooltip("n_curves:Q")],
        ).properties(height=220)
        st.altair_chart(cov, use_container_width=True)

    # =========================
    # (ì¶”ê°€) predictions_long ê¸°ë°˜ ë™ì  ì„±ëŠ¥ figure
    # =========================
    model_id = get_active_model_id()
    
    pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"


    if pred_path.exists():
        pred_long = pd.read_parquet(pred_path)

        st.markdown("### ğŸ“Œ ì¶”ê°€ ì„±ëŠ¥ Figure (ì„œë²„ í‰ê°€ ë¡œê·¸ ê¸°ë°˜)")

        tol = st.slider("ì •í™•ë„ ê¸°ì¤€ |error| <= ?", 0.5, 5.0, 2.0, 0.5, key="perf_tol")
        acc_df = perf_accuracy_fraction_vs_cutoff(pred_long, tol=float(tol))

        acc_chart = (
            alt.Chart(acc_df)
            .mark_line(point=True)
            .encode(
                x=alt.X("cutoff:Q", title="Cutoff"),
                y=alt.Y("acc_frac:Q", title=f"Accuracy Fraction (|err|<= {tol})"),
                tooltip=["cutoff", "acc_frac"],
            )
            .properties(height=260)
        )
        st.altair_chart(acc_chart, use_container_width=True)
        
        # =========================
        # (NEW) Pred vs True (Cutoff stepë³„ Small Multiples)
        # =========================
        st.markdown("### ğŸ“Œ Pred vs True (Cutoff stepë³„ Small Multiples)")

        step = st.radio("cutoff step", [3, 5], horizontal=True, key="pvst_step")
        cmin = int(pred_long["cutoff"].min())
        cmax = int(pred_long["cutoff"].max())
        rng = st.slider("cutoff ë²”ìœ„", min_value=cmin, max_value=cmax, value=(cmin, cmax), step=1, key="pvst_rng")
        cols_per_row = st.slider("í•œ ì¤„ì— ëª‡ ê°œ?", 2, 6, 4, 1, key="pvst_cols")

        cut_list = [c for c in range(rng[0], rng[1] + 1) if (c - rng[0]) % int(step) == 0]
        plot_pred_vs_true_facets(pred_long, cut_list, ncol=int(cols_per_row))

        st.divider()

        cutoff_sel = st.selectbox(
            "Scatter ë³¼ cutoff ì„ íƒ",
            sorted(pred_long["cutoff"].dropna().unique().astype(int).tolist()),
            key="perf_cutoff_sel",
        )
        st.markdown("#### Error vs True Ct (Bias í™•ì¸)")
        # âœ… ì¹œì ˆ ë²„ì „ ì˜µì…˜ (í‚¤ ì¤‘ë³µ ë°©ì§€ìš©ìœ¼ë¡œ perf_ prefix)
        bias_tol = st.slider("Bias í—ˆìš© ëŒ€ì—­(Â±tol)", 0.5, 5.0, 2.0, 0.5, key="perf_bias_tol")
        bias_binw = st.slider("Ct bin í­(í‰ê·  bias ê³„ì‚°)", 1.0, 6.0, 2.0, 0.5, key="perf_bias_binw")
        plot_error_by_true_ct_scatter(
            pred_long,
            cutoff=int(cutoff_sel),
            tol=float(bias_tol),
            bin_width=float(bias_binw),
        )

        dfc = pred_long[pred_long["cutoff"] == int(cutoff_sel)].dropna(subset=["true_ct", "pred_ct"]).copy()
        dfc["abs_err"] = (dfc["pred_ct"] - dfc["true_ct"]).abs()
        hist = (
            alt.Chart(dfc)
            .mark_bar()
            .encode(
                x=alt.X("abs_err:Q", bin=alt.Bin(maxbins=30), title="|Error|"),
                y=alt.Y("count():Q", title="Count"),
                tooltip=[alt.Tooltip("count():Q", title="count")],
            )
            .properties(height=240)
        )
        st.altair_chart(hist, use_container_width=True)

    else:
        st.info("ì¶”ê°€ figureë¥¼ ê·¸ë¦¬ë ¤ë©´ predictions_long.parquetê°€ í•„ìš”í•´ìš”. (Retrain í›„ ìƒì„±ë˜ëŠ” íŒŒì¼)")

    # ---- (ì˜µì…˜) í‘œ ----
    if show_table:
        st.markdown("#### ì›ë³¸ í…Œì´ë¸”")
        st.dataframe(rep, use_container_width=True)
        
        st.markdown("### ğŸ“Š Fold-change (ì¦í­ ë°°ìˆ˜) ë¶„ì„")
    
        st.info("""
        **Fold-change í•´ì„:**
        - Ct ì°¨ì´ 1 = 2ë°° ì¦í­ ì°¨ì´
        - ì„ìƒì  í—ˆìš© ë²”ìœ„: 1.5ë°° ì´ë‚´ (Î”Ct â‰ˆ 0.58)
        - 2ë°° ì´ìƒ ì°¨ì´: ìƒë¬¼í•™ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´
        """)
        
        # Fold-change ê³„ì‚°
        pred_long["fold_change"] = 2 ** abs(pred_long["pred_ct"] - pred_long["true_ct"])
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        fc_bins = [1.0, 1.2, 1.5, 2.0, 3.0, float('inf')]
        fc_labels = ["<1.2x (ìš°ìˆ˜)", "1.2-1.5x (ì–‘í˜¸)", "1.5-2x (ì£¼ì˜)", "2-3x (ë¶ˆëŸ‰)", ">3x (ì‹¬ê°)"]
        
        pred_long["fc_category"] = pd.cut(
            pred_long["fold_change"], 
            bins=fc_bins, 
            labels=fc_labels,
            include_lowest=True
        )
        
        # Cutoffë³„ Fold-change ë¶„í¬
        fc_by_cutoff = pred_long.groupby(["cutoff", "fc_category"]).size().reset_index(name="count")
        fc_by_cutoff = fc_by_cutoff.sort_values("cutoff")
        
        import altair as alt
        
        fc_chart = alt.Chart(fc_by_cutoff).mark_bar().encode(
            x=alt.X("cutoff:Q", title="Cutoff"),
            y=alt.Y("count:Q", title="Count"),
            color=alt.Color("fc_category:N", 
                           title="Fold-change Category",
                           scale=alt.Scale(scheme="redyellowgreen", reverse=True)),
            tooltip=["cutoff", "fc_category", "count"]
        ).properties(height=300)
        
        st.altair_chart(fc_chart, use_container_width=True)
        
        # ì „ì²´ ìš”ì•½
        st.markdown("#### ğŸ“ˆ ì „ì²´ Fold-change ìš”ì•½")
        
        fc_summary = pred_long["fc_category"].value_counts().sort_index()
        total = len(pred_long)
        
        summary_df = pd.DataFrame({
            "Category": fc_summary.index,
            "Count": fc_summary.values,
            "Percentage": (fc_summary.values / total * 100).round(1)
        })
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.dataframe(summary_df, use_container_width=True)
        
        with col2:
            # í—ˆìš© ê°€ëŠ¥(1.5x ì´ë‚´) ë¹„ìœ¨
            acceptable = pred_long["fold_change"] <= 1.5
            acceptable_rate = acceptable.sum() / total
            
            st.metric(
                "ì„ìƒ í—ˆìš© ë²”ìœ„ (â‰¤1.5x)",
                f"{acceptable.sum():,} / {total:,}",
                f"{acceptable_rate*100:.1f}%"
            )
            
            # ì‹¬ê°í•œ ì˜¤ì°¨ (>3x)
            severe = pred_long["fold_change"] > 3.0
            st.metric(
                "ì‹¬ê°í•œ ì˜¤ì°¨ (>3x)",
                f"{severe.sum():,}",
                f"{severe.sum()/total*100:.1f}%",
                delta_color="inverse"
            )
        
        st.divider()
    
    # =========================
    # (NEW) Ct ì˜¤ì°¨ì˜ ìƒë¬¼í•™ì  ì˜ë¯¸
    # =========================
    st.markdown("### ğŸ”¢ Ct ì˜¤ì°¨ì˜ ìƒë¬¼í•™ì  ì˜ë¯¸")
    
    with st.expander("ğŸ’¡ Ct ì˜¤ì°¨ í•´ì„ ê°€ì´ë“œ", expanded=False):
        meaning_df = pd.DataFrame({
            "Ct Error (Î”Ct)": [0.3, 0.5, 1.0, 1.5, 2.0, 3.0],
            "Fold Change": [1.23, 1.41, 2.0, 2.83, 4.0, 8.0],
            "ìƒë¬¼í•™ì  ì˜ë¯¸": [
                "ë¬´ì‹œ ê°€ëŠ¥ (ê¸°ìˆ ì  ë³€ë™ ë²”ìœ„)",
                "í—ˆìš© ë²”ìœ„ (Technical replicate SD)",
                "2ë°° ì°¨ì´ (ì£¼ì˜ í•„ìš”)",
                "ì„ìƒ íŒë‹¨ ê²½ê³„",
                "4ë°° ì°¨ì´ (ì„ìƒì ìœ¼ë¡œ ìœ ì˜ë¯¸)",
                "8ë°° ì°¨ì´ (ì‹¬ê°í•œ ì˜¤ì°¨)"
            ],
            "ê¶Œì¥ ì¡°ì¹˜": [
                "ì •ìƒ",
                "ì •ìƒ",
                "ì¬í™•ì¸ ê¶Œì¥",
                "ì¬ê²€ì‚¬ ê³ ë ¤",
                "ì¬ê²€ì‚¬ í•„ìš”",
                "ìƒ˜í”Œ/ëª¨ë¸ ì¬ê²€í† "
            ]
        })
        
        st.table(meaning_df)
        
        st.caption("""
        **ì°¸ê³  ë¬¸í—Œ:**
        - Technical replicate SD: < 0.5 Ct (Bustin et al., 2009)
        - Biological replicate SD: < 1.0 Ct (MIQE Guidelines)
        - Clinical threshold: Î”Ct < 1.5 (1.5ë°° ì°¨ì´)
        """)
    
    st.divider()

def try_eval_if_truth_exists(df_raw: pd.DataFrame, pred_df: pd.DataFrame, truth_df: pd.DataFrame | None = None) -> None:
    # âœ… truth_dfê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„ ìœ¼ë¡œ í‰ê°€
    src = truth_df if truth_df is not None else df_raw

    true_col = None
    for cand in ["true_ct", "TrueCt", "trueCt", "ct", "Ct", "CT", "Cq", "cq", "CQ"]:
        if cand in src.columns:
            true_col = cand
            break

    if true_col is None:
        st.info("ì—…ë¡œë“œ íŒŒì¼ì— ì •ë‹µ Ct/Cq ì»¬ëŸ¼(true_ct/ct/cq ë“±)ì´ ì—†ì–´ì„œ ì¦‰ì„ í‰ê°€ëŠ” ìƒëµí–ˆì–´ìš”.")
        return

    well_key = None
    for w in ["Well", "well", "WELL"]:
        if w in src.columns:
            well_key = w
            break

    eval_df = pred_df.copy()

    if well_key is not None and "Well" in eval_df.columns:
        truth2 = src[[well_key, true_col]].copy()
        truth2.columns = ["Well", "true_ct"]
        
        # âœ… Well í‘œì¤€í™” (í•µì‹¬)
        truth2["Well"] = truth2["Well"].map(normalize_well)
        eval_df["Well"] = eval_df["Well"].map(normalize_well)
        
        eval_df = eval_df.merge(truth2, on="Well", how="left")

    else:
        eval_df["true_ct"] = pd.to_numeric(src[true_col], errors="coerce").values[: len(eval_df)]

    eval_df["true_ct"] = pd.to_numeric(eval_df["true_ct"], errors="coerce")
    eval_df = eval_df.dropna(subset=["true_ct", "pred_ct"]).copy()
    if len(eval_df) == 0:
        st.warning("ì •ë‹µ Ct ì»¬ëŸ¼ì€ ì°¾ì•˜ëŠ”ë°, predì™€ ë§¤ì¹­ëœ ê°’ì´ ì—†ì–´ìš”. Well ì´ë¦„ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì¤˜.")
        return

    eval_df["err"] = eval_df["pred_ct"] - eval_df["true_ct"]
    mae = float(np.mean(np.abs(eval_df["err"])))
    rmse = float(np.sqrt(np.mean(eval_df["err"] ** 2)))

    st.markdown("### âœ… ì—…ë¡œë“œ ë°ì´í„° ì¦‰ì„ í‰ê°€")
    st.write({"MAE": mae, "RMSE": rmse, "n": int(len(eval_df))})

    st.markdown("**Pred vs True (ì‚°ì ë„)**")
    st.scatter_chart(eval_df[["true_ct", "pred_ct"]], x="true_ct", y="pred_ct", height=320)

    st.markdown("**Residual(ì˜¤ì°¨) ë¶„í¬**")
    # value_countsëŠ” íˆìŠ¤í† ê·¸ë¨ ëŠë‚Œì´ ì•½í•´ì„œ, binsë¡œ ê°€ë³ê²Œ
    bins = np.linspace(eval_df["err"].min(), eval_df["err"].max(), 30) if len(eval_df) > 3 else None
    if bins is not None:
        hist, edges = np.histogram(eval_df["err"].values, bins=bins)
        hist_df = pd.DataFrame({"err_bin_left": edges[:-1], "count": hist}).set_index("err_bin_left")
        st.line_chart(hist_df["count"], height=220)
    else:
        st.line_chart(eval_df["err"], height=220)

def load_curve_from_master(run_id: str, well_id: str) -> pd.DataFrame:
    """
    canonical master_long.parquetì—ì„œ (run_id, well_id) í•œ ê³¡ì„ ë§Œ ë¡œë“œ
    í•„ìš”í•œ ì»¬ëŸ¼ëª…ì´ í™˜ê²½ë§ˆë‹¤ ë‹¬ë¼ì„œ ìœ ì—°í•˜ê²Œ ë§¤í•‘í•¨.
    """
    path = PROJECT_ROOT / "data" / "canonical" / "master_long.parquet"
    if not path.exists():
        raise FileNotFoundError(f"master_long.parquet not found: {path}")

    dataset = ds.dataset(str(path))

    # ì»¬ëŸ¼ í›„ë³´ë“¤(í”„ë¡œì íŠ¸ë§ˆë‹¤ ì´ë¦„ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ)
    cols = set(dataset.schema.names)
    cycle_col = "Cycle" if "Cycle" in cols else ("cycle" if "cycle" in cols else None)
    fluor_col = "Fluor" if "Fluor" in cols else ("fluor" if "fluor" in cols else None)
    run_col   = "run_id" if "run_id" in cols else None
    
    # âœ… ì—¬ê¸°ê°€ í•µì‹¬: Wellì´ ìˆìœ¼ë©´ Wellì„ ìµœìš°ì„ ìœ¼ë¡œ
    if "Well" in cols:
        well_col = "Well"
    elif "well_id" in cols:
        well_col = "well_id"
    elif "well_uid" in cols:
        well_col = "well_uid"
    else:
        well_col = None
    
    if not all([cycle_col, fluor_col, run_col, well_col]):
        raise ValueError(f"master_long columns unexpected. found={sorted(cols)[:50]} ...")
    
    # âœ… well_uidë¥¼ ì“°ëŠ” ê²½ìš°ì—ëŠ” run_id:Well í˜•íƒœë¡œ ë§ì¶°ì¤Œ
    well_value = well_id
    if well_col == "well_uid":
        well_value = f"{run_id}:{well_id}"
    
    filt = (ds.field(run_col) == run_id) & (ds.field(well_col) == well_value)
    table = dataset.to_table(filter=filt, columns=[run_col, well_col, cycle_col, fluor_col])
    df = table.to_pandas()
    
    df = df.rename(columns={cycle_col: "Cycle", fluor_col: "Fluor"})
    df = df.sort_values("Cycle").reset_index(drop=True)
    return df


def load_one_curve_from_predictions_row(row) -> pd.DataFrame:
    """
    predictions_long.parquet rowì— curve_cycles_json / curve_fluor_json ì´ ìˆìœ¼ë©´
    ê·¸ê±¸ë¡œ ì›ë³¸ ê³¡ì„ ì„ ë³µì›í•œë‹¤ (Streamlit Cloud fallback).
    Returns: DataFrame with columns ["Cycle","Fluor"] sorted by Cycle
    """
    import json
    cycles_json = row.get("curve_cycles_json", "") if isinstance(row, dict) else getattr(row, "curve_cycles_json", "")
    fluor_json  = row.get("curve_fluor_json", "")  if isinstance(row, dict) else getattr(row, "curve_fluor_json", "")

    if not cycles_json or not fluor_json:
        raise ValueError("curve_cycles_json / curve_fluor_json is empty (retrain on server with curve embedding).")

    cycles = json.loads(cycles_json)
    fluor  = json.loads(fluor_json)

    df = pd.DataFrame({"Cycle": cycles, "Fluor": fluor})
    df = df.dropna().sort_values("Cycle").reset_index(drop=True)
    return df

def show_hard_review() -> None:
    st.subheader("ğŸ§¨ Hard Sample Review (ì„œë²„ í‰ê°€ ë¡œê·¸ ê¸°ë°˜)")

    # active model id
    model_id = get_active_model_id()

    pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"

    if not pred_path.exists():
        st.info(f"predictions_long.parquetê°€ ì—†ì–´ìš”: {pred_path}\nì¬í•™ìŠµì„ í•œ ë²ˆ ì‹¤í–‰í•´ì„œ ìƒì„±í•´ì¤˜.")
        return

    pred = pd.read_parquet(pred_path)
    need_cols = {"run_id", "well_id", "cutoff", "true_ct", "pred_ct"}
    if not need_cols.issubset(set(pred.columns)):
        st.error(f"predictions_long.parquet ì»¬ëŸ¼ì´ ì˜ˆìƒê³¼ ë‹¬ë¼ìš”. í•„ìš”: {need_cols}, í˜„ì¬: {set(pred.columns)}")
        return

    pred = pred.copy()
    pred["abs_err"] = (pred["pred_ct"] - pred["true_ct"]).abs()

    c_list = sorted(pred["cutoff"].dropna().unique().astype(int).tolist())
    if not c_list:
        st.warning("cutoff ê°’ì´ ë¹„ì–´ìˆì–´ìš”.")
        return

    col1, col2, col3 = st.columns([1.0, 1.0, 1.5])
    with col1:
        best_cutoff = get_best_cutoff_from_report()
        if best_cutoff in c_list:
            default_idx = c_list.index(best_cutoff)
        else:
            default_idx = min(len(c_list)-1, 0)
    
        cutoff = st.selectbox("cutoff ì„ íƒ", c_list, index=default_idx)
    with col2:
        topk = st.slider("Hard Top-K", min_value=5, max_value=200, value=30, step=5)
    with col3:
        err_thr = st.number_input("ë˜ëŠ” |error| ì„ê³„ê°’", value=0.0, step=0.5, help="0ì´ë©´ Top-K ê¸°ì¤€ë§Œ ì‚¬ìš©")

    df = pred[pred["cutoff"] == int(cutoff)].copy()
    if err_thr > 0:
        hard = df[df["abs_err"] >= float(err_thr)].sort_values("abs_err", ascending=False)
    else:
        hard = df.sort_values("abs_err", ascending=False).head(int(topk))
        
    # =========================
    # 0) ì „ì²´ í›„ë³´êµ° ëŒ€ë¹„ hard ì„ ì • ì´ìœ /ë­í¬ ë§Œë“¤ê¸°
    # =========================
    df = df.copy()
    df["err"] = df["pred_ct"] - df["true_ct"]
    df = df.sort_values("abs_err", ascending=False).reset_index(drop=True)
    df["rank_abs_err"] = np.arange(1, len(df) + 1)  # 1ì´ ê°€ì¥ hard
    
    # hard set í‘œì‹œ
    if err_thr > 0:
        rule_text = f"|error| â‰¥ {float(err_thr):.3f}"
        df["is_hard"] = df["abs_err"] >= float(err_thr)
    else:
        rule_text = f"Top-{int(topk)} by |error|"
        df["is_hard"] = df["rank_abs_err"] <= int(topk)
    
    hard = df[df["is_hard"]].copy()
    hard = hard.sort_values("abs_err", ascending=False).reset_index(drop=True)
    
    st.markdown("## ğŸ¯ Pred vs Trueì—ì„œ Hard ê°•ì¡° ë³´ê¸°")

    # hard id set ë§Œë“¤ê¸° (run_id, well_id)
    hard_ids = set(zip(hard["run_id"].astype(str), hard["well_id"].astype(str)))
    
    # (ì•„ì§ ì„ íƒ ìƒ˜í”Œ rid/wid ë§Œë“¤ê¸° ì „ì´ë©´ highlight=Noneìœ¼ë¡œ ë¨¼ì € ê·¸ë¦¬ê³ ,
    #  ì„ íƒ ìƒ˜í”Œ ì„ íƒ í›„ì— ë‹¤ì‹œ í•œ ë²ˆ ê·¸ë ¤ë„ ë¨)
    plot_pred_vs_true_hard_colored(
        df_cut=df,                 # í•´ë‹¹ cutoff ì „ì²´
        hard_ids=hard_ids,
        highlight=None
    )
    
    st.caption("ì„ (y=x)ì—ì„œ ë§ì´ ë²—ì–´ë‚œ ì ë“¤ì„ Hardë¡œ í‘œì‹œí•´ì„œ 'ìš°ë¦¬ê°€ ì´ê±¸ ë¶„ì„ ì¤‘'ì´ë¼ëŠ” ëŠë‚Œì„ ì¤ë‹ˆë‹¤.")
    
    # =========================
    # 1) Hard ì„ ì • ìš”ì•½ ì¹´ë“œ (ì „ì²´ ëŒ€ë¹„)
    # =========================
    st.markdown("## ğŸ” Hard ì„ ì • ìš”ì•½")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Cutoff", int(cutoff))
    c2.metric("ì „ì²´ í›„ë³´(í•´ë‹¹ cutoff)", int(len(df)))
    c3.metric("Hard í›„ë³´", int(len(hard)))
    hard_ratio = (len(hard) / len(df) * 100.0) if len(df) else 0.0
    c4.metric("Hard ë¹„ìœ¨", f"{hard_ratio:.1f}%")
    st.caption(f"Hard ì„ ì • ê¸°ì¤€: **{rule_text}**")
    st.divider()
    
    # =========================
    # 2) ì „ì²´ ë¶„í¬ì—ì„œ hard ìœ„ì¹˜ ë³´ê¸° (íˆìŠ¤í† ê·¸ë¨)
    # =========================
    st.markdown("## ğŸ“Š ì „ì²´ í›„ë³´ ëŒ€ë¹„ Hard ìœ„ì¹˜")
    import altair as alt
    
    hist_base = alt.Chart(df).mark_bar().encode(
        x=alt.X("abs_err:Q", bin=alt.Bin(maxbins=40), title="|error| (abs_err)"),
        y=alt.Y("count():Q", title="Count"),
        tooltip=[alt.Tooltip("count():Q", title="Count")]
    )
    
    layers = [hist_base]
    
    # threshold ë¼ì¸ ë˜ëŠ” topk ì»· ë¼ì¸
    if err_thr > 0:
        thr_df = pd.DataFrame({"abs_err_thr": [float(err_thr)]})
        thr_line = alt.Chart(thr_df).mark_rule(strokeDash=[6,4]).encode(x="abs_err_thr:Q")
        layers.append(thr_line)
    else:
        # TopKì˜ ë§ˆì§€ë§‰ abs_err ê°’ì„ ì»·ìœ¼ë¡œ í‘œì‹œ
        if len(hard) > 0:
            kth = float(hard["abs_err"].min())
            kth_df = pd.DataFrame({"abs_err_kth": [kth]})
            kth_line = alt.Chart(kth_df).mark_rule(strokeDash=[6,4]).encode(x="abs_err_kth:Q")
            layers.append(kth_line)
    
    st.altair_chart(alt.layer(*layers).properties(height=220), use_container_width=True)
    st.divider()
    
    # í‘œëŠ” ì ‘ì–´ì„œ ë³´ê¸° ì˜µì…˜
    with st.expander("ğŸ“‹ Hard í›„ë³´ í‘œ(ì ‘ê¸°/í¼ì¹˜ê¸°)", expanded=False):
        st.dataframe(
            hard[["run_id", "well_id", "true_ct", "pred_ct", "abs_err", "rank_abs_err"]],
            use_container_width=True,
            height=260
        )


    st.caption(f"model_id={model_id} / cutoff={cutoff} / candidates={len(hard)}")
    st.dataframe(hard[["run_id", "well_id", "true_ct", "pred_ct", "abs_err"]], use_container_width=True, height=320)

    # ---- ì„ íƒ ----
    if len(hard) == 0:
        return
    
    items = hard.reset_index(drop=True)  # 0..N-1 ì¸ë±ìŠ¤ ê³ ì •
    
    # âœ… ì„¸ì…˜ ì¸ë±ìŠ¤ í‚¤ (ì •ìˆ˜)
    if "hard_pick_i" not in st.session_state:
        st.session_state["hard_pick_i"] = 0
    
    # âœ… TopK/threshold ë°”ë€Œì–´ items ê¸¸ì´ê°€ ë°”ë€Œì–´ë„ ì•ˆì „í•˜ê²Œ clamp
    st.session_state["hard_pick_i"] = max(
        0, min(int(st.session_state["hard_pick_i"]), len(items) - 1)
    )
    
    st.caption(f"ğŸ“Œ {st.session_state['hard_pick_i']+1} / {len(items)}")
    
    def _fmt(i: int) -> str:
        r = items.iloc[i]
        return f"{r['run_id']} | {r['well_id']} | {r['abs_err']:.3f}"
    
    pick_i = st.selectbox(
        "ê²€í† í•  ìƒ˜í”Œ ì„ íƒ (run_id | well_id | abs_err)",
        options=list(range(len(items))),
        index=int(st.session_state["hard_pick_i"]),
        format_func=_fmt,
        key="hard_pick_i_selectbox",
    )
    
    # âœ… ì„ íƒê°’ ë™ê¸°í™” (ì‚¬ìš©ìê°€ ì§ì ‘ ë“œë¡­ë‹¤ìš´ìœ¼ë¡œ ë°”ê¿”ë„ idx ê°±ì‹ )
    st.session_state["hard_pick_i"] = int(pick_i)
    
    rid = str(items.loc[pick_i, "run_id"])
    wid = str(items.loc[pick_i, "well_id"])
    
    # =========================
    # 3) ì„ íƒ ìƒ˜í”Œì´ hardë¡œ ë“¤ì–´ì˜¨ ì´ìœ  ì„¤ëª…
    # =========================
    chosen = df[(df["run_id"] == rid) & (df["well_id"] == wid)]
    if len(chosen) > 0:
        chosen = chosen.iloc[0]
        st.markdown("### ğŸ§  ì™œ ì´ ìƒ˜í”Œì´ Hardì¸ê°€?")
        reason_lines = [
            f"- Hard ì„ ì • ê¸°ì¤€: **{rule_text}**",
            f"- ì´ ìƒ˜í”Œ |error| = **{float(chosen['abs_err']):.3f}** (err={float(chosen['err']):+.3f})",
            f"- ì „ì²´ í›„ë³´ {len(df)}ê°œ ì¤‘ |error| ìˆœìœ„: **{int(chosen['rank_abs_err'])}ìœ„**",
        ]
        if err_thr > 0:
            reason_lines.append(f"- ì„ê³„ê°’ {float(err_thr):.3f} {'í†µê³¼' if float(chosen['abs_err']) >= float(err_thr) else 'ë¯¸í†µê³¼'}")
        else:
            reason_lines.append(f"- Top-{int(topk)} {'í¬í•¨' if int(chosen['rank_abs_err']) <= int(topk) else 'ë¯¸í¬í•¨'}")
        st.write("\n".join(reason_lines))

    
    sel = items.iloc[int(pick_i)]
    st.markdown("### ì„ íƒ ìƒ˜í”Œ ìš”ì•½")
    st.write(
        {
            "run_id": rid,
            "well_id": wid,
            "cutoff": int(cutoff),
            "true_ct": float(sel["true_ct"]),
            "pred_ct": float(sel["pred_ct"]),
            "abs_err": float(sel["abs_err"]),
        }
    )
    
    # ---- í”Œë¡¯ ----
    st.markdown("### ğŸ“ˆ ì›ë³¸ qPCR ê³¡ì„  ë³´ê¸° (master_long ìš°ì„ , ì—†ìœ¼ë©´ predictions_long JSON fallback)")
    cutoff_i = int(cutoff)

    try:
        curve = None

        # 1) master_long ìˆìœ¼ë©´ ìš°ì„  ë¡œë“œ
        if has_canonical_master_long():
            curve = load_curve_from_master(rid, wid)

        # 2) ì—†ê±°ë‚˜(Cloud) / ëª»ì°¾ì•˜ê±°ë‚˜ / ë¹ˆ dfë©´ -> predictions_longì˜ JSONìœ¼ë¡œ ë³µì›
        if curve is None or len(curve) == 0:
            curve = load_one_curve_from_predictions_row(sel.to_dict())

        if curve is None or len(curve) == 0:
            st.info("ê³¡ì„  ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´. (master_longë„ ì—†ê³ , predictions_long JSONë„ ë¹„ì–´ìˆìŒ)")
        else:
            curve = curve.sort_values("Cycle").reset_index(drop=True)
            curve["segment"] = np.where(curve["Cycle"] <= cutoff_i, "early(<=cutoff)", "late")

            import altair as alt

            line = (
                alt.Chart(curve)
                .mark_line()
                .encode(
                    x=alt.X("Cycle:Q", title="Cycle"),
                    y=alt.Y("Fluor:Q", title="Fluor"),
                    tooltip=["Cycle", "Fluor", "segment"],
                )
            )
            vline = (
                alt.Chart(pd.DataFrame({"Cycle": [cutoff_i]}))
                .mark_rule(strokeDash=[6, 4])
                .encode(x="Cycle:Q")
            )
            st.altair_chart(line + vline, use_container_width=True)

            st.markdown("#### ğŸ” Early êµ¬ê°„ í™•ëŒ€ (<= cutoff)")
            early = curve[curve["Cycle"] <= cutoff_i].copy()
            if len(early) < 2:
                st.info("early êµ¬ê°„ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ í™•ëŒ€ í”Œë¡¯ì„ ëª» ê·¸ë ¤.")
            else:
                eline = (
                    alt.Chart(early)
                    .mark_line()
                    .encode(
                        x=alt.X("Cycle:Q", title="Cycle (early)"),
                        y=alt.Y("Fluor:Q", title="Fluor"),
                        tooltip=["Cycle", "Fluor"],
                    )
                )
                st.altair_chart(eline, use_container_width=True)

    except Exception as e:
        st.warning(f"ê³¡ì„  ë¡œë”©/í”Œë¡¯ ì‹¤íŒ¨: {e}")
    
    # ---- ë¹ ë¥¸ ì´ë™ ----
    st.markdown("### â­ï¸ ë¹ ë¥¸ ì´ë™")
    
    def _next_sample():
        # ë²„íŠ¼ ì½œë°±ì—ì„œë§Œ idx ì¦ê°€ (ì—¬ê¸°ì„œ setí•˜ë©´ Streamlitì´ ìì—°ìŠ¤ëŸ½ê²Œ reruní•¨)
        i = int(st.session_state.get("hard_pick_i", 0))
        if i < len(items) - 1:
            st.session_state["hard_pick_i"] = i + 1
            st.session_state["hard_pick_i_selectbox"] = i + 1  # selectboxë„ ê°™ì´ ë°€ì–´ì¤Œ(ì•ˆì „)
        else:
            st.session_state["hard_pick_i"] = len(items) - 1
    
    st.button("ë‹¤ìŒ ìƒ˜í”Œ ë³´ê¸°", type="secondary", key="btn_next_hard", on_click=_next_sample)

# -------------------------
# UI
# -------------------------
st.caption("ì—…ë¡œë“œí•œ qPCR curve ë°ì´í„°ë¡œ Ctë¥¼ ì˜ˆì¸¡í•˜ê±°ë‚˜, ì„œë²„ì— ëˆ„ì ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”.")

# -------------------------
# Sidebar (cutoff / retrain range)
# -------------------------
cutoffs = discover_cutoffs(MODELS_DIR)
if not cutoffs:
    st.error(f"ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆì–´: {MODELS_DIR} (ct_xgb_cutoff_*.json ì—†ìŒ)")
    st.stop()
    
# discover_cutoffs(MODELS_DIR) í›„, cutoff selectbox ì „ì— í†µì§¸ë¡œ

# discover_cutoffs ì•„ë˜, cutoff selectbox ì „ì— ì´ ì½”ë“œ ë„£ê¸° (ê¸°ì¡´ ì‚¬ì´ë“œë°” ì½”ë“œ ì‚­ì œ)

# discover_cutoffs(MODELS_DIR) í›„, cutoff selectbox ì „ì— ì´ ì½”ë“œë¡œ êµì²´

# ìˆ˜ì •ëœ ì½”ë“œ (ë²„íŠ¼ ë¶€ë¶„ë§Œ ì‚­ì œ):
with st.sidebar:
    st.title("CPHOTONICS | Early Ct Predictor")
    st.divider()
    
    # ê¸°ì¡´ cutoff ë“± (ì´ ì•„ë˜ ê·¸ëŒ€ë¡œ)
    best = get_best_cutoff_from_report()
    default_cutoff = best if (best in cutoffs) else (30 if 30 in cutoffs else cutoffs[-1] if cutoffs else 20)
    cutoff = int(st.selectbox(
        "Cutoff(ì‚¬ìš© cycle ìˆ˜)",
        cutoffs,
        index=cutoffs.index(default_cutoff) if default_cutoff in cutoffs else 0,
        key="sidebar_cutoff",
    ))
    
    st.divider()
    st.subheader("ì¬í•™ìŠµ (ì„œë²„ ë°ì´í„° ê¸°ì¤€)")
    min_c = st.number_input("min_cutoff", min_value=1, max_value=200, value=10, step=1, key="sidebar_min_c")
    max_c = st.number_input("max_cutoff", min_value=1, max_value=200, value=40, step=1, key="sidebar_max_c")

cutoff = int(cutoff)
min_c = int(min_c)
max_c = int(max_c)

# ìˆ˜ì •ëœ ì½”ë“œ:
tabs = st.tabs(["ğŸ“ˆ Performance", "ğŸ“Š Data Catalog", "ğŸ§ª Predict (Upload)", "ğŸ§¨ Hard Review", "ğŸ›  Retrain(Admin)"])

with tabs[0]:
    show_train_report()

# ============================================
# Data Catalog íƒ­ - ì™„ì „íˆ ìƒˆë¡œìš´ ë²„ì „
# streamlit_app.pyì˜ tabs[1] ë¶€ë¶„ì— ë„£ì„ ì½”ë“œ
# ============================================

with tabs[1]:  # Data Catalog íƒ­
    st.header("ğŸ“Š Data Quality Control & Catalog")
    st.markdown("QC ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ========================================
    # 1. QC Catalog ë¡œë“œ
    # ========================================
    @st.cache_data
    def load_qc_catalog():
        qc_path = QC_DIR / "master_catalog.parquet"
        if qc_path.exists():
            return pd.read_parquet(qc_path)
        return pd.DataFrame()
    
    qc_df = load_qc_catalog()
    
    if qc_df.empty:
        st.warning("âš ï¸ QC catalog not found")
        st.info("""
        **QC ë°ì´í„°ë¥¼ ìƒì„±í•˜ë ¤ë©´:**
        
        ì„œë²„ì—ì„œ ì‹¤í–‰:
        ```bash
        cd ~/qpcr_v2
        python scripts/save_qc_results.py
        ```
        """)
        st.stop()
    
    # ========================================
    # 2. ì „ì²´ ìš”ì•½ í†µê³„ (ìƒë‹¨ ì¹´ë“œ)
    # ========================================
    total = len(qc_df)
    pass_c = (qc_df['qc_status'] == 'PASS').sum()
    fail_c = (qc_df['qc_status'] == 'FAIL').sum()
    flag_c = (qc_df['qc_status'] == 'FLAG').sum()
    usable = qc_df['usable'].sum() if 'usable' in qc_df.columns else pass_c
    
    st.subheader("ğŸ“ˆ ì „ì²´ ìš”ì•½")
    col1, col2, col3, col4, col5 = st.columns(5)
    col1.metric("Total Wells", f"{total:,}")
    col2.metric("âœ… PASS", f"{pass_c:,}", f"{pass_c/total*100:.1f}%")
    col3.metric("âŒ FAIL", f"{fail_c:,}", f"{fail_c/total*100:.1f}%")
    col4.metric("âš ï¸ FLAG", f"{flag_c:,}", f"{flag_c/total*100:.1f}%")
    col5.metric("ğŸŸ¢ Usable", f"{usable:,}", f"{usable/total*100:.1f}%")
    
    st.divider()
    
    # ========================================
    # 3. ì‹œê°í™” (2ì—´)
    # ========================================
    st.subheader("ğŸ“Š QC ë¶„í¬ ì‹œê°í™”")
    
    viz_col1, viz_col2 = st.columns(2)
    
    with viz_col1:
        st.markdown("#### QC Status Distribution")
        status_counts = qc_df['qc_status'].value_counts()
        
        import plotly.express as px
        fig_pie = px.pie(
            values=status_counts.values,
            names=status_counts.index,
            color=status_counts.index,
            color_discrete_map={
                'PASS': '#00CC66',
                'FAIL': '#FF4444', 
                'FLAG': '#FFAA00'
            },
            hole=0.4
        )
        fig_pie.update_traces(textposition='inside', textinfo='percent+label')
        fig_pie.update_layout(showlegend=True, height=350)
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with viz_col2:
        st.markdown("#### Ct Bin Distribution")
        if 'ct_bin' in qc_df.columns:
            ct_counts = qc_df['ct_bin'].value_counts().sort_index()
            fig_bar = px.bar(
                x=ct_counts.index,
                y=ct_counts.values,
                labels={'x': 'Ct Bin', 'y': 'Count'},
                color=ct_counts.values,
                color_continuous_scale='Viridis'
            )
            fig_bar.update_layout(showlegend=False, height=350)
            st.plotly_chart(fig_bar, use_container_width=True)
        else:
            st.info("ct_bin ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ========================================
    # 4. Fail Reason ë¶„ì„ (Top 10)
    # ========================================
    if 'fail_reason' in qc_df.columns:
        st.subheader("ğŸ” Fail Reason Analysis (Top 10)")
        
        fail_df = qc_df[qc_df['qc_status'] != 'PASS'].copy()
        if not fail_df.empty:
            fail_counts = fail_df['fail_reason'].value_counts().head(10)
            
            fig_fail = px.bar(
                x=fail_counts.values,
                y=fail_counts.index,
                orientation='h',
                labels={'x': 'Count', 'y': 'Fail Reason'},
                color=fail_counts.values,
                color_continuous_scale='Reds'
            )
            fig_fail.update_layout(showlegend=False, height=400)
            st.plotly_chart(fig_fail, use_container_width=True)
        else:
            st.success("ğŸ‰ No failed samples!")
    
    st.divider()
    
    # ========================================
    # 5. QC Metrics ë¶„í¬ (r2, snr)
    # ========================================
    if 'r2' in qc_df.columns and 'snr' in qc_df.columns:
        st.subheader("ğŸ“ QC Metrics Distribution")
        
        metric_col1, metric_col2 = st.columns(2)
        
        with metric_col1:
            fig_r2 = px.histogram(
                qc_df,
                x='r2',
                nbins=50,
                labels={'r2': 'RÂ² Value'},
                title='RÂ² Distribution',
                color_discrete_sequence=['#3498db']
            )
            fig_r2.add_vline(x=0.98, line_dash="dash", line_color="red", 
                            annotation_text="Threshold (0.98)")
            st.plotly_chart(fig_r2, use_container_width=True)
        
        with metric_col2:
            fig_snr = px.histogram(
                qc_df,
                x='snr',
                nbins=50,
                labels={'snr': 'SNR Value'},
                title='SNR Distribution',
                color_discrete_sequence=['#e74c3c']
            )
            st.plotly_chart(fig_snr, use_container_width=True)
    
    st.divider()
    
    # ========================================
    # 6. í•„í„°ë§ ê°€ëŠ¥í•œ í…Œì´ë¸”
    # ========================================
    st.subheader("ğŸ” Detailed Data Table (Filterable)")
    
    # í•„í„° ì˜µì…˜
    filter_col1, filter_col2, filter_col3 = st.columns(3)
    
    with filter_col1:
        status_filter = st.multiselect(
            "QC Status",
            options=['PASS', 'FAIL', 'FLAG'],
            default=['PASS', 'FAIL', 'FLAG']
        )
    
    with filter_col2:
        if 'ct_bin' in qc_df.columns:
            ct_bins = sorted(qc_df['ct_bin'].dropna().unique())
            ct_filter = st.multiselect(
                "Ct Bin",
                options=ct_bins,
                default=ct_bins
            )
        else:
            ct_filter = None
    
    with filter_col3:
        search_well = st.text_input("Search Well ID", "")
    
    # í•„í„° ì ìš©
    filtered = qc_df[qc_df['qc_status'].isin(status_filter)].copy()
    
    if ct_filter and 'ct_bin' in qc_df.columns:
        filtered = filtered[filtered['ct_bin'].isin(ct_filter)]
    
    if search_well:
        if 'well_uid' in filtered.columns:
            filtered = filtered[filtered['well_uid'].str.contains(search_well, case=False, na=False)]
        elif 'Well' in filtered.columns:
            filtered = filtered[filtered['Well'].str.contains(search_well, case=False, na=False)]
    
    st.write(f"Showing **{len(filtered):,}** / **{total:,}** wells")
    
    # í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ
    display_cols = ['well_uid', 'run_id', 'Well', 'ct_value', 'ct_bin', 
                   'qc_status', 'fail_reason', 'usable', 'r2', 'snr']
    available_cols = [c for c in display_cols if c in filtered.columns]
    
    # í…Œì´ë¸” í‘œì‹œ
    st.dataframe(
        filtered[available_cols],
        use_container_width=True,
        height=500
    )
    
    # ========================================
    # 7. ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    # ========================================
    st.subheader("ğŸ’¾ Download Data")
    
    download_col1, download_col2 = st.columns(2)
    
    with download_col1:
        # ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        csv_all = qc_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ“¥ Download All Data (CSV)",
            data=csv_all,
            file_name=f"qc_catalog_all_{cutoff}.csv",
            mime="text/csv"
        )
    
    with download_col2:
        # í•„í„°ëœ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        csv_filtered = filtered.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ“¥ Download Filtered Data (CSV)",
            data=csv_filtered,
            file_name=f"qc_catalog_filtered_{cutoff}.csv",
            mime="text/csv"
        )
    
    # ========================================
    # 8. ìƒì„¸ í†µê³„ (ì ‘ê¸°/í¼ì¹˜ê¸°)
    # ========================================
    with st.expander("ğŸ“Š ìƒì„¸ í†µê³„ ë³´ê¸°", expanded=False):
        stat_col1, stat_col2, stat_col3 = st.columns(3)
        
        with stat_col1:
            st.markdown("**Ct Value í†µê³„**")
            if 'ct_value' in qc_df.columns:
                st.write({
                    "Mean": f"{qc_df['ct_value'].mean():.2f}",
                    "Median": f"{qc_df['ct_value'].median():.2f}",
                    "Std": f"{qc_df['ct_value'].std():.2f}",
                    "Min": f"{qc_df['ct_value'].min():.2f}",
                    "Max": f"{qc_df['ct_value'].max():.2f}"
                })
        
        with stat_col2:
            st.markdown("**RÂ² í†µê³„**")
            if 'r2' in qc_df.columns:
                st.write({
                    "Mean": f"{qc_df['r2'].mean():.4f}",
                    "Median": f"{qc_df['r2'].median():.4f}",
                    "Min": f"{qc_df['r2'].min():.4f}",
                    "< 0.98": f"{(qc_df['r2'] < 0.98).sum()} wells"
                })
        
        with stat_col3:
            st.markdown("**SNR í†µê³„**")
            if 'snr' in qc_df.columns:
                st.write({
                    "Mean": f"{qc_df['snr'].mean():.2f}",
                    "Median": f"{qc_df['snr'].median():.2f}",
                    "Min": f"{qc_df['snr'].min():.2f}",
                    "Max": f"{qc_df['snr'].max():.2f}"
                })
            
with tabs[2]:
    st.subheader("ğŸ§ª Predict (Upload)")
    up = st.file_uploader("qPCR íŒŒì¼ ì—…ë¡œë“œ (csv/xlsx)", type=["csv", "xlsx", "xls"])
    if up is None:
        st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì˜ˆì¸¡ì„ ì§„í–‰í•´ìš”.")
    else:
        run_id = _safe_stem(up.name) + "_" + datetime.now().strftime("%Y%m%d_%H%M%S")

        # âœ… 1) ì—…ë¡œë“œ ì½ê¸°
        raw_obj = read_uploaded_table(up)

        # âœ… 2) ì—‘ì…€ì´ë©´ ì‹œíŠ¸ ë¶„ë¦¬, CSVë©´ ê·¸ëŒ€ë¡œ
        df_curve, df_truth, curve_sheet, truth_sheet = split_excel_sheets(raw_obj)
        if isinstance(raw_obj, dict):
            st.write("ğŸ“„ Sheets:", list(raw_obj.keys()))
            for nm, df_ in raw_obj.items():
                st.write(f"--- {nm} ---")
                st.write("columns:", [str(c) for c in df_.columns])

        # âœ… 3) ë¯¸ë¦¬ë³´ê¸°
        if curve_sheet is not None:
            st.caption(f"ì—…ë¡œë“œ curve ì‹œíŠ¸: {curve_sheet}")
        st.dataframe(df_curve.head(30), use_container_width=True)

        if df_truth is not None:
            st.caption(f"ì •ë‹µ Ct/Cq ì‹œíŠ¸ ê°ì§€ë¨: {truth_sheet}")
            st.dataframe(df_truth.head(30), use_container_width=True)

        # âœ… 4) long ë³€í™˜ì€ curve_dfë¡œë§Œ!
        try:
            df_long = infer_long_df(df_curve, run_id=run_id)
        except Exception as e:
            st.error(f"ì—…ë¡œë“œ íŒŒì¼ì„ long í˜•íƒœë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            st.stop()
        
        # =========================
        # (NEW) Multi-cutoff Sweep (ì—…ë¡œë“œ ë°ì´í„°)
        # =========================
        do_sweep = st.toggle("ì—¬ëŸ¬ cutoffë¡œ í•œ ë²ˆì— ë¹„êµ(3/5 step)", value=False, key="do_sweep_upload")


        if do_sweep:
            step2 = st.radio("step", [3, 5], horizontal=True, key="upload_step")
            sweep_min = st.number_input("min cutoff", 1, 200, 10, 1, key="upload_min")
            sweep_max = st.number_input("max cutoff", 1, 200, 40, 1, key="upload_max")

            sweep_cutoffs = list(range(int(sweep_min), int(sweep_max) + 1, int(step2)))

            missing = []
            preds_all = []
            
            with st.spinner(f"Sweep ì˜ˆì¸¡ ì¤‘... (cutoffs={sweep_cutoffs})"):
                for c in sweep_cutoffs:
                    try:
                        p = predict_ct(df_long, cutoff=int(c))
                        p["cutoff"] = int(c)
            
                        # ì—…ë¡œë“œ ì˜ˆì¸¡ ê²°ê³¼ì— well_id ì—†ì„ ìˆ˜ ìˆì–´ì„œ ì•ˆì „í•˜ê²Œ ìƒì„±
                        if "well_id" not in p.columns:
                            if "Well" in p.columns:
                                p["well_id"] = p["Well"].astype(str)
                            elif "well_uid" in p.columns:
                                p["well_id"] = p["well_uid"].astype(str)
            
                        preds_all.append(p)
            
                    except Exception as e:
                        # ëª¨ë¸ íŒŒì¼ ì—†ëŠ” cutoff(=No such file...)ë©´ ì—¬ê¸°ë¡œ ë“¤ì–´ì˜´
                        missing.append((int(c), str(e)))
            
            if missing:
                st.warning("ì¼ë¶€ cutoffëŠ” ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ì„œ ìŠ¤í‚µí–ˆì–´: " + ", ".join(str(c) for c, _ in missing))
            
            if not preds_all:
                st.error("ì„ íƒí•œ cutoff ë²”ìœ„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ì–´. min/maxë¥¼ ì¤„ì—¬ì¤˜.")
                st.stop()
            
            preds_all = pd.concat(preds_all, ignore_index=True)


            # ---- truth ìˆìœ¼ë©´ cutoffë³„ ì„±ëŠ¥(MAE/RMSE) ----
            if df_truth is not None:
                tmp = preds_all.copy()

                # true ì»¬ëŸ¼ ì°¾ê¸°
                true_col = None
                for cand in ["true_ct", "ct", "Ct", "Cq", "cq", "CQ", "CT"]:
                    if cand in df_truth.columns:
                        true_col = cand
                        break

                # well ì»¬ëŸ¼ ì°¾ê¸°
                wcol = None
                for cand in ["Well", "well", "WELL"]:
                    if cand in df_truth.columns:
                        wcol = cand
                        break

                if true_col and wcol:
                    truth2 = df_truth[[wcol, true_col]].copy()
                    truth2.columns = ["Well", "true_ct"]

                    # âœ… Well í‘œì¤€í™”(ë§¤ì¹­ í•µì‹¬)
                    truth2["Well"] = truth2["Well"].map(normalize_well)
                    if "Well" in tmp.columns:
                        tmp["Well"] = tmp["Well"].map(normalize_well)
                    elif "well_id" in tmp.columns:
                        tmp["well_id"] = tmp["well_id"].map(normalize_well)

                    # merge í‚¤: tmpì— Wellì´ ìˆìœ¼ë©´ Wellë¡œ, ì—†ìœ¼ë©´ well_idë¡œ
                    if "Well" in tmp.columns:
                        tmp = tmp.merge(truth2, on="Well", how="left")
                    else:
                        tmp = tmp.merge(truth2.rename(columns={"Well": "well_id"}), on="well_id", how="left")

                    tmp["err"] = tmp["pred_ct"] - tmp["true_ct"]
                    g = tmp.dropna(subset=["true_ct", "pred_ct"]).groupby("cutoff")

                    perf = g.apply(lambda x: pd.Series({
                        "mae": float(np.mean(np.abs(x["err"]))),
                        "rmse": float(np.sqrt(np.mean(x["err"] ** 2))),
                        "n": int(len(x)),
                    })).reset_index()

                    st.markdown("#### cutoffë³„ MAE/RMSE")
                    pm = perf.melt(id_vars=["cutoff"], value_vars=["mae", "rmse"], var_name="metric", value_name="value")
                    st.altair_chart(
                        alt.Chart(pm).mark_line(point=True).encode(
                            x="cutoff:Q",
                            y="value:Q",
                            strokeDash="metric:N",
                            tooltip=["cutoff", "metric", "value", "n:Q"],
                        ).properties(height=260),
                        use_container_width=True
                    )

                    # ---- Pred vs True small multiples (sweep) ----
                    st.markdown("#### Pred vs True (Sweep Small Multiples)")
                    ncol = st.slider("í•œ ì¤„ì— ëª‡ ê°œ?", 2, 6, 4, 1, key="upload_sweep_cols")

                    # plot_pred_vs_true_facetsê°€ ìš”êµ¬í•˜ëŠ” ì»¬ëŸ¼ ë§ì¶”ê¸°
                    if "run_id" not in tmp.columns:
                        tmp["run_id"] = run_id
                    if "well_id" not in tmp.columns:
                        tmp["well_id"] = tmp["Well"].astype(str)

                    plot_pred_vs_true_facets(tmp.rename(columns={"pred_ct": "pred_ct", "true_ct": "true_ct"}), sweep_cutoffs, ncol=int(ncol))
                else:
                    st.info("Sweep ì„±ëŠ¥ì„ ê³„ì‚°í•˜ë ¤ë©´ truth ì‹œíŠ¸ì— Well + (ct/cq/true_ct) ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•´ìš”.")

            st.divider()

        # âœ… 5) ì˜ˆì¸¡
        pred_df = predict_ct(df_long, cutoff=int(cutoff))
        st.success("ì˜ˆì¸¡ ì™„ë£Œ!")

        # =========================
        # (NEW) Prediction ì‹ ë¢°ë„/ì •ë‹µë¥ (í™•ë¥ ) ìš”ì•½
        # =========================
        st.markdown("### âœ… ì˜ˆì¸¡ ì‹ ë¢°ë„ / ëª‡ ê°œ ë§ì·„ëŠ”ì§€")
        
        tol_u = st.slider(
            "ì •ë‹µ íŒì • ê¸°ì¤€ (|pred-true| <= tol)",
            0.5, 5.0, 2.0, 0.5,
            key="upload_tol_summary",
        )
        
        # --- ì„œë²„ ë¡œê·¸ ê¸°ë°˜ 'ì˜ˆìƒ ì •ë‹µë¥ (í™•ë¥ )' ê³„ì‚° (ìˆìœ¼ë©´) ---
        active_path = PROJECT_ROOT / "reports" / "active_model.txt"
        model_id = active_path.read_text().strip() if active_path.exists() else "model_server_latest_xgb"
        
        pred_path = PROJECT_ROOT / "reports" / model_id / "predictions_long.parquet"

        expected_rate = None
        if pred_path.exists():
            try:
                server_pred_long = pd.read_parquet(pred_path)
                acc_df_srv = perf_accuracy_fraction_vs_cutoff(server_pred_long, tol=float(tol_u))
                hit = acc_df_srv[acc_df_srv["cutoff"] == int(cutoff)]
                if not hit.empty:
                    expected_rate = float(hit["acc_frac"].iloc[0])
            except Exception:
                expected_rate = None
        
        # --- ì—…ë¡œë“œ íŒŒì¼ truthê°€ ìˆìœ¼ë©´ "ë§ì¶˜ ê°œìˆ˜" ê³„ì‚° ---
        def _build_eval_df_with_truth(pred_df: pd.DataFrame, df_truth: pd.DataFrame) -> pd.DataFrame:
            # true_ct column ì°¾ê¸°
            true_col = None
            for cand in ["true_ct", "TrueCt", "trueCt", "ct", "Ct", "CT", "Cq", "cq", "CQ"]:
                if cand in df_truth.columns:
                    true_col = cand
                    break
            if true_col is None:
                return pd.DataFrame()
        
            # Well column ì°¾ê¸°
            wcol = None
            for cand in ["Well", "well", "WELL"]:
                if cand in df_truth.columns:
                    wcol = cand
                    break
            if wcol is None:
                return pd.DataFrame()
        
            truth2 = df_truth[[wcol, true_col]].copy()
            truth2.columns = ["Well", "true_ct"]
        
            out = pred_df.copy()
            if "Well" not in out.columns:
                return pd.DataFrame()
        
            # Well í‘œì¤€í™”
            truth2["Well"] = truth2["Well"].map(normalize_well)
            out["Well"] = out["Well"].map(normalize_well)
        
            out = out.merge(truth2, on="Well", how="left")
            out["pred_ct"] = pd.to_numeric(out["pred_ct"], errors="coerce")
            out["true_ct"] = pd.to_numeric(out["true_ct"], errors="coerce")
            out = out.dropna(subset=["pred_ct", "true_ct"]).copy()
            if out.empty:
                return pd.DataFrame()
        
            out["abs_err"] = (out["pred_ct"] - out["true_ct"]).abs()
            out["within_tol"] = out["abs_err"] <= float(tol_u)
            return out
        
        eval_df = pd.DataFrame()
        if "pred_df" in locals() and pred_df is not None and df_truth is not None:
            eval_df = _build_eval_df_with_truth(pred_df, df_truth)
        else:
            st.info("ì˜ˆì¸¡ ê²°ê³¼(pred_df) ë˜ëŠ” truth(df_truth)ê°€ ì•„ì§ ì—†ì–´ì„œ ì‹ ë¢°ë„ ê³„ì‚°ì„ ìƒëµí–ˆì–´.")

        # --- UI ì¶œë ¥ (truth ìˆìœ¼ë©´ ì‹¤ì œ ì •ë‹µë¥  / ì—†ìœ¼ë©´ ì˜ˆìƒ ì •ë‹µë¥ ) ---
        cA, cB, cC, cD = st.columns(4)
        
        if not eval_df.empty:
            n_total = int(len(eval_df))
            n_hit = int(eval_df["within_tol"].sum())
            hit_rate = (n_hit / n_total) if n_total else 0.0
            mae_u = float(eval_df["abs_err"].mean())
            rmse_u = float(np.sqrt(np.mean((eval_df["pred_ct"] - eval_df["true_ct"]) ** 2)))
        
            cA.metric("ë§ì¶˜ ê°œìˆ˜", f"{n_hit} / {n_total}")
            cB.metric(f"ì •ë‹µë¥ (Â±{tol_u:g})", f"{hit_rate*100:.1f}%")
            cC.metric("MAE(ì—…ë¡œë“œ)", f"{mae_u:.3f}")
            cD.metric("RMSE(ì—…ë¡œë“œ)", f"{rmse_u:.3f}")
        
            if expected_rate is not None:
                st.caption(f"ì°¸ê³ : ì„œë²„ í‰ê°€ ë¡œê·¸ ê¸°ì¤€ ì´ cutoffì˜ **ì˜ˆìƒ ì •ë‹µë¥ (Â±{tol_u:g}) â‰ˆ {expected_rate*100:.1f}%**")
        
            st.progress(hit_rate)
            st.caption("í•´ì„: ì •ë‹µë¥ ì€ **|pred-true| <= tol** ë§Œì¡± ë¹„ìœ¨ì…ë‹ˆë‹¤.")
        
            # (ì˜µì…˜) Wellë³„ ì˜¤ì°¨ ë§‰ëŒ€ + ìƒ‰ìƒìœ¼ë¡œ ë§/í‹€ í‘œì‹œ
            with st.expander("ğŸ” Wellë³„ ì˜¤ì°¨(ë§/í‹€ ìƒ‰ìƒ) ë³´ê¸°", expanded=False):
                import altair as alt
                bar = (
                    alt.Chart(eval_df)
                    .mark_bar()
                    .encode(
                        x=alt.X("Well:N", sort=alt.SortField("abs_err", order="descending"), title="Well"),
                        y=alt.Y("abs_err:Q", title="|Error|"),
                        color=alt.Color("within_tol:N", title=f"within Â±{tol_u:g}", legend=None),
                        tooltip=["Well", "true_ct", "pred_ct", "abs_err", "within_tol"],
                    )
                    .properties(height=260)
                )
                st.altair_chart(bar, use_container_width=True)
        
        else:
            # truth ì—†ì„ ë•Œ: ì˜ˆìƒ ì •ë‹µë¥ ë§Œ
            if expected_rate is not None:
                cA.metric(f"ì˜ˆìƒ ì •ë‹µë¥ (Â±{tol_u:g})", f"{expected_rate*100:.1f}%")
                cB.metric("cutoff", int(cutoff))
                cC.metric("Wells", int(df_long["Well"].nunique()) if "Well" in df_long.columns else int(df_long["well_uid"].nunique()))
                cD.metric("ê¸°ì¤€", "ì„œë²„ ë¡œê·¸ ê¸°ë°˜")
                st.progress(expected_rate)
                st.caption(
                    f"truth(ì •ë‹µ Ct/Cq)ì´ ì—†ì–´ì„œ ì—…ë¡œë“œ ë°ì´í„°ì˜ 'ë§ì¶˜ ê°œìˆ˜'ëŠ” ê³„ì‚° ë¶ˆê°€. "
                    f"ëŒ€ì‹  ì„œë²„ í‰ê°€ ë¡œê·¸ì—ì„œ **ì´ cutoffê°€ Â±{tol_u:g} ì•ˆì— ë“¤ì–´ê°ˆ í™•ë¥ (ì˜ˆìƒ ì •ë‹µë¥ )** ì„ ë³´ì—¬ì¤˜ìš”."
                )
            else:
                st.info(
                    "truth(ì •ë‹µ Ct/Cq) ì‹œíŠ¸ë„ ì—†ê³ , ì„œë²„ í‰ê°€ ë¡œê·¸(predictions_long.parquet)ë„ ì—†ì–´ì„œ "
                    "ì •ë‹µë¥ /í™•ë¥  ìš”ì•½ì„ í‘œì‹œí•  ìˆ˜ ì—†ì–´ìš”."
                )

        # ---- ìš”ì•½ ì¹´ë“œ ----
        c1, c2, c3 = st.columns(3)
        c1.metric("Cutoff", int(cutoff))
        c2.metric("Wells", int(pred_df["Well"].nunique()) if "Well" in pred_df.columns else len(pred_df))
        c3.metric("Pred Ct (min ~ max)", f"{pred_df['pred_ct'].min():.3f} ~ {pred_df['pred_ct'].max():.3f}")

        st.divider()

        # âœ… í‘œëŠ” ê¸°ë³¸ ìˆ¨ê¹€(ì›í•˜ë©´ ì¼œê¸°)
        show_tables = st.toggle("í‘œë„ ê°™ì´ ë³´ê¸°(ë””ë²„ê¹…ìš©)", value=False, key="pred_show_tables")

        # ---- figure 1) ì—…ë¡œë“œ ê³¡ì„  preview ----
        st.markdown("### ğŸ“ˆ ì—…ë¡œë“œ ê³¡ì„  Preview")
        plot_uploaded_curve_preview(df_long, cutoff=int(cutoff), max_wells=6)

        # ---- figure 2) Pred Ct ë¶„í¬ ----
        st.markdown("### ğŸ“Š Pred Ct ë¶„í¬")
        plot_pred_ct_hist(pred_df)

        # ---- figure 3) í’ˆì§ˆì§€í‘œ(CV) vs Ct ----
        st.markdown("### ğŸ§ª í’ˆì§ˆì§€í‘œ(CV) vs Ct")
        plot_cv_vs_ct(df_long, pred_df, cutoff=int(cutoff))

        # ---- í‘œëŠ” ì˜µì…˜ ----
        if show_tables:
            st.markdown("### ğŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸”")
            st.dataframe(pred_df, use_container_width=True)

        # âœ… 6) truth ìˆìœ¼ë©´ ì¦‰ì„ í‰ê°€ (ì¤‘ìš”: df_truth ë„˜ê¸°ê¸°!)
        try_eval_if_truth_exists(df_curve, pred_df, truth_df=df_truth)

        st.divider()
        
        # =========================
        # (A) Pred vs True + error color (Upload)
        # =========================
        st.markdown("### ğŸ¯ Pred vs True (ì—…ë¡œë“œ ë°ì´í„°, ì˜¤ì°¨ ìƒ‰ìƒ í‘œì‹œ)")
        
        if df_truth is not None:
            # eval_df ë§Œë“¤ê¸°: try_eval_if_truth_existsì™€ ë™ì¼í•œ ë°©ì‹(Well normalize)
            true_col = None
            for cand in ["true_ct", "TrueCt", "trueCt", "ct", "Ct", "CT", "Cq", "cq", "CQ"]:
                if cand in df_truth.columns:
                    true_col = cand
                    break
        
            wcol = None
            for cand in ["Well", "well", "WELL"]:
                if cand in df_truth.columns:
                    wcol = cand
                    break
        
            if true_col and wcol:
                truth2 = df_truth[[wcol, true_col]].copy()
                truth2.columns = ["Well", "true_ct"]
                truth2["Well"] = truth2["Well"].map(normalize_well)
        
                tmp_eval = pred_df.copy()
                tmp_eval["Well"] = tmp_eval["Well"].map(normalize_well)
                tmp_eval = tmp_eval.merge(truth2, on="Well", how="left")
        
                tmp_eval["true_ct"] = pd.to_numeric(tmp_eval["true_ct"], errors="coerce")
                tmp_eval["pred_ct"] = pd.to_numeric(tmp_eval["pred_ct"], errors="coerce")
                tmp_eval = tmp_eval.dropna(subset=["true_ct", "pred_ct"]).copy()
        
                if not tmp_eval.empty:
                    tmp_eval["err"] = tmp_eval["pred_ct"] - tmp_eval["true_ct"]
                    tmp_eval["abs_err"] = tmp_eval["err"].abs()
        
                    import altair as alt
                    line_df = _line_y_eq_x(tmp_eval.rename(columns={"true_ct": "true_ct", "pred_ct": "pred_ct"}))
        
                    base = alt.Chart(tmp_eval).mark_circle(size=70, opacity=0.85).encode(
                        x=alt.X("true_ct:Q", title="True Ct/Cq"),
                        y=alt.Y("pred_ct:Q", title="Pred Ct/Cq"),
                        color=alt.Color("abs_err:Q", title="|Error|"),
                        tooltip=["Well", "true_ct", "pred_ct", "err", "abs_err"]
                    ).interactive()
        
                    diag = alt.Chart(line_df.rename(columns={"x": "true_ct", "y": "pred_ct"})).mark_line().encode(
                        x="true_ct:Q", y="pred_ct:Q"
                    )
        
                    st.altair_chart((diag + base).properties(height=360), use_container_width=True)
                    st.caption("âœ… ì ì´ ëŒ€ê°ì„ (y=x)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ˆì¸¡ì´ ì˜ ë§ëŠ” ê±°ì•¼. ìƒ‰ì´ ì§„í• ìˆ˜ë¡(|Error| í¼) ë” ë§ì´ í‹€ë¦° ìƒ˜í”Œ.")
                else:
                    st.info("truthëŠ” ìˆëŠ”ë° predì™€ ë§¤ì¹­ëœ í–‰ì´ ì—†ì–´(Well ë§¤ì¹­ í™•ì¸ í•„ìš”).")
            else:
                st.info("truth ì‹œíŠ¸ì—ì„œ Well/true_ct(Ct/Cq) ì»¬ëŸ¼ì„ ëª» ì°¾ì•„ì„œ Pred vs True ì°¨íŠ¸ëŠ” ìƒëµí–ˆì–´.")
        else:
            st.info("truth ì‹œíŠ¸ê°€ ì—†ì–´ì„œ Pred vs True(ì •ë‹µ ê¸°ë°˜) ì°¨íŠ¸ëŠ” ìƒëµí–ˆì–´.")
        
        # =========================
        # (B) Upload Hard-like Review (Top-K |error|)
        # =========================
        st.markdown("### ğŸ§¨ Upload Hard Review (Top-K |error|)")
        
        if df_truth is not None and 'tmp_eval' in locals() and (tmp_eval is not None) and (not tmp_eval.empty):
            topk_u = st.slider("Hard Top-K (ì—…ë¡œë“œ)", 5, 50, 15, 5, key="pred_upload_hard_topk")
        
            hard_u = tmp_eval.sort_values("abs_err", ascending=False).head(int(topk_u)).reset_index(drop=True)
        
            c1, c2, c3 = st.columns(3)
            c1.metric("n (eval)", int(len(tmp_eval)))
            c2.metric("Hard Top-K", int(len(hard_u)))
            c3.metric("Worst |error|", f"{float(hard_u['abs_err'].iloc[0]):.3f}")
        
            with st.expander("ğŸ“‹ Hard í›„ë³´ í‘œ(ì—…ë¡œë“œ)", expanded=False):
                st.dataframe(hard_u[["Well", "true_ct", "pred_ct", "err", "abs_err"]], use_container_width=True, height=280)
        
            # ì„ íƒí•´ì„œ ê³¡ì„  ë³´ê¸°
            def _fmt_u(i: int) -> str:
                r = hard_u.iloc[i]
                return f"{r['Well']} | |err|={r['abs_err']:.3f} (err={r['err']:+.3f})"
        
            pick_u = st.selectbox(
                "ê²€í† í•  Hard ìƒ˜í”Œ ì„ íƒ(ì—…ë¡œë“œ)",
                options=list(range(len(hard_u))),
                format_func=_fmt_u,
                key="pred_upload_hard_pick",
            )
        
            well_pick = str(hard_u.loc[int(pick_u), "Well"])
        
            st.markdown("#### ğŸ“ˆ ì„ íƒ Hard ìƒ˜í”Œì˜ ì›ë³¸ ê³¡ì„ (ì—…ë¡œë“œ df_long)")
            curve_sel = df_long[df_long["Well"].astype(str) == str(well_pick)].copy()
            if curve_sel.empty:
                st.info("ì„ íƒ wellì˜ curveë¥¼ df_longì—ì„œ ëª» ì°¾ì•˜ì–´.")
            else:
                import altair as alt
                curve_sel = curve_sel.sort_values("Cycle").reset_index(drop=True)
                cutoff_i = int(cutoff)
                curve_sel["segment"] = np.where(curve_sel["Cycle"] <= cutoff_i, "early(<=cutoff)", "late")
        
                line = alt.Chart(curve_sel).mark_line().encode(
                    x=alt.X("Cycle:Q", title="Cycle"),
                    y=alt.Y("Fluor:Q", title="Fluor"),
                    tooltip=["Cycle", "Fluor", "segment"]
                )
                vline = alt.Chart(pd.DataFrame({"Cycle": [cutoff_i]})).mark_rule(strokeDash=[6,4]).encode(x="Cycle:Q")
                st.altair_chart(line + vline, use_container_width=True)
        
                st.markdown("#### ğŸ” Early í™•ëŒ€(<=cutoff)")
                early = curve_sel[curve_sel["Cycle"] <= cutoff_i].copy()
                if len(early) >= 2:
                    eline = alt.Chart(early).mark_line().encode(
                        x=alt.X("Cycle:Q", title="Cycle (early)"),
                        y=alt.Y("Fluor:Q", title="Fluor"),
                        tooltip=["Cycle", "Fluor"]
                    )
                    st.altair_chart(eline, use_container_width=True)
        else:
            st.info("truth ê¸°ë°˜ í‰ê°€ê°€ ì—†ì–´ì„œ(ë˜ëŠ” ë§¤ì¹­ ì‹¤íŒ¨) ì—…ë¡œë“œ Hard ReviewëŠ” ìƒëµí–ˆì–´.")

        # =========================
        # (C) Pred stability across cutoffs (ì—°ê²°ì„ )
        #  - Multi-cutoff Sweepì„ ì¼°ì„ ë•Œë§Œ ì˜ë¯¸ ìˆìŒ
        # =========================
        st.markdown("### ğŸ”— Cutoffì— ë”°ë¥¸ ì˜ˆì¸¡ ë³€í™”(ì„  ì—°ê²°)")
        
        if 'preds_all' in locals() and isinstance(preds_all, pd.DataFrame) and (not preds_all.empty):
            # í•œ ë²ˆì— ë„ˆë¬´ ë§ìœ¼ë©´ ë³´ê¸° í˜ë“¤ì–´ì„œ, ë³´ì—¬ì¤„ well ìˆ˜ ì œí•œ
            max_w = st.slider("í‘œì‹œí•  Well ê°œìˆ˜(ìƒìœ„)", 5, 40, 15, 5, key="pred_stab_maxw")
        
            show_wells = preds_all["Well"].astype(str).unique().tolist()[:int(max_w)]
            stab = preds_all[preds_all["Well"].astype(str).isin(show_wells)].copy()
        
            # truthê°€ ìˆìœ¼ë©´ ê°™ì´ ê·¸ë¦¬ê¸°(ê°€ëŠ¥í•˜ë©´)
            if df_truth is not None:
                true_col = None
                for cand in ["true_ct", "ct", "Ct", "Cq", "cq", "CQ"]:
                    if cand in df_truth.columns:
                        true_col = cand; break
                wcol = None
                for cand in ["Well", "well", "WELL"]:
                    if cand in df_truth.columns:
                        wcol = cand; break
        
                if true_col and wcol:
                    truth2 = df_truth[[wcol, true_col]].copy()
                    truth2.columns = ["Well", "true_ct"]
                    truth2["Well"] = truth2["Well"].map(normalize_well)
        
                    stab["Well"] = stab["Well"].map(normalize_well)
                    stab = stab.merge(truth2, on="Well", how="left")
        
            import altair as alt
            base = alt.Chart(stab.dropna(subset=["cutoff", "pred_ct"])).encode(
                x=alt.X("cutoff:Q", title="Cutoff"),
                y=alt.Y("pred_ct:Q", title="Pred Ct"),
                color=alt.Color("Well:N", legend=None),
                tooltip=["Well", "cutoff", "pred_ct"] + (["true_ct"] if "true_ct" in stab.columns else [])
            )
        
            lines = base.mark_line(point=True).properties(height=320)
        
            if "true_ct" in stab.columns and stab["true_ct"].notna().any():
                # TrueëŠ” cutoffë§ˆë‹¤ ë³€í•˜ì§€ ì•Šìœ¼ë‹ˆê¹Œ ì ì„ ìœ¼ë¡œ ê°™ì´ ë³´ì—¬ì£¼ë©´ â€œì •ë‹µ ëŒ€ë¹„ í”ë“¤ë¦¼â€ì´ ë°”ë¡œ ë³´ì„
                true_line = alt.Chart(stab.dropna(subset=["cutoff","true_ct"])).mark_line(strokeDash=[6,4]).encode(
                    x="cutoff:Q",
                    y="true_ct:Q",
                    detail="Well:N",
                    color=alt.value("gray"),
                )
                st.altair_chart((lines + true_line).interactive(), use_container_width=True)
                st.caption("ì‹¤ì„ =Pred ë³€í™”, ì ì„ =True(ê³ ì •). Predê°€ cutoffì— ë”°ë¼ ì•ˆì •ì ì´ë©´ ì‹¤ì„ ì´ ì ì„  ì£¼ë³€ì—ì„œ í¬ê²Œ í”ë“¤ë¦¬ì§€ ì•Šì•„.")
            else:
                st.altair_chart(lines.interactive(), use_container_width=True)
                st.caption("truthê°€ ì—†ì–´ì„œ True ê¸°ì¤€ì„ ì€ ìƒëµí–ˆì–´. ê·¸ë˜ë„ cutoffì— ë”°ë¥¸ ì˜ˆì¸¡ ì•ˆì •ì„±ì€ í™•ì¸ ê°€ëŠ¥.")
        else:
            st.info("Multi-cutoff Sweepì„ ì¼œë©´(ê·¸ë¦¬ê³  preds_all ìƒì„±ë˜ë©´) cutoffë³„ ì˜ˆì¸¡ ì—°ê²°ì„ ì´ ìƒê²¨.")

                    
        # =========================
        # (ì¶”ê°€) ê·¸ë¦¼ ì¤‘ì‹¬ 4íƒ­ UI (ì›í•˜ë©´ ìœ ì§€)
        # =========================
        import altair as alt

        t1, t2, t3, t4 = st.tabs(["ğŸ“Š Ct Overview", "ğŸ“ˆ Wellë³„ Ct", "ğŸ§¬ Curve ë³´ê¸°", "ğŸ§¾ Data(í‘œ)"])

        with t1:
            hist = (
                alt.Chart(pred_df)
                .mark_bar()
                .encode(
                    x=alt.X("pred_ct:Q", bin=alt.Bin(maxbins=30), title="Predicted Ct"),
                    y=alt.Y("count():Q", title="Count"),
                    tooltip=[alt.Tooltip("count():Q", title="count")],
                )
                .properties(height=280)
            )
            st.altair_chart(hist, use_container_width=True)

        with t2:
            bar = (
                alt.Chart(pred_df)
                .mark_bar()
                .encode(
                    x=alt.X("Well:N", sort=alt.SortField("pred_ct", order="ascending"), title="Well"),
                    y=alt.Y("pred_ct:Q", title="Predicted Ct"),
                    tooltip=["Well", "pred_ct"],
                )
                .properties(height=320)
            )
            st.altair_chart(bar, use_container_width=True)

        with t3:
            wells = pred_df["Well"].astype(str).tolist()
            pick_well = st.selectbox("ê³¡ì„  ë³¼ Well ì„ íƒ", wells, index=0, key="pred_pick_well")

            curve_sel = df_long[df_long["Well"].astype(str) == str(pick_well)].copy()
            if curve_sel.empty:
                st.info("ì„ íƒí•œ Wellì˜ curve ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´.")
            else:
                curve_sel = curve_sel.sort_values("Cycle").copy()
                cutoff_i = int(cutoff)

                line = (
                    alt.Chart(curve_sel)
                    .mark_line()
                    .encode(
                        x=alt.X("Cycle:Q", title="Cycle"),
                        y=alt.Y("Fluor:Q", title="Fluor"),
                        tooltip=["Cycle", "Fluor"],
                    )
                )
                vline = (
                    alt.Chart(pd.DataFrame({"Cycle": [cutoff_i]}))
                    .mark_rule(strokeDash=[6, 4])
                    .encode(x="Cycle:Q")
                )
                st.altair_chart(line + vline, use_container_width=True)

                ct_val = float(pred_df.loc[pred_df["Well"].astype(str) == str(pick_well), "pred_ct"].iloc[0])
                st.caption(f"âœ… Well={pick_well} / Pred Ct={ct_val:.3f} (cutoff={cutoff_i})")

        with t4:
            with st.expander("í‘œë¡œ ë³´ê¸° (ì›ë³¸/ì˜ˆì¸¡ ê²°ê³¼)", expanded=False):
                if isinstance(raw_obj, dict):
                    st.caption("curve ì‹œíŠ¸(ìƒë‹¨ ì¼ë¶€)")
                    st.dataframe(df_curve.head(30), use_container_width=True)
                    if df_truth is not None:
                        st.caption("truth ì‹œíŠ¸(ìƒë‹¨ ì¼ë¶€)")
                        st.dataframe(df_truth.head(30), use_container_width=True)
                else:
                    st.caption("ì—…ë¡œë“œ ì›ë³¸(ìƒë‹¨ ì¼ë¶€)")
                    st.dataframe(df_curve.head(30), use_container_width=True)

                st.caption("ì˜ˆì¸¡ ê²°ê³¼")
                st.dataframe(pred_df, use_container_width=True)


# -------------------------
# Tab 2: Hard Review
# -------------------------
with tabs[3]:
    show_hard_review_with_buckets()

# -------------------------
# Tab 3: Retrain (Admin)
# -------------------------
with tabs[4]:
    st.subheader("2) ëˆ„ì  ë°˜ì˜ í›„ ì¬í•™ìŠµ (ê´€ë¦¬ì ë²„íŠ¼)")

    if running_on_streamlit_cloud():
        st.warning(
            "Streamlit Cloudì—ëŠ” canonical ë°ì´í„°(master_long.parquet)ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ìš”.\n"
            "ì„œë²„/ë¡œì»¬ì—ì„œ í•™ìŠµ í›„ reports/ ê²°ê³¼ë¬¼ë§Œ ë°°í¬í•˜ì„¸ìš”."
        )
        st.stop()  # âœ… ì—¬ê¸°ì„œ íƒ­ ì‹¤í–‰ì„ ëë‚´ë²„ë¦¬ë©´ step3ê°€ ì ˆëŒ€ í˜¸ì¶œë˜ì§€ ì•ŠìŒ

    st.info(
        "ì´ ë²„íŠ¼ì€ **í˜„ì¬ ì„œë²„ì— ì €ì¥ëœ canonical ë°ì´í„°(master_long.parquet)** ê¸°ì¤€ìœ¼ë¡œ "
        "ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ê³  data/models/by_cutoffì— ë®ì–´ì”ë‹ˆë‹¤.\n\n"
        "âš ï¸ ë°ì´í„° ingest(= raw -> canonical)ëŠ” ì´ ë²„íŠ¼ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ìš”. "
        "ìƒˆ raw ë°ì´í„°ë¥¼ canonicalë¡œ ë°˜ì˜í•˜ë ¤ë©´ ingest íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¨¼ì € master_longì„ ì—…ë°ì´íŠ¸í•´ì¤˜ì•¼ í•´ìš”."
    )

    meta = load_meta(int(cutoff))
    if meta:
        with st.expander("ì„ íƒëœ ëª¨ë¸ ë©”íƒ€ ë³´ê¸°"):
            st.json(meta)

    can_retrain = has_canonical_master_long() and (not running_on_streamlit_cloud())

    if not can_retrain:
        st.warning(
            "Streamlit Cloudì—ëŠ” í•™ìŠµ ë°ì´í„°(master_long.parquet)ê°€ ì—†ì–´ì„œ ì¬í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ìš”.\n"
            "ë¡œì»¬/ì„œë²„ì—ì„œ í•™ìŠµì„ ëŒë¦° ë’¤, reports/ ê²°ê³¼ë¬¼ë§Œ repoì— ì»¤ë°‹í•´ì„œ ë°°í¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìš´ì˜í•˜ì„¸ìš”."
        )
    
    if st.button("ì¬í•™ìŠµ ì‹¤í–‰", type="secondary", key="btn_retrain", disabled=not can_retrain):
        with st.spinner("ì¬í•™ìŠµ ì¤‘... (ë¡œê·¸ ìƒì„± ì¤‘)"):
            code, log = run_retrain(int(min_c), int(max_c))
    
        st.text_area("í•™ìŠµ ë¡œê·¸", log, height=380)
    
        if code == 0:
            st.success("ì¬í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ íŒŒì¼ì´ ê°±ì‹ ëì–´ìš”.")
            show_train_report()
        else:
            st.error(f"ì¬í•™ìŠµ ì‹¤íŒ¨ (return code={code}) - ë¡œê·¸ë¥¼ í™•ì¸í•´ì¤˜.")
            
try:
    st.caption("VERSION: " + (PROJECT_ROOT / "VERSION.txt").read_text().strip())
except Exception:
    st.caption("VERSION: (missing)")

